```
Geometria
A.A. 2020-2021
Docente: M. Trombetti
Docente: M. Trombetti
Dispense tratte dal corso di Informatica a cura dello studente S. Cerrone
Dispense tratte dal corso di Informatica
cura dello studente S. Cerrone

Sommario
0. Operazioni su di un Insieme ............................................................................................................ 4
Operazione interna a ùëÜ .................................................................................................................................. 4
Operazione esterna a ùëÜ ................................................................................................................................. 4
Prodotto scalare standard ............................................................................................................................. 4
1. Operazioni su Matrici (su ‚Ñù) ............................................................................................................ 4
Somma di matrici e prodotto per uno scalare (reale) ................................................................................... 4
Prodotto righe per colonne e propriet√† ........................................................................................................ 4
Matrice trasposta .......................................................................................................................................... 5
Matrice a gradini ........................................................................................................................................... 5
Operazioni elementari di Riga ....................................................................................................................... 6
2. Sistemi Lineari ................................................................................................................................ 6
Equazioni lineari su ‚Ñù .................................................................................................................................... 6
Sistemi equivalenti ........................................................................................................................................ 7
Soluzioni di un sistema lineare ...................................................................................................................... 7
Sistema omogeneo ........................................................................................................................................ 9
Notazioni per i sistemi lineari ........................................................................................................................ 9
3. Spazi Vettoriali ............................................................................................................................... 9
Definizione ..................................................................................................................................................... 9
Esempi di spazi vettoriali ............................................................................................................................... 9
Propriet√† degli spazi vettoriali ..................................................................................................................... 11
Definizioni .................................................................................................................................................... 11
Sottospazi vettoriali ..................................................................................................................................... 11
Esempi di sottospazi vettoriali..................................................................................................................... 12
Alcune propriet√† e definizioni di sottospazi vettoriali (sottospazio generato) ........................................... 13
Somma diretta di sottospazi vettoriali ........................................................................................................ 14
Dipendenza ed indipendenza lineare .......................................................................................................... 14
Sistemi di vettori equivalenti ....................................................................................................................... 15
Relazione tra dipendenza e linearmente dipendenza ................................................................................. 16
Vettori finitamente generabili ..................................................................................................................... 18
Dimensione di uno spazio vettoriale ........................................................................................................... 19
Relazione di Grassmann .............................................................................................................................. 21
Forma canonica ........................................................................................................................................... 22
Metodi per estrarre una base da un sottospazio generato ........................................................................ 22
4. Matrici e Sistemi lineari ................................................................................................................ 23
Determinante di una matrice quadrata ...................................................................................................... 23
Minore di una matrice ................................................................................................................................. 24
Criteri di compatibilit√† di sistemi di equazioni lineari ................................................................................. 26
Risoluzione di sistemi lineari in situazioni particolari .................................................................................. 27
Sistemi omogenei ........................................................................................................................................ 28
Risoluzione sistema omogeneo con ùëõ ‚àí 1 equazioni indipendenti ed ùëõ incognite .................................... 29
5. Applicazioni Lineari ....................................................................................................................... 30
Definizione ................................................................................................................................................... 30
Esempi ......................................................................................................................................................... 30
Proposizioni ................................................................................................................................................. 31
Concetto di Ker e sue propriet√† .................................................................................................................. 32
Isomorfismo coordinato .............................................................................................................................. 33
Forma canonica delle applicazioni lineari e matrice di passaggio ............................................................... 34
2

6. Matrici simili e diagonalizzazione (spazi vettoriali) ......................................................................... 35
Matrici simili ................................................................................................................................................ 35
Diagonalizzazione di un endomorfismo ...................................................................................................... 36
Diagonalizzazione di una matrice ................................................................................................................ 40
Spazi Vettoriali: Prodotti diretti esterni ...................................................................................................... 40
7. Geometria .................................................................................................................................... 41
Spazio vettoriale dei vettori geometrici liberi dello spazio (e del piano) .................................................... 41
Cambiamenti di riferimento ........................................................................................................................ 44
Prodotto vettoriale ...................................................................................................................................... 44
Rappresentazioni ......................................................................................................................................... 45
Rappresentazione di un piano nello spazio ............................................................................................. 45
Rappresentazione della retta nel piano .................................................................................................. 47
Altre tipologie di espressione ...................................................................................................................... 48
Espressione di una retta passante per due punti .................................................................................... 48
Coseni direttori ........................................................................................................................................ 48
Rette parallele ......................................................................................................................................... 48
Distanza tra insiemi ................................................................................................................................. 49
Punto medio di un segmento .................................................................................................................. 49
Asse del segmento ................................................................................................................................... 49
Rappresentazione ordinaria di una retta nello spazio ............................................................................ 49
3

0. Operazioni su di un Insieme
Operazione interna a ùë∫
Sia ùëÜ un insieme non vuoto, un‚Äôoperazione ‚ä•: ùëÜ √ó ùëÜ ‚Üí ùëÜ si dice interna a S. Tutte le operazioni interne
possono essere impropriamente considerate esterne; un esempio di operazione interna √® +:‚Ñù2 √ó ‚Ñù2 ‚Üí ‚Ñù2
Operazione esterna a ùë∫
Siano ùëÜ, ùê¥ ‚â† ‚àÖ, un‚Äôoperazione ‚ä•: ùëÜ √ó ùê¥ ‚Üí ùëÜ si dice esterna a ùëÜ con dominio di operatori in ùê¥ perch√© gli
oggetti di ùê¥ agiscono su ùëÜ portandoli in altri oggetti di ùëÜ. Esempio √® la moltiplicazione ‚ãÖ ‚à∂ ‚Ñù2 √ó ‚Ñù ‚Üí ‚Ñù2.
Prodotto scalare standard
‚ãÖ ‚à∂ ‚Ñùùëõ √ó ‚Ñùùëõ ‚Üí ‚Ñù, il quale prende in input due ennuple di numeri reali e restituisce un numero reale; non √®
n√© un‚Äôoperazione interna ne esterna, infatti (ùëé1, ‚Ä¶ , ùëéùëõ) ‚ãÖ (ùëè1,‚Ä¶ , ùëèùëõ) = ùëé1 ‚ãÖ ùëè1 +‚ãØ+ ùëéùëõ ‚ãÖ ùëèùëõ. Un prodotto
scalare standard ha la propriet√† di essere una forma bilineare simmetrica definita positiva. Ovvero:
1) Simmetria: ùëé ‚ãÖ ùëè = ùëè ‚ãÖ ùëé con ùëé = (ùëé1,‚Ä¶ , ùëéùëõ) e ùëè = (ùëè1,‚Ä¶ , ùëèùëõ)
2) Bilineare: (‚Ñé ‚ãÖ ùëé + ùëò ‚ãÖ ùëè) ‚ãÖ ùëê = ‚Ñé(ùëé ‚ãÖ ùëê) + ùëò(ùëè ‚ãÖ ùëê) con ‚Ñé, ùëò scalari
Dimostrazione: Sia ùëé = (ùëé1, ùëé2), ùëè = (ùëè1, ùëè2) e ùëê = (ùëê1, ùëê2) allora:
(‚Ñé ‚ãÖ ùëé + ùëò ‚ãÖ ùëè) ‚ãÖ ùëê = (‚Ñé(ùëé1, ùëé2) + ùëò(ùëè1, ùëè2))(ùëê1, ùëê2) = (‚Ñéùëé1ùëê1 + ‚Ñéùëé2ùëê2) + (ùëòùëè1ùëê1 + ùëòùëè2ùëê2)
= ‚Ñé(ùëé1ùëê1 + ùëé2ùëê2) + ùëò(ùëè1, ùëê1 + ùëè2ùëê2) = ‚Ñé(ùëé ‚ãÖ ùëê) + ùëò(ùëè ‚ãÖ ùëê)
3) Positiva: ùëé ‚ãÖ ùëé ‚â• 0 e ùëé ‚ãÖ ùëé = 0 ‚áî ùëé = (0, ‚Ä¶ ,0)
1. Operazioni su Matrici (su ‚Ñù)
Somma di matrici e prodotto per uno scalare (reale)
Una matrice ùê¥ ad ùëõ righe ed ùëö colonne si scrive nel modo seguente:
ùê¥ = ( ‚ãÆ ‚ã±
ùëé1,1 ‚ãØ ùëé1,ùëö
ùëéùëõ,1 ‚ãØ ùëéùëõ,ùëö
‚ãÆ )
A volte posso denotare la matrice in modo generico come ùê¥ = (ùëéùëñùëó)ùëñ=1,‚Ä¶,ùëõ ;ùëó=1,‚Ä¶,ùëö , o usare la definizione
formale ùê¥ = {1,‚Ä¶ , ùëõ} √ó {1,‚Ä¶ , ùëö} ‚Üí ‚Ñù. L‚Äôinsieme delle matrici su ‚Ñù con ùëõ righe ed ùëö colonne viene
rappresentato da ùëÄùëõ,ùëö(‚Ñù) o semplicemente ‚Ñùùëõ,ùëö.
Per le matrici possiamo definire un‚Äôoperazione interna +: ‚Ñùùëõ,ùëö √ó ‚Ñùùëõ,ùëö ‚Üí ‚Ñùùëõ,ùëö del tipo:
(( ‚ãÆ ‚ã±
[/
\\
ùëé1,1 ‚ãØ ùëé1,ùëö
ùëéùëõ,1 ‚ãØ ùëéùëõ,ùëö
‚ãÆ ) , ( ‚ãÆ ‚ã±
\
/ \
ùëè1,1 ‚ãØ ùëè1,ùëö
ùëèùëõ,1 ‚ãØ ùëèùëõ,ùëö
‚ãÆ )) ‚Üí (
ùëé1,1 + ùëè1,1 ‚ãØ ùëé1,ùëö + ùëè1,ùëö
‚ãÆ
J)\ùëéùëõ,1 + ùëèùëõ,1 ‚ãØ ùëéùëõ,ùëö + ùëèùëõ,ùëö
‚ã±
‚ãÆ
)
Possiamo definire anche un‚Äôoperazione esterna ‚ãÖ ‚à∂ ‚Ñùùëõ,ùëö √ó ‚Ñù ‚Üí ‚Ñùùëõ,ùëö del tipo ((ùëéùëñùëó), ‚Ñé) ‚Üí (‚Ñé ‚ãÖ ùëéùëñùëó)
Prodotto righe per colonne e propriet√†
Il prodotto righe per colonne √® un‚Äôoperazione del tipo ‚ãÖ ‚à∂ ‚Ñùùëõ,ùëö √ó ‚Ñùùëö,ùëô ‚Üí ‚Ñùùëõ,ùëô; infatti affinch√©
quest‚Äôoperazione sia possibile devo avere le colonne della matrice di sinistra uguale alle righe della matrice
di destra, e come risultato avr√≤ una matrice che ha il numero di righe della matrice a sinistra e il numero di
colonne della matrice di destra.
Si definisce prodotto righe per colonne di ùê¥ ‚àà ‚Ñùùëö,ùëõ e ùêµ ‚àà ‚Ñùùëõ,ùëû
la matrice, che si denota con ùê¥ √ó ùêµ o
semplicemente con ùê¥ùêµ, definita dalla posizione ùê¥ùêµ = (ùê¥ùëñ ¬∑ ùêµùëó) ‚àà ‚Ñùùëö,ùëû
Esercizio: Fare, ove possibile, il prodotto righe per colonne delle seguenti matrici:
(
5 ‚àí1
0 2
1 ‚àí3
) ‚ãÖ (2 0
1 2
)
( 3 1
‚àí1 4
) ‚ãÖ (5 ‚àí2) (5 ‚àí1 2
0 2 3
) ‚ãÖ (
2 0 3 ‚àí1
1 2
0 2
0 ‚àí1 0 0
) (1 1
2 2
) ‚ãÖ ( 1 ‚àí1
‚àí1 1
)
4

Il prodotto righe per colonne fornisce le seguenti propriet√†:
1) Distributivit√† a destra: ‚àÄùê¥ ‚àà ‚Ñùùëö,ùëõ e ‚àÄùêµ, ùê∂ ‚àà ‚Ñùùëõ,ùëû, ùê¥(ùêµ + ùê∂) = ùê¥ùêµ + ùê¥ùê∂
Dim.: ùê¥(ùêµ + ùê∂) = (ùëéùëñùëó) (ùëèùëòùëô + ùëêùëòùëô
2) Distributivit√† a sinistra: (ùê¥ + ùêµ)ùê∂ = ùê¥ùê∂ + ùêµùê∂, ‚àÄùê¥, ùêµ ‚àà ‚Ñùùëö,ùëõ ùëí ‚àÄùê∂ ‚àà ‚Ñùùëõ,ùëû
La dimostrazione √® analoga alla precedente
YT)
/
3) ùê¥ùêµ ‚â† ùêµùê¥, ad esempio: (0 1Vac
) (1 1
iN0 17X0 0
) = (0 0
0 0
) invece (1 1
0 0
) (0 1
0 1
) = (0 2
0 0
)
4) Ha elemento neutro formato dalla matrice identica ùêºùëõ formata da tutti zeri eccetto per la diagonale
principale con 1. La matrice identica deve essere quadrata: ùêºùëõ = (
1
‚ã±
0
1
0
). La diagonale secondaria
sar√† quella da destra a sinistra. La matrice identica ha la propriet√† che ùêºùëõùê¥ = ùê¥ùêºùëõ = ùê¥.
ESERCIZIO: verifica che ùêºùëõùê¥ = ùê¥ùêºùëõ = ùê¥. E che ùêºùëõ sia l‚Äôunico elemento neutro.
5) Gode di propriet√† commutativa nel caso di matrici scalari ùëÜ di ordine ùëõ (che occupano solo la
diagonale principale con una stessa costante, mentre il resto √® 0), infatti qualunque sia la matrice ùê¥
di ordine ùëõ avremo ùê¥ùëÜ = ùëÜùê¥. Possiamo definire ùëÜùëõ = ‚Ñé ‚ãÖ ùêºùëõ.
Nota bene che in generale non commutano matrici diagonali (tutti 0 eccetto per la diagonale) che
non hanno la stessa costante.
6) ‚àÄùê¥ ‚àà ‚Ñùùëö,ùëõ, ‚àÄùêµ ‚àà ‚Ñùùëõ,ùëû e ‚àÄ‚Ñé ‚àà ‚Ñù ùê¥(‚Ñéùêµ) = ‚Ñé(ùê¥ùêµ) = (‚Ñéùê¥)ùêµ
Dim.: (ùíÇùíäùíã)[ùíâ(ùíÉùíäùíã)] = (ùëéùëñùëó)(‚Ñéùëèùëñùëó) = (ùëéùëñ ‚ãÖ ‚Ñéùëèùëó) = ùíâ(ùíÇùíä ‚ãÖ ùíÉùíã) = (‚Ñéùëéùëñ ‚ãÖ ùëèùëó) = (‚Ñéùëéùëñùëó)(ùëèùëñùëó) = [ùíâ(ùíÇùíäùíã)](ùíÉùíäùíã)
ùê¥(ùêµùê∂) = (ùê¥ùêµ)ùê∂
7) Associativit√†: ‚àÄùê¥ ‚àà ‚Ñùùëö,ùëõ, ‚àÄùêµ ‚àà ‚Ñùùëõ,ùëû, ‚àÄùê∂ ‚àà ‚Ñùùëû,ùëó
ùëõ
ùê¥(ùêµùê∂) = ùê¥ùê∑ = ùëéùëñ ‚ãÖ ùëëùëó =‚àë(ùëéùëñùëòùëëùëòùëó)
NW
~ 7a
ùëò=1
=‚àë(ùëéùëñùëò (‚àë(ùëèùëòùëô ùëêùëôùëó)
ùëû
al (wa.
ùëõ
ùëò=1
ùëô=1
))
\\
yy
=‚àë(ùëéùëñùëò ùëèùëòùëôùëêùëôùëó)
ùëò,ùëô
= ùê¥ùêµùê∂ = (ùê¥ùêµ)ùê∂
Matrice trasposta
L‚Äôoperazione di trasposta √® un‚Äôoperazione unaria ùë°: ‚Ñùùëõ,ùëö ‚Üí ‚Ñùùëö,ùëõ dove semplicemente scambio le righe con
le colonne: ùê¥ = (1 2 3
4 5 6
) ‚Üí (
1 4
2 5
3 6
) = ùê¥ùë° . Formalmente una i-riga diverr√† una i-colonna: ùëéùëñ ‚Üí (ùëéùë°)
ùëñ
‚èü    
ùëëùëòùëô
) = (ùëéùëñ ‚ãÖ ùëëùëó) = (ùëéùëñ ‚ãÖ (ùëèùëó + ùëêùëó)) = (ùëéùëñ ‚ãÖ ùëèùëó) + (ùëéùëñ ‚ãÖ ùëêùëó) = ùê¥ùêµ + ùê¥ùê∂
Sia ùê¥ ‚àà ‚Ñùùëö,ùëõ, ùêµ ‚àà ‚Ñùùëõ,ùëû. Allora la trasposta ha le seguenti propriet√†:
1) (ùê¥ùêµ)ùë° = ùêµùë°ùê¥ùë°
Dimostrazione: consideriamo ùê∂ = ùê¥ùêµ, ùê∑ = (ùê¥ùêµ)ùë° = ùê∂ùë° e ùê∏ = ùêµùë°ùê¥ùë° con ùê¥ùë° = ùê¥‚Ä≤, ùêµùë° = ùêµ‚Ä≤ allora:
ùê∑ = ùëëùëñùëó = ùëêùëóùëñ = ùëéùëó ‚ãÖ ùëèùëñ =‚àëùëéùëóùëòùëèùëòùëñ
ùëò
=‚àëùëéùëòùëó
XN
‚Ä≤ ùëèùëñùëò
‚Ä≤
Lad
ùëò
=‚àëùëèùëñùëò
‚Ä≤
ùëò
2) ùêµùë°ùê¥ùë° ‚â† ùê¥ùë°ùêµùë° il che significa in particolare che (ùê¥ùêµ)ùë° = ùêµùë°ùê¥ùë° ‚â† ùê¥ùë°ùêµùë°
3) (ùê¥ùêµùê∂)ùë° = ùê∂ùë°ùêµùë°ùê¥ùë°
4) (‚Ñéùê¥ + ùëòùêµ)ùë° = ‚Ñé(ùê¥ùë°) + ùëò(ùêµùë°) che si generalizza in (‚Ñé1ùê¥1 +‚ãØ+ ‚Ñéùëõùê¥ùëõ)ùë° = ‚Ñé1(ùê¥1
ùë° ) +‚ãØ+ ‚Ñéùëõ(ùê¥ùëõ
ùë° )
5) (ùê¥ùë°)ùë° = ùê¥; ovviamente la trasposta della trasposta √® la matrice originale.
Matrice a gradini
Il concetto della matrice a gradine √® che il numero degli zeri che precedono il primo elemento diverso da zero
in ogni riga aumenta di riga in riga fino ad eventuali righe costituite da soli zeri. Il primo elemento non nullo
che si incontra da sinistra si chiama pivot. Una delle propriet√† della matrice a gradini √® che se una matrice a
gradini √® priva di righe nulle allora il numero di righe deve essere minore o uguale al numero di colonne.
Di seguito esempi di matrici a gradini:
(
1 2 0 3
0 1 2 3
0 0 4 5
) (1 0 0 0)
(2 0 0
0 1 0
) (
1 2 0 0
0 2 0 3
0
0
0
0
0
0
0
0
)
5
‚Ä≤
ùëéùëòùëó
= ùëèùëñ ‚ãÖ ùëéùëó = ùëíùëñùëó = ùê∏
t 4
t 14

Operazioni elementari di Riga
ùê¥ = (
ùëé1 1 ‚ãØ ùëé1 ùëõ
‚ãÆ
‚ã±
ùëéùëö 1 ‚ãØ ùëéùëö ùëõ
‚ãÆ ) ‚àà ‚Ñùùëö,ùëõ che possiamo rappresentare con le notazioni ùê¥ = (
Le operazioni elementari di riga sono:
1) Definiamo la funzione ùê∏1
ùëé1
‚ãÆ
ùëéùëõ
) = (ùëé1 ‚Ä¶ ùëéùëõ).
ùëñ,ùëó: ‚Ñùùëö,ùëõ ‚Üí ‚Ñùùëö,ùëõ che prende la riga i-esima della matrice e la scambia con la
riga j-esima della stessa matrice.
2) ùê∏2
‚Ñé,ùëñ: ‚Ñùùëö,ùëõ ‚Üí ‚Ñùùëö,ùëõ che non fa altro che moltiplicare la i-esima riga per una costante ‚Ñé ‚â† 0 e ‚Ñé ‚àà ‚Ñù
3) ùê∏3
ùëñ,ùëó: ‚Ñùùëö,ùëõ ‚Üí ‚Ñùùëö,ùëõ che agisce sostituendo alla j-esima riga la somma tra la i-esima riga e la j-esima
riga.
ùê¥ ‚üº ùê∏1
ùëñ,ùëó =
 
 
(
 
ùëé1
‚ãÆ
   
ùëéùëó
‚ãÆ
ùëéùëõ)
 
 
 
ùëéùëñ
‚ãÆ
ùê¥ ‚üº ùê∏2
‚Ñé,ùëñ =
(
 
 
ùëéùëõ )
 
ùëé1
‚ãÆ
‚Ñéùëéùëñ
‚ãÆ
 
ùê¥ ‚üº ùê∏3
ùëñ,ùëó =
 
 
(
 
 
ùëé1
‚ãÆ
ùëéùëñ
‚ãÆ
ùëéùëñ + ùëéùëó
‚ãÆ
ùëéùëõ
 
 
)
 
 
Queste 3 operazioni possono essere combinate tra di loro dando luce ad altre operazioni, utile √® la seguente:
4) ùê∏4
‚Ñé,ùëñ,ùëó che combina ùê∏2 ed ùê∏3, quindi ùëéùëó ‚Üí ‚Ñéùëéùëñ + ùëéùëó (utile per annullare una riga)
Se ùê¥‚Ä≤ √® una matrice che si pu√≤ ottenere da ùê¥ mediante un numero finito di operazioni elementari allora
diciamo che ùê¥ e ùê¥‚Ä≤ sono equivalenti (per righe).
Proposizione: Per ogni matrice esiste una matrice a gradini ad essa equivalente.
Dimostrazione per induzione sul numero di righe: Sia ùê¥ ‚àà ‚Ñùùëõ,ùëö, per ùëõ = 1 avremo una matrice gi√† a gradini,
ùê¥ = (ùëé11 ‚Ä¶ ùëé1ùëö), supposta vera per ùëõ dimostriamo che sia vera per ùëõ + 1: se ùê¥ √® una matrice nulla allora
√® anche a gradini, ora se ùê¥ ‚â† 0 possiamo supporre che ci sia almeno un elemento non nullo, per cui posso
prendere la prima riga non nulla e scambiarla con un‚Äôeventuale riga nulla usando ùê∏1. Ora non ci resta che
annullare l‚Äôelemento al di sotto, a tal scopo possiamo usare le operazioni ùê∏2, ùê∏3 o ùê∏4. Iterando questo
processo potr√≤ annullare, se necessario, tutte le righe successive.
ESEMPIO: (
1 ‚àí1 0 1
1 2 0 0
0 1 0 1
) ùê∏4
‚àí1,1,2
‚ü∂
(
1 ‚àí1 0 1
0 3 0 ‚àí1
0 1
0 1 ‚ü∂ 0 3 0 ‚àí1
) ùê∏1
2,3
(
1 ‚àí1 0 1
0 1
0 1 ) ùê∏4
‚àí3,2,3
‚ü∂
ESERCIZIO riduci a gradini, ove necessario, le seguenti matrici: (
0 0 0 1
0 1 1 1
1
1
1
1
2. Sistemi Lineari
Equazioni lineari su ‚Ñù
Diciamo equazione lineare sul campo ‚Ñù nelle incognite ùë•1,‚Ä¶ , ùë•ùëõ una equazione del tipo ùíÇùüèùíôùüè +‚ãØ+
ùíÇùíèùíôùíè = ùíÉ con ùëé1, ‚Ä¶ , ùëéùëõ, ùëè ‚àà ‚Ñù, gli ùëéùëñ sono chiamati coefficienti delle rispettive incognite ùë•ùëñ mentre ùëè √® il
termine noto. Nel caso ùëè = 0 allora l‚Äôequazione si dice lineare omogenea.
Si definisce un sistema lineare ùëö equazioni ed ùëõ incognite ùëÜ {
ùëé1 1ùë•1 +‚ãØ+ ùëé1 ùëõùë•ùëõ = ùëè1
‚ãÆ
ùëéùëö 1ùë•1 +‚ãØ+ ùëéùëö ùëõùë•ùëõ = ùëèùëö
soluzione del sistema si intende la n-upla (ùë¶1, ‚Ä¶ , ùë¶ùëõ) tale che ciascuna delle equazioni risulta verificata
sostituendo ad ùë•ùëñ ùë¶ùëñ. L‚Äôinsieme delle soluzioni lo indico con ùëÜ = {(ùë¶1, ‚Ä¶ , ùë¶ùëõ)| ùë†ùëñùëéùëõùëú ùë†ùëúùëôùë¢ùëßùëñùëúùëõùëñ ùëëùëñ ùëÜ}
6
, mentre con
1
1
1
1
(
1 ‚àí1 0 1
0 1 0 1
0 0 0 ‚àí4
) , (
1 1 1
1 1 1
1
1
1
1
1
1
) , (
ùúã
ùëí
0 ùëí
ùúã)
ùúã
)

Un sistema si dice compatibile se ammette almeno una soluzione, incompatibile altrimenti. Nel caso il
sistema sia compatibile allora esso pu√≤ essere determinato nel caso abbia una sola soluzione e
indeterminato se ammette infinite soluzioni (fondamentalmente in ‚Ñù si hanno solo questi due casi per un
sistema compatibile, quindi o una oppure infinite).
Definisco la matrice dei coefficienti (o matrice incompleta) ùê¥ ‚àà ‚Ñùùëö,ùëõ = ( ‚ãÆ ‚ã± ‚ãÆ ) con ùëõ numero
ùëé11 ‚ãØ ùëé1ùëõ
ùëéùëö1 ‚ãØ ùëéùëöùëõ
delle incognite ed ùëö delle equazioni, se a questa matrice aggiungo la colonna dei termini noti avr√≤ la matrice
completa(
ùëé11 ‚ãØ ùëé1ùëõ
‚ãÆ ‚ã± ‚ãÆ
ùëéùëö1 ‚ãØ ùëéùëöùëõ ùëèùëö
|
ùëè1
‚ãÆ
) (la linea divisoria delinea quale sia la matrice dei coefficienti).
Se la matrice ha una particolare forma anche il sistema √® detto avere quella forma.
Sistemi equivalenti
Un sistema ùëÜ √® equivalente a ùëÜ‚Ä≤ se ammettono le stesse soluzione ovvero ùëÜ = ùëÜ‚Ä≤ (con la segnatura specifico
che sto parlando delle soluzioni di quel sistema), per dimostrare che due sistemi sono equivalenti si pu√≤ usare
la doppia inclusione (ovvero verificare che ùëÜ √® contenuto in ùëÜ‚Ä≤ e che ùëÜ‚Ä≤ √® contenuto in ùëÜ).
Proposizione: Ogni sistema di equazioni lineari √® equivalente ad un sistema a gradini.
Dimostrazione: bisogna verificare che le operazioni ùê∏1, ùê∏2, ùê∏3, ùê∏4 non influiscono nell‚Äôinsieme delle soluzioni
del sistema lineare. Per ùê∏1 che scambia due righe √® evidente che non influisce nel sistema poich√© il sistema
non dipende dall‚Äôordine dell‚Äôequazioni. ùê∏2 moltiplica una riga per uno scalare ‚Ñé ‚â† 0, diciamo che se ùë¶ ‚àà ùëÜ
avr√≤ ùë¶1ùëéùëñ1 +‚ãØ+ ùë¶ùëõùëéùëñùëõ = ùëèùëñ che √® equivalente a ùë¶1(‚Ñéùëéùëñ1) +‚ãØ+ ùë¶ùëõ(‚Ñéùëéùëñùëõ) = ‚Ñé(ùë¶1ùëéùëñ1 +‚ãØ+ ùë¶ùëõùëéùëñùëõ) = ‚Ñéùëèùëñ
Ora resta da provare la ùê∏3 (essendo ùê∏4 combinazione di ùê∏2 e ùê∏3), questa operazione significa sostituire ad
una riga ùëéùëó = ùëéùëñ + ùëéùëó, quindi avr√≤ una equazione di questo tipo: (ùëéùëñ1 + ùëéùëó1)ùë•1 +‚ãØ+ (ùëéùëñùëõ + ùëéùëóùëõ) = ùëèùëñ + ùëèùëó
quindi cambier√† solo l‚Äôequazione j-esima che comunque √® scritta come somma di due equazioni,
praticamente sommo un equazione membro a membro, il che non mi cambia le soluzioni del sistema. Cos√¨
abbiamo dimostrato che ùëÜ ‚äÜ ùëÜ‚Ä≤, il viceversa ùëÜ‚Ä≤ ‚äÜ ùëÜ √® evidente poich√© se partiamo dal nostro sistema
possiamo invertire le operazioni ùê∏ùëñ tramite operazioni inverse per ritrovarci sempre il sistema ùëÜ. Quindi
verificato che le operazioni sulle righe non influiscono sulle equazioni abbiamo anche dimostrato che un
sistema √® equivalente ad un sistema a gradini poich√© per ogni matrice esiste una matrice a gradini ad essa
equivalente.
La precedente preposizione ci aiuta a risolvere sistemi complessi poich√© in un sistema a gradini le soluzioni
sono dirette; quindi, dovr√≤ solo trovarmi la matrice a gradini della nostra matrice completa.
Soluzioni di un sistema lineare
Sia ùëö il numero di equazioni ed ùëõ il numero di incognite di un sistema ùëÜ, si possono definire i seguenti casi:
‚Ä¢
ùëö = ùëõ = 1: ùëé11ùë•1 = ùëè1 {
ùëé11 = ùëè1 = 0: ùëíùëûùë¢ùëéùëßùëñùëúùëõùëí ùëñùëëùëíùëõùë°ùëñùëêùëé ‚áí ùëñùëõùëìùëñùëõùëñùë°ùëí ùë†ùëúùëôùë¢ùëßùëñùëúùëõùëñ
ùëé11 ‚â† 0 ‚áí ùë•1 = ùëé11
‚àí1ùëè1
ùëé11 = 0 ùëí ùëè1 ‚â† 0 ùëõùëúùëõ ùëíùë†ùëñùë†ùë°ùëúùëõùëú ùë†ùëúùëôùë¢ùëßùëñùëúùëõùëñ
‚Ä¢ ùëö = 1 ùëí ùëõ > 1: ùëé11ùë•1 +‚ãØ+ ùëé1ùëõùë•ùëõ = ùëè1 {
ùëé11 = ‚ãØ = ùëé1ùëõ = ùëè1 = 0 ùëíùëûùë¢ùëéùëßùëñùëúùëõùëñ ùëñùëëùëíùëõùë°ùëñùëêùëé
ùëé11 = ‚ãØ = ùëé1ùëõ = 0 ùëí ùëè1 ‚â† 0 ùëõùëúùëõ ùëíùë†ùëñùë†ùë°ùëúùëõùëú ùë†ùëúùëôùë¢ùëßùëñùëúùëõùëñ
in
questo caso, se esiste un elemento diverso da zero, supponiamo ùëé11 posso scrivere il mio sistema come
ùëé11ùë•1 = ùëè1 ‚àí ùëé12ùë•2 ‚àí‚ãØ‚àí ùëé1ùëõùë•ùëõ dove poi mi √® permesso scegliere un valore ùë¶ùëñ per ogni ùë•2, ‚Ä¶ , ùë•ùëõ con
cui avr√≤ che ‚àÉ! ùë¶1: (ùë¶1, ùë¶2, ‚Ä¶ , ùë¶ùëõ) ùë†ùëñùëé ùë†ùëúùëôùë¢ùëßùëñùëúùëõùëí ùëëùëñ ùëÜ, in questo frangente si dice che il sistema lineare
ammetta ‚àû‚àí 1 soluzioni (perch√© faccio ùëõ ‚àí 1 scelte arbitrarie, l‚Äôaltra dipende da queste).
7

‚Ä¢ ùëö, ùëõ > 1: ùëéùë£ùëü√≤ ùë¢ùëõ ùë†ùëñùë†ùë°ùëíùëöùëé ùëÜ: {
ùëé11ùë•1 +‚ãØ+ ùëé1ùëõùë•ùëõ = ùëè1
‚ãÆ
ùëéùëö1ùë•1 +‚ãØ+ ùëéùëöùëõùë•ùëõ = ùëèùëõ
Elimino da ùëÜ‚Ä≤ le equazioni identiche 0 = 0, quindi ottengo ùëù equazioni non identiche dove ùëù coincide con
il numero di pivot della matrice completa di ùëÜ‚Ä≤. Cos√¨ facendo otteniamo un sistema ùëÜ‚Ä≤‚Ä≤ che supponiamo
ancora avere ùëö equazioni ed ùëõ incognite, cos√¨ facendo potr√≤ avere i seguenti casi (chiamiamo ùëÜ‚Ä≤‚Ä≤
semplicemente ùëÜ poich√© hanno le stesse soluzioni):
o ùëÜ contiene equazioni del tipo 0 = ùëèùêº(‚â† 0) allora il nostro sistema √® incompatibile (non ammette
soluzioni); in questo caso la matrice incompleta ha l‚Äôultima riga tutta nulla
o La matrice incompleta non ha l‚Äôultima riga nulla dove, sia # = ùëõùë¢ùëöùëíùëüùëú ùëëùëñ, allora avr√≤ #ùëùùëñùë£ùëúùë° =
#ùëíùëûùë¢ùëéùëßùëñùëúùëõùëñ ‚â§ #ùëñùëõùëêùëúùëîùëõùëñùë°ùëí(= #ùëêùëúùëôùëúùëõùëõùëí), questo quindi √® il caso ùëö ‚â§ ùëõ che si pu√≤
ulteriormente suddividere in:
‚ñ™ ùëö = ùëõ: qui la matrice ha un numero di righe uquale a quello della colonne, quindi l‚Äôunica
possibilit√† di trovare delle soluzioni sia quello in cui non abbia zeri sulla diagonale
principale, altrimenti per assurdo mi trover√≤ con una riga di elementi tutti nulli. Avr√≤
come soluzione: ùë•ùëõ =
ùëèùëõ
ùëéùëõùëõ
= ùëèùëõùëéùëõùëõ
‚àí1 che sostituisco nell‚Äôequazione precedente, cos√¨
potr√≤ esplicitare il valore di ùëéùëõ‚àí1 ed avr√≤ l‚Äôequazione ùëéùëõ‚àí1 ,ùëõ‚àí1 ùë•ùëõ‚àí1 + ùëéùëõ‚àí1ùë•ùëõ = ùëèùëõ‚àí1,
iterando questo procedimento avr√≤ tutte le soluzioni e quindi ùëÜ √® determinato.
‚ñ™ ùëö < ùëõ: significa che ci sono pi√π incognite che equazioni, in questo caso si prende la prima
riga e di questa andiamo a prendere il primo elemento da sinistra diverso da zero, che si
trova nella posizione i-esima, dunque avr√≤: ùëé1ùëñùë•ùëñ +‚ãØùëé1ùëõùë•ùëõ = ùëè1, questo procedimento
lo iteriamo per tutte le righe restanti prendendo oltre al primo elemento non nullo anche
il suo indice di colonna. Questi indici di colonna sono associati anche alle incognite
corrispondenti. Le incognite che non sono state prese andranno a destra dell‚Äôequazione,
con i termini noti (ad esempio: ùëé11ùë•1 +‚ãØ+ ùëé1ùëùùë•ùëù = ùëè1 ‚àí ùëé1(ùëù+1)ùë•ùëù+1 +‚ãØ‚àí ùëé1ùëõùë•ùëõ).
In questo modo, per ogni scelta di (ùë•ùëù+1,‚Ä¶ , ùë•ùëõ) avr√≤ una sola soluzione e quindi mi
trover√≤ in un sistema dove il numero di equazioni √® pari al numero di incognite e di
conseguenza posso procedere con il metodo precedente dove ùëö = ùëõ. Quindi avr√≤ un
insieme di soluzioni dove alcune incognite sono funzioni delle altre (ùëÜ √® indeterminato).
ESEMPI:
‚Ä¢ ùëÜ: {
ùë•1 ‚àí ùë•2 + 3ùë•3 = 1
ùë•1 + ùë•2 = 4
2ùë•1 + 2ùë•2 + 2ùë•3 = 9
ùëÜ‚Ä≤: {
‚Ä¢ ùëÜ: {
ùë•1 ‚àí ùë•2 + 3ùë•3 = 1
2ùë•2 ‚àí 3ùë•3 = 3
2ùë•3 = 1
2ùë•1 + ùë•2 = 1
ùë•1 + ùë•2 + ùë•3 ‚àí ùë•4 = 2
ùë•1 ‚àí ùë•3 + ùë•4 = 1
{
ùë•1 + ùë•2 + ùë•3 ‚àí ùë•4 = 2
ùë•2 + 2ùë•3 ‚àí 2ùë•4 = 3
0 = 2
{
‚áí ùê¥ = (
1 ‚àí1 3
1 1 0
2 2 2
ùë•1 = 7 4‚ÅÑ
ùë•2 = 9 4‚ÅÑ
ùë•3 = 1 2‚ÅÑ
‚áí ùëÜ = {(
7
4 ,
‚Üí (
2 1 0
1 1 1
1 0 ‚àí1
1
4
9
) ‚Üí     (
ùê∏4
‚àí2.2.3
1 ‚àí1 3
1 1 0
0 0 2
1
4
1
)‚Üí    (
ùê∏4
‚àí1.1.2
1 ‚àí1 3
0 2 ‚àí3
0 0
2
9
4 ,
1
2)} ; ùëÜ √® ùë¢ùëõ ùë†ùëñùë†ùë°ùëíùëöùëé ùëëùëíùë°ùëíùëüùëöùëñùëõùëéùë°ùëú
0 1
‚àí1 2
1 1
) ‚Üí (
1 1 1
0 1 2
0 0 0
‚áí ùëÜ √® ùë¢ùëõ ùë†ùëñùë†ùë°ùëíùëöùëé ùëñùëõùëêùëúùëöùëùùëéùë°ùëñùëèùëñùëôùëí (0 = 2)
‚Ä¢ ùëÜ: {
2ùë•1 ‚àí ùë•2 + ùë•3 ‚àí ùë•4 = 1
ùë•3 + ùë•4 = 1
ùë•3 = ‚àí2
{
2ùíôùüè ‚àí ùë•2 + ùë•3 ‚àí ùë•4 = 1
ùíôùüë + ùë•4 = 1
ùíôùüí = ‚àí2
‚Üí (
2 ‚àí1 1
0 0 1
0 0 1
‚àí1 1
1
0 ‚àí2
1 ) ‚Üí (
2 ‚àí1 1
0 0 1
0 0 0
{
2ùíôùüè + ùë•3 ‚àí ùë•4 = 1 + ùë•2
ùíôùüë + ùë•4 = 1
ùíôùüí = ‚àí2
‚Üí ùëÜ = {(
‚àí1 1
1 1
1 3
) ‚Üí
cs
uN
ùë¶
2 ‚àí 2, ùë¶, ‚àí2,3) |ùë¶ ‚àà ‚Ñù}
ùëÜ √® ùë¢ùëõ ùë†ùëñùë†ùë°ùëíùëöùëé ùëñùëõùëëùëíùë°ùëíùëüùëöùëñùëõùëéùë°ùëú ùëêùëúùëõ ‚àûùíä ùíîùíêùíçùíñùíõùíäùíêùíèùíä ùíÑùíêùíè ùíä ùíèùíñùíéùíÜùíìùíê ùíÖùíä ùíäùíèùíÑùíêùíàùíèùíäùíïùíÜ, ùëûùë¢ùëñùëõùëëùëñ ‚àû1 ùë†ùëúùëôùë¢ùëßùëñùëúùëõùëñ
8
‚àí1 2
‚àí2 3
0 2
) ‚Üí
1
3
1
) ‚áí
ùëíùëûùë¢ùëñùë£ùëéùëôùëíùëõùë°ùëí
‚áî         ùëÜ‚Ä≤ùëé ùëîùëüùëéùëëùëñùëõùëñ

Sistema omogeneo
Per sistema omogeneo si intende un sistema che ha come termini noti tutti zeri, questa tipologia di sistemi √®
sempre compatibile poich√© ammette o la soluzione banale (0, ‚Ä¶ ,0), oppure infinite soluzioni (tra cui anche
quella banale), quindi se ho una soluzione diversa da quella banale allora ne avr√≤ infinite.
Notazioni per i sistemi lineari
Posso denotare un sistema lineare con la seguente rappresentazione compatta: ùê¥ùëã = ùêµ, infatti
( ‚ãÆ ‚ã± ‚ãÆ )(
ùëé11 ‚ãØ ùëé1ùëõ
ùëéùëö1 ‚ãØ ùëéùëöùëõ
\f \ f
/\ J
ùë•1
‚ãÆ
ùë•ùëõ
) = (
ùëé11ùë•1 +‚ãØ+ ùëé1ùëõùë•ùëõ
‚ãÆ
\ùëéùëö1ùë•1 +‚ãØ+ ùëéùëöùëõùë•ùëõ
) = (
ùëè1
‚ãÆ
ùëèùëõ
); di conseguenza ùëå = (
ùë¶1
‚ãÆ
ùë¶ùëõ
) √® soluzione del
sistema se verifica l‚Äôuguaglianza ùê¥ùëå = ùêµ. Ovviamente un sistema √® omogeneo se ùê¥ùëã = ùêµ = 0.
3. Spazi Vettoriali
Definizione
Sia ùëâ un insieme non vuoto, +: ùëâ √ó ùëâ ‚Üí ùëâ un operazione interna (se indico un operazione interna con + √®
perch√© ho la commutativit√†) e ‚ãÖ ‚à∂ ‚Ñù √ó ùëâ ‚Üí ùëâ un operazione esterna; la terna (ùëΩ,+, ‚ãÖ
) √® detta spazio
vettoriale sul campo ‚Ñù se:
1) (ùëâ, +) √® un gruppo abeliano, ovvero gode di
i.
Propriet√† associativa ((ùëé + ùëè) + ùëê = ùëé + (ùëè + ùëê))
ii.
iii.
Propriet√† commutativa (ùëé + ùëè = ùëè + ùëé)
Elemento neutro(‚àÉùúÄ ‚àà ùëâ(‚àÄùëé ‚àà ùëâ, ùúÄ + ùëé = ùëé = ùëé + ùúÄ))
iv. Opposto (‚àÄùëé ‚àà ùëâ, ‚àÉ! ùëè(ùëé + ùëè = ùúÄ))
2) ‚àÄ‚Ñé, ùëò ‚àà ‚Ñù, ‚àÄùë£ ‚àà ùëâ risulta (‚Ñéùëò) ‚ãÖ ùë£ = ‚Ñé ‚ãÖ (ùëòùë£) (da non confondere con la propriet√† associativa,
poich√© anche se simile ‚ãÖ √® un operazione esterna).
3) ‚àÄùë£ ‚àà ùëâ risulta 1 ‚ãÖ ùë£ = ùë£
4) Distributivit√† di ‚ãÖ rispetto a + in ‚Ñù: ‚àÄ‚Ñé, ùëò ‚àà ‚Ñù, ‚àÄùë£ ‚àà ùëâ risulta (‚Ñé + ùëò)ùë£ = ‚Ñéùë£ + ùëòùë£
5) Distributivit√† di ‚ãÖ rispetto a + in ùëâ: ‚àÄ‚Ñé ‚àà ‚Ñù, ‚àÄùë£, ùë§ ‚àà ùëâ risulta ‚Ñé(ùë£ + ùë§) = ‚Ñéùë£ + ‚Ñéùë§
Nello spazio vettoriale cos√¨ definito si hanno le seguenti notazioni:
‚Ä¢ Gli elementi di ùëâ si dicono vettori
‚Ä¢ Gli elementi di ‚Ñù si dicono scalari
‚Ä¢ + √® detta addizione tra vettori
‚Ä¢ ‚ãÖ √® detta moltiplicazione di uno scalare per un vettore
Proposizione: Sia (ùëâ, +,‚ãÖ) uno spazio vettoriale sul campo ‚Ñù allora:
1) ‚àÉ! Elemento neutro rispetto a + (lo indicheremo con il simbolo 0)
Dim.: siano ùë£0 e ùë§0 elementi neutri allora avrei ùë£0 = ùë£0 + ùë§0 = ùë§0
2) ‚àÄùë£ ‚àà ùëâ, ‚àÉ! opposto per ùë£ (che indicheremo con ‚àíùë£)
Dim.: Siano ùë£‚Ä≤ e ùë£‚Ä≤‚Ä≤ opposti di ùë£ ho: ùë£‚Ä≤ = ùë£‚Ä≤ + 0 = ùë£‚Ä≤ + (ùë£ + ùë£‚Ä≤‚Ä≤) = (ùë£‚Ä≤ + ùë£) + ùë£‚Ä≤‚Ä≤ = 0 + ùë£‚Ä≤‚Ä≤ = ùë£‚Ä≤‚Ä≤
Possiamo definire ora le seguenti notazioni:
‚Ä¢ La scrittura ùë£ ‚àí ùë§ √® un‚Äôabbreviazione di ùë£ + (‚àíùë§)
‚Ä¢ Vettore nullo: 0
‚Ä¢ L‚Äôopposto del vettore nullo ‚àí0 non √® altro che 0
Esempi di spazi vettoriali
1) Spazio vettoriale numerico di ordine ùëõ: (‚Ñùùëõ,+,‚ãÖ)
‚Ä¢ ‚ãÖ ‚à∂ ‚Ñù √ó ‚Ñùùëõ ‚Üí ‚Ñùùëõ (‚Ñé, ùë£ = (ùë£1, ‚Ä¶ , ùë£ùëõ)) ‚Ü¶ (‚Ñéùë£1,‚Ä¶ , ‚Ñéùë£ùëõ)
‚Ä¢ +: ‚Ñùùëõ √ó ‚Ñùùëõ ‚Üí ‚Ñùùëõ ((ùë£ùëñ), (ùë§ùëñ)) ‚Ü¶ (ùë£ùëñ + ùë§ùëñ)
9

2) Spazio vettoriale di una matrice di ordine ùëö, ùëõ: (‚Ñùùëö,ùëõ,+,‚ãÖ)
‚Ä¢ ‚ãÖ ‚à∂ ‚Ñù √ó ‚Ñùùëö,ùëõ ‚Üí ‚Ñùùëö,ùëõ
(‚Ñé, (ùëéùëñùëó)) ‚Ü¶ (‚Ñéùëéùëñùëó)
‚Ä¢ +: ‚Ñùùëö,ùëõ √ó ‚Ñùùëö,ùëõ ‚Üí ‚Ñùùëö,ùëõ ((ùëéùëñùëó), (ùëèùëñùëó)) ‚Ü¶ (ùëéùëñùëó + ùëèùëñùëó)
3) Spazio vettoriale dei polinomi ad una indeterminante ùë• sul campo reale: (‚Ñù[ùíô], +,‚ãÖ) dove:
‚Ä¢ ‚Ñù[ùë•] = {ùëé0 + ùëé1ùë• + ùëé2ùë•2 +‚ãØ+ ùëéùëõùë•ùëõ|ùëéùëñ ‚àà ‚Ñù, ùëõ ‚àà ‚Ñï}
‚Ä¢ ‚ãÖ ‚à∂ ‚Ñù √ó ‚Ñù[ùë•] ‚Üí ‚Ñù[ùë•]
(‚Ñé, ùëé0 + ùëé1ùë• +‚ãØ+ ùëéùëõùë•ùëõ) ‚Ü¶ ‚Ñéùëé0 +‚ãØ+ (‚Ñéùëéùëõ)ùë•ùëõ
‚Ä¢ + ‚à∂ ‚Ñù[ùë•] √ó ‚Ñù[ùë•] ‚Üí ‚Ñù[ùë•]
( ùëé0 +‚ãØ+ ùëéùëõùë•ùëõ, ùëè0 +‚ãØ+ ùëèùëöùë•ùëö ) ‚Ü¶ (ùëé0 + ùëè0) + (ùëé1 +
ùëè1)ùë• +‚ãØ+ (ùëéùëõ + ùëèùëõ)ùë•ùëõ + ùëèùëõ+1ùë•ùëõ+1 +‚ãØ+ ùëèùëöùë•ùëö
assumendo che ùëõ ‚â§ ùëö
Principio di identit√† dei polinomi:
ùëé0 + ùëé1ùë• + ùëé2ùë•2 +‚ãØ+ ùëéùëõùë•ùëõ = ùëè0 + ùëè1ùë• +‚ãØ+ ùëèùëöùë•ùëö ‚áî ùëõ = ùëö ùëí ùëé0 = ùëè0,‚Ä¶ , ùëéùëõ = ùëèùëõ
Osservazione:
‚Ä¢ ùëù(ùë•), ùëû(ùë•) polinomi di gradi ‚â§ ùëõ allora ùëù(ùë•) + ùëû(ùë•) ha grado al pi√π ùëõ
(Esempio: (3ùë• + 1) + (4ùë•2 + 1) ‚â§ 2)
‚Ä¢ ‚Ñé ‚àà ‚Ñù, ‚Ñéùëù(ùë•) ha grado ‚â§ ùëõ pi√π precisamente ha lo stesso grado di ùëù(ùë•) eccetto se moltiplicato
per il polinomio nullo. (Esempio: ‚Ñé(3ùë• + 1) = (3‚Ñé)ùë• + ‚Ñé)
Queste descritte sopra sono sottostrutture di ‚Ñù[ùë•], quindi possiamo definire ora un insieme ‚Ñùùëõ[ùë•] =
{ùëù(ùë•) ‚àà ‚Ñù[ùë•]|ùëîùëüùëéùëëùëú(ùëù(ùë•)) ‚â§ ùëõ} dove + e ‚ãÖ sono operazioni ben definite una volta ristretto dominio e
codominio (in questo caso tutte le propriet√† descritte per ‚Ñù[ùë•] si mantengono anche per le sue
sottostrutture).
Avremo dunque un nuovo spazio vettoriale: (‚Ñùùíè[ùíô], +,‚ãÖ):
‚Ä¢ ‚ãÖ ‚à∂ ‚Ñù √ó ‚Ñùùëõ[ùë•] ‚Üí ‚Ñùùëõ[ùë•]
‚Ä¢ +: ‚Ñùùëõ[ùë•] √ó‚Ñùùëõ[ùë•] ‚Üí ‚Ñùùëõ[ùë•]
ESERCIZI: verificare per tutti gli esempi sopracitati che valgano le propriet√† di definizione di spazio vettoriale
Spazio della geometria euclidea ùëÜ (nel caso si parli di un piano si usa ùëÜùúã): Sia ùëÇ un punto di tale spazio
definiamo i seguenti oggetti:
‚Ä¢ ùëÇùê¥‚Éó‚Éó‚Éó‚Éó‚Éó  segmento orientato di primo estremo ùëÇ e secondo ùê¥
‚Ä¢ L‚Äôinsieme di tutti i segmenti orientati che partono da O e
si dirigono in punti arbitrari dello spazio S √® ùëÜ0 = {ùëÇùê¥|ùê¥ ‚àà ùëÜ}
A
O
‚Ä¢ Sia ùëÇùê¥‚Éó‚Éó‚Éó‚Éó‚Éó , ùëÇùêµ‚Éó‚Éó‚Éó‚Éó‚Éó  ‚àà ùëÜùëÇ definisco ùëÇùê¥‚Éó‚Éó‚Éó‚Éó‚Éó  + ùëÇùêµ‚Éó‚Éó‚Éó‚Éó‚Éó  = ùëÇùê∂‚Éó‚Éó‚Éó‚Éó‚Éó  con la regola del parallelogrammo:
‚Ä¢ Definisco ùêµùê∂‚Éó‚Éó‚Éó‚Éó‚Éó  vettore equipollente di ùëÇùê¥‚Éó‚Éó‚Éó‚Éó‚Éó  se e solo se hanno direzione, modulo e verso equivalenti
4) Spazio vettoriale dei vettori geometrici applicati in un punto: (ùëÜ0,+,‚ãÖ):
‚Ä¢ +: ùëÜùëÇ √ó ùëÜùëÇ ‚Üí ùëÜùëÇ
(regola del parallelogrammo)
‚Ä¢ ‚ãÖ ‚à∂ ‚Ñù √ó ùëÜùëÇ ‚Üí ùëÜùëÇ
(‚Ñé, ùëÇùê¥‚Éó‚Éó‚Éó‚Éó‚Éó ) ‚Ü¶ ‚ÑéùëÇùê¥‚Éó‚Éó‚Éó‚Éó‚Éó 
Esempio:
‚Ä¢ Elemento neutro ùëÇùëÇ‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó 
‚Ä¢ Elemento opposto fornito dal vettore con stessa direzione e modulo ma verso opposto
Spazio vettoriale dei vettori geometrici liberi (ordinari) ùëÜ: diamo le seguenti definizioni:
‚Ä¢ definisco vettore geometrico libero l‚Äôinsieme [ùê¥ùêµ‚Éó‚Éó‚Éó‚Éó‚Éó ] = {ùê∂ùê∑‚Éó‚Éó‚Éó‚Éó‚Éó |ùê∂ùê∑‚Éó‚Éó‚Éó‚Éó‚Éó  ùëíùëûùë¢ùëñùëùùëúùëôùëôùëíùëõùë°ùëí ùëéùëë ùê¥ùêµ‚Éó‚Éó‚Éó‚Éó‚Éó }
‚Ä¢ prender√† il nome di insieme quoziente ùí± = {[ùê¥ùêµ‚Éó‚Éó‚Éó‚Éó‚Éó ]|ùê¥, ùêµ ‚àà ùëÜ} (al variare di ùê¥, ùêµ in ùëÜ)
‚Ä¢ [ùê¥ùêµ‚Éó‚Éó‚Éó‚Éó‚Éó ] = ùê¥ùêµ, quindi l‚Äôelemento neutro sar√† il vettore nullo ùëÇ mentre l‚Äôopposto ‚àíùê¥ùêµ = [‚àíùê¥ùêµ‚Éó‚Éó‚Éó‚Éó‚Éó ]
5) Spazio vettoriale dell‚Äôinsieme (ùí±, +,‚ãÖ)
‚Ä¢ +: ùí± √ó ùí± ‚Üí ùí±
ùê¥ùêµ + ùê∂ùê∑ = ùëÇùê∏
‚Ä¢ ‚ãÖ ‚à∂ ‚Ñù √ó ùí± ‚Üí ùí±
‚Ñé ‚ãÖ ùê¥ùêµ = ‚Ñéùê¥ùêµ
10
fr NZ N
‚Äò
‚Äò

Propriet√† degli spazi vettoriali
Sia ùëâ uno spazio vettoriale su ‚Ñù: allora ‚àÄùíó, ùíò, ùíõ ‚àà ùëΩ e ‚àÄùíâ, ùíå ‚àà ‚Ñù avremo
1) ùë£ + ùë§ = ùëß ‚áí ùë£ = ùëß ‚àí ùë§
Si dimostra semplicemente sommando ambo i membri per l‚Äôopposto di ùë§
2) ùë£ + ùë§ = ùë§ ‚áí ùë£ = ùë§ ‚àí ùë§ = 0
3) 0 ‚ãÖ ùë£ = 0 = ‚Ñé0
0ùë£ = (0 + 0)ùë£ = 0 ‚ãÖ ùë£ + 0 ‚ãÖ ùë£ ‚áí 0ùë£ = 0 in simil modo ‚Ñé ‚ãÖ 0 = ‚Ñé ‚ãÖ (0 + 0) = ‚Ñé0 + ‚Ñé0 ‚áí ‚Ñé0 = 0
4) ‚Ñéùë£ = 0 ‚áî ‚Ñé = 0 ùëú ùë£ = 0
Dim.‚áê valida per la propriet√† 3; ‚áí: supponiamo ‚Ñé ‚â† 0 quindi ‚Ñéùë£ = 0 ‚áí ‚Ñé‚àí1(‚Ñéùë£) = ‚Ñé‚àí10 ‚áí
(‚Ñé‚àí1‚Ñé)ùë£ = 0 sempre per la propriet√† 3, se ‚Ñé = 0 la dimostrazione √® banale.
5) ‚Ñé(‚àíùë£) = ‚àí(‚Ñéùë£) = (‚àí‚Ñé)ùë£
Dimostriamo che ‚Ñé(‚àíùë£) √® l‚Äôopposto di ‚Ñéùë£ quindi bisogna dimostrare che ‚Ñéùë£ + ‚Ñé(‚àíùë£) = 0, dunque
mettendo ‚Ñé in evidenzia ‚Ñé (ùë£ + (‚àíùë£)) = ‚Ñé(ùë£ ‚àí ùë£) = ‚Ñé ‚ãÖ 0 = 0; quindi abbiamo dimostrato la prima
uguaglianza, similmente per la seconda dobbiamo dimostrare che (‚àí‚Ñé)ùë£ sia l‚Äôopposto di ‚Ñéùë£, per
uno degli assiomi dello spazio vettoriale posso scrivere (‚àí‚Ñé)ùë£ + ‚Ñéùë£ = (‚àí‚Ñé + ‚Ñé)ùë£ = 0ùë£ = 0
6) (‚àí1)ùë£ = ‚àí(1ùë£) = ‚àíùë£
Applicando pi√π volte la propriet√† 5: (‚àí‚Ñé)(‚àíùë£) = ‚àí[‚Ñé(‚àíùë£)] = ‚àí[‚àí‚Ñéùë£] = ‚Ñéùë£
L‚Äôopposto dell‚Äôopposto √® il vettore di partenza ‚àí(‚àíùë£) = ùë£
ùë£ + (‚àíùë£) = 0
7) Propriet√† associativa generalizzata ùë£1, ‚Ä¶ , ùë£ùëõ: essendo le somme sempre suddivise in agglomerati di
tre oggetti posso estendere la propriet√† associativa (ùë£1 + ùë£2) + ùë£3 = ùë£1 + (ùë£2 + ùë£3) per ùëõ elementi
poich√© il posizionamento delle parentesi non conta: ùë£1 + ùë£2 +‚ãØ+ ùë£ùëõ (quindi posso ometterle)
8) Propriet√† commutativa generalizzata: come in precedenza essendo ùë£ + ùë§ = ùë§ + ùë£ possiamo
generalizzarla per ùëõ elementi poich√© l‚Äôordine degli addendi non conta.
9) Propriet√† distributiva generalizzata: (‚Ñé1 + ‚Ñé2 +‚ãØ+ ‚Ñéùëõ)ùë£ = ‚Ñé1ùë£ + ‚Ñé2ùë£ +‚ãØ+ ‚Ñéùëõùë£
Essendo vera per ùëõ = 2 (√® assioma), supponiamola vera per ùëõ(‚â• 2) e dimostriamola vera per ùëõ + 1:
(‚Ñé1 + ‚Ñé2 +‚ãØ+ ‚Ñéùëõ + ‚Ñéùëõ+1)ùë£ = (‚Ñé1 +‚ãØ+ ‚Ñéùëõ)ùë£ + ‚Ñéùëõ+1ùë£ = ‚Ñé1ùë£ +‚ãØ+ ‚Ñéùëõùë£ + ‚Ñéùëõ+1ùë£ stessa cosa
(dimostrare come esercizio) per:
‚Ñé(ùë£1 + ùë£2 +‚ãØ+ ùë£ùëõ) = ‚Ñéùë£1 + ‚Ñéùë£2 +‚ãØ+ ‚Ñéùë£ùëõ
Definizioni
Combinazione lineare: Presi ‚Ñé1, . . , ‚Ñéùëõ ‚àà ‚Ñù , ùë£1,‚Ä¶ , ùë£ùëõ, ùë£ ‚àà ùëâ e se √® possibile scrivere ùë£ = ‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùëõùë£ùëõ
allora ùë£ sar√† detto combinazione lineare dei vettori ùë£ùëñ mediante gli scalari ‚Ñéùëñ.
Esempio in ‚Ñù3: (3,0,1) = 3(1,0,0) + 1(0,0,1), combinazione lineare di (1,0,0) e (0,0,1) mediante 3 e 1
Proporzionalit√†: Siano ùë£, ùë§ ‚àà ùëâ si dicono proporzionali se ‚àÉ‚Ñé ‚â† 0 ‚àà ‚Ñù tale che ùë£ = ‚Ñéùë§
La proporzionalit√† √® una relazione d‚Äôequivalenza essendo:
1) riflessiva: ùë£ = 1 ‚ãÖ ùë£
2) simmetrica: ùë£ = ‚Ñéùë§ con ‚Ñé ‚â† 0 ‚áí ùë§ = ‚Ñé‚àí1ùë£
3) transitiva: sia ‚Ñé, ùëò ‚â† 0 se ùë£ = ‚Ñéùë§ e ùë§ = ùëòùë¢ allora ùë£ = (‚Ñéùëò)ùë¢
Sottospazi vettoriali
Sia (ùëâ, +,‚ãÖ) spazio vettoriale e sia ùêª ‚äÜ ùëâ diverso dal vuoto, ùêª √® detto stabile (o chiuso) rispetto all‚Äôaddizione
se ‚àÄùë£, ùë§ ‚àà ùêª ‚áí ùë£ + ùë§ ‚àà ùêª (la loro somma √® ancora in ùêª) mentre ùêª √® detto stabile (o chiuso) rispetto alla
moltiplicazione per uno scalare se ‚àÄ‚Ñé ‚àà ‚Ñù, ‚àÄùë£ ‚àà ùêª ‚áí ‚Ñéùë£ ‚àà ùêª. Se ùêª √® stabile rispetto alla moltiplicazione e
all‚Äôaddizione significa che le operazioni + ‚à∂ ùêª √ó ùêª ‚Üí ùêª e ‚ãÖ ‚à∂ ‚Ñù√ó ùêª ‚Üí ùêª sono ben definite.
Definizione: Se ùêª √® chiuso (o stabile)rispetto a somma e prodotto allora √® detto sottospazio vettoriale
Se ùêª √® sottospazio di ùëâ allora valgono per ùêª tutte le propriet√† dello spazio vettoriale ùëâ. Si indica con ùêª ‚â§ ùëâ.
Tutti i sottospazi sono anche sottoinsieme, ma il viceversa non √® vero (ad esempio ‚àÖ ‚â∞ ùëâ).
11

Teorema: Se ho un sottospazio vettoriale con le operazioni ristrette allora esso √® anche uno spazio vettoriale.
Sia ùêª ‚â§ ùëâ ‚üπ (ùêª, +ùêºùêª ,‚ãÖùêºùêª) √® spazio vettoriale.
Dimostrazione: (ùêª; +ùêºùêª) √® sia associativa che commutativa (praticamente tutte le propriet√† che non sono
esistenziali sono automaticamente soddisfatte per la stabilit√† dell‚Äôoperazione); ha lo stesso elemento neutro
di ùëâ e contiene anche gli opposti (ùë£ ‚àà ùêª ‚â† ‚àÖ ‚áí (‚àí1)ùë£ = ‚àíùë£ùêºùëâ ) che appartengono ad ùêª poich√© ho lo stesso
elemento neutro 0. 1 ‚ãÖùêºùêª
ùë£ = 1 ‚ãÖ ùë£ = ùë£ e cos√¨ via sono automaticamente verificate anche le altre propriet√†.
Esempi di sottospazi vettoriali
1) Sia ùëâ spazio vettoriale ha sempre i sottospazi banali: {0} e ùëâ
2) Spazio vettoriale ‚Ñù3: sono sottospazi {(0,0,0)} e ‚Ñù3
3) Spazio vettoriale ‚Ñù2: sottospazio ùêª = {(ùë•, ùë¶)|ùë• = ùë¶}, infatti non √® vuoto, √® stabile rispetto alla somma
poich√© ‚àÄùë£, ùë§ ‚àà ùêª posto ùë£ = (ùë•, ùë¶) ‚àà ùêª, ùë§ = (ùëß, ùë°) ‚àà ùêª ‚áí (ùë•, ùë¶) + (ùëß, ùë°) = (ùë• + ùëß, ùë¶ + ùë°) ‚àà ùêª essendo
infatti ùë• + ùëß = ùë¶ + ùëß = ùë¶ + ùë°; inoltre ùêª √® stabile anche rispetto al prodotto essendo ‚àÄ‚Ñé ‚àà ‚Ñù, ‚àÄùë£ ‚àà ùêª ‚áí
ùë£ = (ùë•, ùë¶) ‚àà ùêª ‚áí ‚Ñé(ùë•, ùë¶) = (‚Ñéùë•, ‚Ñéùë¶) ‚àà ùêª essendo ‚Ñéùë• = ‚Ñéùë¶ poich√© ùë• = ùë¶.
4) ùêª = {3ùë• + 1,0} ‚â∞ ‚Ñù[ùë•] infatti: (3ùë• + 1) + (3ùë• + 1) = 6ùë• + 2 ‚àâ ùêª
5) ùêª = {(ùë•, ùë¶, 0,0) ‚àà ‚Ñù4|ùë• = ùë¶2} ‚â∞ ‚Ñù4 poich√© (1,1,0,0) ‚àà ùêª + (1,1,0,0) ‚àà ùêª = (2,2,0,0) ‚àâ ùêª per 2 ‚â† 22
f
Esercizio: determinare se ùêª = {(ùëé
ùëê
ùëè
ùëë
) ‚àà ‚Ñù2| (ùëé
ùëê
ùëè
ùëë
) (1 1\
0 1J) = (1 1\7s
\0 17X
) (ùëé
ùëê
Soluzione: sviluppiamo il prodotto righe per colonne in ambo i due membri: (ùëé
‚Äò
\
ùëê
da cui possiamo ricavare le seguenti informazioni: { ùëê + ùëë = ùëë ‚áí ùëê = 0
&
ùëé + ùëè = ùëè + ùëë ‚áí ùëé = ùëë
come ùêª = {(ùëé
Cs
ùëè
uN
0 ùëé
ùëè
Ny
ùëëyd
)} √® sottospazio vettoriale.
ùëé + ùëè
ùëê + ùëë
) = (ùëé + ùëê
ùëê
ùëè + ùëë
ùëë
)
e quindi riscrivere l‚Äôinsieme ùêª
) |ùëé. ùëè ‚àà ‚Ñù}; ora resta da verificare che non sia vuoto (quindi si controlla che esista
NOI
a |
Y
J
l‚Äôelemento neutro rispetto la somma), ed in questo caso √® la matrice (0 0\
‚Äò
\0 0J) ‚àà ùêª, e che ùêª sia stabile rispetto
/
alla somma ed al prodotto, risulta infatti: (
\
ùëé1 ùëè1
0 ùëé1
‚Ñé (ùëé
f
ùëè
XN
0 ùëé
) = (‚Ñéùëé ‚Ñéùëè
N
7
f
XN
0 ‚Ñéùëé
\
/
) + (
/
\
ùëé2 ùëè2
0 ùëé2
\
/
) = (
J
\
ùëé1 + ùëé2 ùëè1 + ùëè2
0
ùëé1 + ùëé2
) ‚àà ùêª, ed anche
) ‚àà ùêª. Di conseguenza abbiamo dimostrato che ùêª √® sottospazio vettoriale.
\
7
Esempio: Spazio vettoriale dei vettori geometrici applicati in ùëÇ‚Üí
Supponendo di trovarci nel sottospazio vettoriale formato
dai vettori di ùëÇùê¥‚Éó‚Éó‚Éó‚Éó‚Éó , possiamo affermare che esso contiene
tutti i vettori situati nella retta passante per i punti ùëÇ ed ùê¥
(vedi figura a destra).
Lo stesso ragionamento si pu√≤ fare per un piano ùúã:
supponendo di trovarci nel sottospazio formato dai vettori
‚Éó ùëÇùê∂‚Éó‚Éó‚Éó‚Éó  e ùëÇùê¥‚Éó‚Éó‚Éó‚Éó‚Éó 
allora ad esso appartengono tutti quei vettori
contenuti nello stesso piano (figura in basso).
, { ‚Äòs. oR
Esune
‚Äò
/
/
\y
‚ÄúOo.
‚Äò\
‚Äò
\
$
MGAIY am
HoH,ZI
ly
Esempio 1
‚Äî_‚Äî_‚Äî_
t
a
hyo ely
Hy=4Coy) /yeR
Osservazione:
l‚Äôunione di
sottospazi vettoriali non √® in
genere un sottospazio.
L‚Äôesempio uno ci mostra,
geometricamente parlando, che
l‚Äôunione delle due rette non √®
sottospazio vettoriale, l‚Äôesempio
due √® un esempio pi√π concreto.
12

Alcune propriet√† e definizioni di sottospazi vettoriali (sottospazio generato)
1) ùëä ‚â§ ùëâ √® stabile rispetto alla somma di n oggetti:
Siano ùë§1,‚Ä¶ , ùë§ùëõ ‚àà ùëä si ha ùë§1 + ùë§2 ‚àà ùëä ‚áí (ùë§1 + ùë§2) + ùë§3 ‚àà ùëä iterando questo ragionamento
avremo che ùë§1 +‚ãØ+ ùë§ùëõ ‚àà ùëä. Da questa propriet√† possiamo osservare che presi ùëõ scalari, ‚Ñé1,‚Ä¶ , ‚Ñéùëõ,
qualunque combinazione lineare ùëä contiene ogni combinazione lineare dei suoi elementi:
‚Ñé1ùë§1,‚Ä¶ , ‚Ñéùëõùë§ùëõ ‚àà ùëä ‚áí ‚Ñé1ùë§1 +‚ãØ+ ‚Ñéùëõùë§ùëõ ‚àà ùëä
2) sia ‚Ñí una famiglia di sottospazi di ùëâ, l‚Äôintersezione dei sottospazi della famiglia ‚Äúelle tondo‚Äù: ‚ãÇ ùêø
ùêø‚àà‚Ñí √® un
sottospazio, pi√π precisamente l‚Äôintersezione di una qualunque famiglia di sottospazi √® un sottospazio ( a
differenza dell‚Äôunione). Esempio: {(0, ùë¶)|ùë¶ ‚àà ‚Ñù} ‚à© {(ùë•, 0)|ùë• ‚àà ‚Ñù} = {(0,0)} ‚â§ ‚Ñù2
Dimostrazione: l‚Äôelemento neutro (l‚Äôinsieme nullo) √® sempre contenuto in ogni intersezione, la somma √®
stabile; infatti, siano ùë£, ùë§ ‚àà ‚ãÇ ùêø
ùêø‚àà‚Ñí ‚áí ùë£, ùë§ ‚àà ùêø ‚àÄùêø ‚àà ‚Ñí ‚áí ùë£ + ùë§ ‚àà ùêø ‚àÄùêø ‚àà ‚Ñí ‚áí ùë£ + ùë§ ‚àà ‚ãÇ ùêø
ùêø‚àà‚Ñí , sia
invece, per il prodotto, ùë£ ‚àà ‚ãÇ ùêø
ùêø‚àà‚Ñí , ‚Ñé ‚àà ‚Ñù ‚áí ùë£ ‚àà ùêø ‚àÄùêø ‚àà ‚Ñí ‚áí ‚Ñéùë£ ‚àà ùêø ‚àÄùêø ‚àà ‚Ñí ‚áí ‚Ñéùë£ ‚àà ‚ãÇ ùêø
ùêø‚àà‚Ñí
Problema: Siano ùêª, ùêæ ‚â§ ùëâ. Qual √® il pi√π piccolo sottospazio che contiene sia ùêª che ùêæ? (i.c., ùêª ‚à™ ùêæ). Cerco
quindi un sottoinsieme contenete sia ùêª che ùêæ, ovvero ùëâ ‚äá ùêª ‚à™ ùêæ, prendo poi l‚Äôintersezione di tutti i
sottospazi ùêø di ùëâ che contengono ùêª ‚à™ ùêæ, √® questo insieme, chiamato sottospazio generato da ùêª e ùêæ, oltre
ad essere la risposta del problema √® anche il pi√π piccolo sottospazio che contiene sia ùêª che ùêæ.
In simboli: ‚å©ùêª, ùêæ‚å™ = ‚å©ùêª ‚à™ ùêæ‚å™ = ùêª + ùêæ = {‚Ñé + ùëò|‚Ñé ‚àà ùêª, ùëò ‚àà ùêæ}; ed √® il pi√π piccolo sottoinsieme, infatti:
1) 0 = 0 ‚àà ùêª + 0 ‚àà ùêæ; quini ha elemento neutro.
2) La somma √® chiusa poich√© presi ùë§, ùë§1 ‚àà ùêª + ùêæ posso scrivere che ‚àÉ‚Ñé, ‚Ñé1 ‚àà ùêª, ‚àÉùëò, ùëò1 ‚àà ùêæ tale che
ùë§ = ‚Ñé + ùëò e ùë§1 = ‚Ñé1 + ùëò1 allora ùë§ + ùë§1 = (‚Ñé + ùëò) + (‚Ñé1 + ùëò1) = (‚Ñé + ‚Ñé1)
+ (ùëò + ùëò1) ‚àà ùêª + ùêæ
‚èü    
‚àà ùêª
3) Il prodotto √® anch‚Äôesso chiuso, e si dimostra allo stesso modo: ‚àÄùë• ‚àà ‚Ñù
‚Ä¢ ùë•‚Ñé ‚àà ùêª ‚áí ùë•‚Ñé
‚àà ùêª
+ 0
‚àà ùêæ
‚Ä¢ ùëò ‚àà ùêª ‚áí 0
‚àà ùêª
+ ùë•ùëò
‚àà ùêæ
‚àà ùêª + ùêæ ‚áí ùêª ‚äÜ ùêª + ùêæ
‚àà ùêª + ùêæ ‚áí ùêæ ‚äÜ ùêª + ùêæ
4) resta da dimostrare che sia il pi√π piccolo (affinch√© sia sottospazio generato)
‚Ä¢ ùêø ‚â§ ùëâ ‚à∂ ùêª ‚à™ ùêæ ‚äÜ ùêø
‚àÄ‚Ñé ‚àà ùêª, ùëò ‚àà ùêæ ‚áí ‚Ñé, ùëò ‚àà ùêø ‚áí ‚Ñé + ùëò ‚àà ùêø ‚áí ùêª + ùêæ ‚äÜ ùêø ed √® quindi contenuto in tutti i
sottospazi che contengono l‚Äôunione di ùêª e ùêæ; verificando cos√¨ l‚Äôuguaglianza ‚å©ùêª, ùêæ‚å™ = ‚å©ùêª ‚à™ ùêæ‚å™
Esempio
Hye }ee) [xeAS H,=4ley)/yeRY
Hat, =) Colt Con)/ xR, yerR¬ß
nets 2 Gye li
(m=(0)+ (2,9) ‚Ç¨ Nyt Hy
‚Ç¨ My √© H,
Generalizzazione: Siano ùêª1,‚Ä¶ , ùêªùëõ sottospiazi, il sottospazio generato degli ùëõ sottospazi non √® altro che la
somma dei sottospazi, in simboli: ‚å©ùêª1,‚Ä¶ , ùêªùëõ‚å™ = ùêª1 +‚ãØ+ ùêªùëõ = {‚Ñé1 + ‚Ñé2 +‚ãØ+ ‚Ñéùëõ|‚Ñéùëñ ‚àà ùêªùëñ}
Osservazione: ùêª ‚â§ ùëâ ‚áí ‚å©ùêª‚å™ = ùêª, il pi√π piccolo sottospazio generato da ùêª √® ùêª stesso, allo stesso modo se
ho ùë£1, ‚Ä¶ , ùë£ùëõ ‚àà ùëâ il sottospazio generato ‚å©ùë£1,‚Ä¶ , ùë£ùëõ‚å™ = {‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùëõùë£ùëõ|‚Ñéùëñ ‚àà ‚Ñù} non √® altro che l‚Äôinsieme
di tutte le possibili combinazioni lineari dei vettori ùë£1,‚Ä¶ , ùë£ùëõ con gli scalari che variano in maniera arbitraria.
13
‚èü    
‚àà ùêæ

Somma diretta di sottospazi vettoriali
Sia ùëâ spazio vettoriale e ùêª1, ùêª2 ‚â§ ùëâ due sottospazi, ùêª1 + ùêª2 √® detta somma di ùêª1 e ùêª2 (generalizzabile per
ùëõ elementi ùêª1 +‚ãØùêªùëõ), la somma di due sottospazi si dice somma diretta se la loro intersezione (non vuota)
√® la pi√π piccola possibile, ovvero il singleton dell‚Äôelemento neutro, quindi √® somma diretta e si denota con
ùêª1‚®Åùêª2 se ùêª1 ‚à© ùêª2 = {0}. Ad esempio sia ùêª1 = {(ùë•, 0)|ùë• ‚àà ‚Ñù} e ùêª2 = {(0, ùë•)|ùë• ‚àà ‚Ñù}, l‚Äôinsieme ‚Ñù2 = ùêª1 +
ùêª2 √® somma diretta, infatti ùêª1 ‚à© ùêª2 = {(0,0)}, quindi si scriver√† ‚Ñù2 = ùêª1‚®Åùêª2.
ùêª1 e ùêª2 sono detti supplementari se ùëâ = ùêª1 + ùêª2, mentre saranno complementari se ùëâ = ùêª1‚®Åùêª2
Somma diretta di pi√π sottospazi: supponiamo di avere ùëõ spazi vettoriali, allora ùêª1,‚Ä¶ , ùêªùëõ sono in somma
diretta se l‚Äôintersezione di un qualunque ùêªùëñ con il sottospazio generato da tutti gli altri sia singleton
dell‚Äôelemento neutro, dunque se ùêªùëñ ‚à© ‚ü®ùêªùëó|ùëó ‚â† ùëñ‚ü© = {0}, pi√π precisamente se ùêª1 ‚à© ‚å©ùêª2, ùêª3,‚Ä¶ , ùêªùëõ‚å™ = {0};
ùêª2 ‚à© ‚å©ùêª1, ùêª3,‚Ä¶ , ùêªùëõ‚å™ = {0} e cos√¨ via per tutti gli ùêªùëñ.
Esempio 1: ùêª1 = {(ùë•, 0)|ùë• ‚àà ‚Ñù}, ùêª2 = {(0, ùë•)|ùë• ‚àà ‚Ñù} e ùêª3 = {(ùë•, ùë•)|ùë• ‚àà ‚Ñù} non sono in somma diretta
anche se presi a due a due generano l‚Äôelemento neutro, infatti ùêª3 ‚à© ‚å©ùêª1, ùêª2‚å™ = ùêª3 ‚à©‚Ñù2 = ùêª3 ‚â† {(0,0)}
Esempio 2: ùêª1 = {(ùë•, 0,0)|ùë• ‚àà ‚Ñù}, ùêª2 = {(0, ùë•, 0)|ùë• ‚àà ‚Ñù} e ùêª3 = {(0,0, ùë•)|ùë• ‚àà ‚Ñù} avremo che l‚Äôinsieme
ùêª1 ‚à© ‚å©ùêª2, ùêª3‚å™ = {(0,0,0)} dato che ‚å©ùêª2, ùêª3‚å™ = ùêª2 + ùêª3 = {(0, ùë•, ùë¶)|ùë•, ùë¶ ‚àà ‚Ñù} in modo analogo si procede
con gli altri insiemi e dunque possiamo scrivere ùêª1‚®Åùêª2‚®Åùêª3.
Esempio 3: ‚å©ùëü, ùë†‚å™ = ùëü + ùë† √® tutto il piano, inoltre sapendo che ogni vettore
geometrico si pu√≤ scrivere come somma di un elemento che sta in ùë† ed uno che sta
in ùëü, (i sottospazi ùë† e ùëü sono supplementari; la loro somma fa tutto lo spazio) e che la
loro intersezione √® intuitivamente il vettore degenere ùëÇùëÇ‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó , risulter√† ùëÖ ‚à© ùëÜ = {ùëÇùëÇ‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó }.
Di conseguenza √® anche complementare (l‚Äôintersezione √® insieme banale): ùëü‚®Åùë†.
Dipendenza ed indipendenza lineare
Sia ùëâ spazio vettoriale e siano ùë£1, ùë£2,‚Ä¶ , ùë£ùëõ ‚àà ùëâ, essi sono detti linearmente dipendenti (o legati) se e solo se
esistono ùëõ scalari non tutti nulli tali che la loro combinazione lineare sia il vettore nullo: ‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùëõùë£ùëõ = 0.
Se tali scalari non esistono allora tali vettori sono detti linearmente indipendenti ( o liberi), quest‚Äôultimo caso
implica come unica soluzione 0ùë£1 +‚ãØ+ 0ùë£ùëõ = 0, quindi che ‚Ñé1 = ‚Ñé2 = ‚ãØ = ‚Ñéùëõ = 0 ‚àÄ‚Ñéùëñ ‚àà ‚Ñù
Esempi: (1,0), (0,1) ‚àà ‚Ñù2 sono linearmente indipendenti infatti ‚Ñé1(1,0) + ‚Ñé2(0,1) = (0,0) ‚áí ‚Ñé1 = ‚Ñé2 = 0;
(0,0), (1,0) sono linearmente dipendenti : 3(0,0) + 0(1,0) = (0,0), l‚Äôelemento neutro implica la dipendenza
(1,0), (2,0): ‚àí 2(1,0) + 1(2,0) = (0,0), se ci sono due elementi proporzionali implica la dipendenza.
Osservazione: se un sistema di vettori contiene in s√© un sistema di vettori linearmente dipendenti allora
anche il sistema totale pi√π grande sar√† linearmente dipendente (metto lo scalare a 0 per i vettori non
contenuti nel sistema dipendente).
Diremo che un vettore ùë£ dipende da ùë£1, ùë£2, ‚Ä¶ , ùë£ùëõ per definizione, se ùë£ √® combinazione lineare dei ùë£ùëñ ovvero
se ‚àÉùíâùüè, ‚Ä¶ , ùíâùíè ‚àà ‚Ñù ‚à∂ ùíó = ùíâùüèùíóùüè + ùíâùíèùíóùíè. Di seguito alcune propriet√†:
‚Ä¢ 0 dipende sempre da qualunque sistema ùë£1,‚Ä¶ , ùë£ùëõ
0 = 0ùë£1 +‚ãØ+ 0ùë£ùëõ
‚Ä¢ ùë£ùëñ dipende sempre da ùë£1,‚Ä¶ , ùë£ùëõ
ùë£ùëñ = 0ùë£1 +‚ãØ+ 0ùë£ùëñ‚àí1 + 1ùë£ùëñ + 0ùë£ùëñ+1 +‚ãØ+ 0ùë£ùëõ
‚Ä¢ Sia ùë£ dipendente da ùë£1,‚Ä¶ , ùë£ùëõ e ciascun ùë£ùëñ dipendente da ùë§1,‚Ä¶ , ùë§ùëö, allora (una sorta di propriet√†
transitiva) ùë£ dipende da ùë§1,‚Ä¶ , ùë§ùëö
Dimostrazione: per ipotesi ‚àÉ‚Ñéùëñ ‚àà ‚Ñù ‚à∂ ùë£ = ‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùëõùë£ùëõ e, sempre per definizione, ‚àÉùëòùëñùëó ‚àà ‚Ñù ‚à∂
ùë£ùëñ = ùëòùëñ1ùë§1 +‚ãØ+ ùëòùëñùëöùë§ùëö dove ùë£ùëñ = ùë£1, ‚Ä¶ , ùë£ùëõ, di conseguenza posso scrivere il mio vettore ùë£ come
segue ùë£ = ‚Ñé1(ùëò11ùë§1 +‚ãØ+ ùëò1ùëöùë§ùëö) +‚ãØ+ ‚Ñéùëõ(ùëòùëõ1ùë§1 +‚ãØ+ ùëòùëõùëöùë§ùëö); utilizzando poi la propriet√†
distributiva generalizzata (mettendo in evidenza i ùë§ùëñ) avr√≤ ùë£ = ùëê1ùë§1 +‚ãØ+ ùëêùëöùë§ùëö
14

‚Ä¢ Se ùë£, ùë§ dipendono da ùë£1,‚Ä¶ , ùë£ùëõ allora ùë£ + ùë§ dipende da ùë£1,‚Ä¶ , ùë£ùëõ
Dimostrazione:
ùë£ = ‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùëõùë£ùëõ
ùë§ = ùëò1ùë£1 +‚ãØ+ ùëòùëõùë£ùëõ
‚áí ùë£ + ùë§ = (‚Ñé1 + ùëò1)ùë£1 +‚ãØ+ (‚Ñéùëõ + ùëòùëõ)ùë£ùëõ
‚Ä¢ ùë£ dipende da ùë£1,‚Ä¶ , ùë£ùëõ ‚áî ùë£ ‚àà ‚å©ùë£1, ‚Ä¶ , ùë£ùëõ‚å™
Per dimostrare la precedente equivalenza bisogna verificare che (come precedentemente osservato)
‚å©ùë£1,‚Ä¶ , ùë£ùëõ‚å™ = {‚Ñé1 ùë£1 +‚ãØ+ ‚Ñéùëõ ùë£ùëõ|‚Ñéùëñ ‚àà ‚Ñù}, a tal scopo dimostriamo che l‚Äôinsieme delle combinazioni
lineari ùêª = {‚Ñé1 ùë£1 +‚ãØ+ ‚Ñéùëõ ùë£ùëõ|‚Ñéùëñ ‚àà ‚Ñù} sia effettivamente un sottospazio: √® evidente che contenga
l‚Äôelemento neutro poich√© esso √® sempre dipendente da qualunque sistema, anche la stabilit√† della
somma √® evidente per la propriet√† precedente, in maniera simile si dimostra la stabilit√† rispetto al
prodotto, infatti se ùë£ dipende da ùë£1, ‚Ä¶ , ùë£ùëõ allora anche ‚Ñéùë£ = (‚Ñé‚Ñé1)ùë£1 +‚ãØ+ (‚Ñé‚Ñéùëõ)ùë£ùëõ dipender√† da
ùë£1,‚Ä¶ , ùë£ùëõ. Inoltre, ùêª contiene i vettori ùë£1,‚Ä¶ , ùë£ùëõ
infatti ùë£ùëñ dipende da ùë£1,‚Ä¶ , ùë£ùëõ; dunque abbiamo
dimostrato che ùêª oltre ad essere un sottospazio, contiene ùë£1,‚Ä¶ , ùë£ùëõ. Prendiamo ora un sottospazio
ùëä che contiene ‚å©ùë£1,‚Ä¶ , ùë£ùëõ‚å™, se ùëä contiene i vettori ùë£1, ‚Ä¶ , ùë£ùëõ allora esso deve contenere anche tutte
le sue combinazioni lineari e quindi ùêª ‚äÜ ùëä; abbiamo cos√¨ dimostrato che ùêª √® il pi√π piccolo
sottospazio che rispetto all‚Äôinclusione le contiene, ovvero che ùêª = ‚å©ùë£1, ‚Ä¶ , ùë£ùëõ‚å™.
Osservazioni:
‚Ä¢
ùëä1 = ‚å©ùë£1,‚Ä¶ , ùë£ùëõ‚å™
ùëä2 = ‚å©ùë§1,‚Ä¶ , ùë§ùëö‚å™
‚áí ùëä1 + ùëä2 = ‚å©ùë£1, ‚Ä¶ , ùë£ùëõ, ùë§1,‚Ä¶ , ùë§ùëö‚å™
Dimostrazione: ùëä1 + ùëä2 = {ùëé + ùëè | ùëé ‚àà ùëä1 ùëí ùëè ‚àà ùëä2} e sappiamo che ùëä1 + ùëä2 ‚äá ùëä1 ‚à™ ùëä2 =
‚å©ùë£1,‚Ä¶ , ùë£ùëõ‚å™ ‚à™ ‚å©ùë§1,‚Ä¶ , ùë§ùëö‚å™; l‚Äôunione dei due sottospazi generati sono contenuti dunque in ùëä1 + ùëä2
e quindi l‚Äôinsieme ùëä1 + ùëä2 contiene tutte le sue combinazioni lineari, ci√≤ vuol dire che contiene tutte
le combinazioni lineari di ùë£1,‚Ä¶ , ùë£ùëõ, ùë§1,‚Ä¶ , ùë§ùëö ovvero il sottospazio generato ‚å©ùë£1, ‚Ä¶ , ùë£ùëõ, ùë§1,‚Ä¶ , ùë§ùëö‚å™,
abbiamo cos√¨ dimostrato che ùëä1 + ùëä2 ‚äá ‚å©ùë£1, ‚Ä¶ , ùë£ùëõ, ùë§1,‚Ä¶ , ùë§ùëö‚å™, rimane da verificare l‚Äôaltra
inclusione (al fine di dimostrare la tesi per doppia inclusione): prendiamo un oggetto ùë£ ‚àà ùëä1 + ùëä2
quindi ùë£ = ùëé + ùëè = (‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùëõùë£ùëõ) + (ùëò1ùë§1 +‚ãØùëòùëöùë§ùëö) ‚àà ‚å©ùë£1,‚Ä¶ , ùë£ùëõ, ùë§1,‚Ä¶ , ùë§ùëö‚å™, allora se
ogni elemento ùë£ = ùëé + ùëè ‚àà ùëä1 + ùëä2 allora ùëä1 + ùëä2 ‚äÜ ‚å©ùë£1,‚Ä¶ , ùë£ùëõ, ùë§1,‚Ä¶ , ùë§ùëö‚å™.
‚Ä¢ Il sottospazio generato da sottospazi generati non √® altro che il sottospazio generato dai singoli
elementi, ovvero ùëä = ‚å©‚å©ùë£1, ‚Ä¶ , ùë£ùëõ‚å™, ‚å©ùë§1,‚Ä¶ , ùë§ùëö‚å™‚å™ = ‚å©ùë£1, ‚Ä¶ , ùë£ùëõ, ùë§1,‚Ä¶ , ùë§ùëö‚å™.
La dimostrazione √® banale ricordando che ùëä1 + ùëä2 = ‚å©ùëä1, ùëä2‚å™.
‚Ä¢ ‚å©ùë£, ùë£, 0, ùë§‚å™ = ‚å©ùë£, ùë§‚å™ (praticamente posso eliminare gli elementi nulli o ripetuti)
La dimostrazione √® evidente per la definizione successiva
Sistemi di vettori equivalenti
Sistemi di vettori che dipendono gli uni dagli altri (e viceversa) sono detti equivalenti. Pi√π formalmente posso
dire che il sistema ùë£1, ‚Ä¶ , ùë£ùëõ √® equivalente a ùë§1,‚Ä¶ , ùë§ùëö se ogni ùë£ùëñ dipendono da ùë§1,‚Ä¶ , ùë§ùëö e ogni ùë§ùëó
dipendono da ùë£1,‚Ä¶ , ùë£ùëõ.
Esempi: ‚Ñù2‚å©(0,1), (0,2), (0,0)‚å™; posso ridurlo, essendo uno nullo e l‚Äôaltro proporzionale, alla coppia (0,1),
quindi ‚å©(0,1), (0,2), (0,0)‚å™ = ‚å©(0,1)‚å™ = {(0, ùë•)|ùë• ‚àà ‚Ñù}. I seguenti sottospazi sono equivalenti: ‚å©(0,1), (1,0)‚å™
e ‚å©(1,1), (1,0)‚å™, essendo (0,1) ‚ü∑ (0,1) + (1,0) = (1,1), quindi anche (1,1) dipende da (0,1) e (1,0).
Sia ùëÜ ‚äÜ ùëâ il pi√π piccolo sottospazio a contenere ùëÜ, posso definire il sottospazio generato ‚å©ùëÜ‚å™ = ‚ãÉ ùêø
ùêø‚àà‚Ñí
ricordando che ‚Ñí = {ùêø ‚â§ ùëâ|ùëÜ ‚äÜ ùêø}.
Esercizio: Sia ‚å©{ùë£ ‚àà ‚Ñù|ùë£ ùëñùëõùë°ùëíùëüùëú ùëùùëéùëüùëñ}‚å™ = ùêª, come si caratterizza questo sottospazio?
Soluzione: Essendo 1 contenuto nel sottospazio generato (posso scrivere ad esempio
1
2 ‚ãÖ 2) allora ùêª √® tutto
‚Ñù potendo scrivere qualunque ùëü come 1ùëü. Quindi il sottospazio generato da oggetti pari (in realt√† qualsiasi
oggetto diverso da 0), formano tutto ‚Ñù.
15

Relazione tra dipendenza e linearmente dipendenza
Sia ùë£1,‚Ä¶ , ùë£ùëõ ‚àà ùëâ (con ùëõ ‚â• 2, poich√© se ùëõ = 1 allora per essere linearmente dipendente deve essere
l‚Äôelemento nullo) allora:
I.
ùë£1,‚Ä¶ , ùë£ùëõ sono linearmente dipendenti ‚ü∫ ‚àÉùëñ ‚à∂ ùë£ùëñ dipende dai rimanenti.
Dimostriamo prima l‚Äôimplicazione ‚áí: per ipotesi, essendo i vettori linearmente dipendenti, esiste
‚Ñé1,‚Ä¶ , ‚Ñéùëõ ‚àà ‚Ñù non tutti nulli, tale che ‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùëõùë£ùëõ = 0, supponiamo che ‚Ñé1 ‚â† 0 allora posso
scrivere ‚Ñé1ùë£1 = ‚àí‚Ñé2ùë£2 +‚ãØ‚àí ‚Ñéùëõùë£ùëõ, moltiplicando ambo i membri per il reciproco ‚Ñé1
‚àí1 risulter√†
ùë£1 = (‚àí‚Ñé1
‚àí1‚Ñé2)ùë£2 +‚ãØ+ (‚àí‚Ñé1
‚àí1‚Ñéùëõ)ùë£ùëõ, ovvero che ùë£1 √® combinazione lineare di ùë£2, ‚Ä¶ , ùë£ùëõ, ovvero
che ùë£1 dipende dai rimanenti. Resta ora da provare l‚Äôimplicazione ‚áê: Supponiamo che esista almeno
un vettore dipendente dai rimanenti, quindi che ‚ü∫ ‚àÉùëñ ‚à∂ ùë£ùëñ dipendente da ùë£1, ‚Ä¶ , ùë£ùëñ‚àí1, ùë£ùëñ+1,‚Ä¶ , ùë£ùëõ,
supponiamo che sia ùëñ = 1, allora: ‚àÉ‚Ñé2,‚Ä¶ , ‚Ñéùëõ ‚àà ‚Ñù ‚à∂ ùë£1 = ‚Ñé2ùë£2 +‚ãØ+ ‚Ñéùëõùë£ùëõ, portando tutti i membri
a sinistra avremo: 1ùë£1 ‚àí ‚Ñé2ùë£2 +‚ãØ‚àí ‚Ñéùëõùë£ùëõ = 0 ovvero che ùë£1,‚Ä¶ , ùë£ùëõ sono linearmente dipendenti.
II.
ùë£1,‚Ä¶ , ùë£ùëõ √® indipendente ‚ü∫ nessun ùë£ùëñ dipende dai rimanenti.
Si dimostra sostanzialmente negando la precedente.
Esempi (tipologie di svolgimento degli esercizi):
1) (1,0,0), (1,2,3), (0,0,1) sono linearmente indipendenti; infatti scrivendo una generica combinazione
lineare ‚Ñé1(1,0,0) + ‚Ñé2(1,2,3) + ‚Ñé3(0,0,1) = (0,0,0) avr√≤ (‚Ñé1 + ‚Ñé2, 2‚Ñé2, 3‚Ñé2 + ‚Ñé3) = (0,0,0) da cui
trover√≤ il seguente sistema di equazioni: {
‚Ñé1 + ‚Ñé2 = 0
2‚Ñé2 = 0
3‚Ñé2 + ‚Ñé3 = 0
2) (4,1,4), (4,1,3) sono indipendenti poich√© non sono proporzionali e tutti diversi da zero.
3) (1,0,0), (1,3,0), (3,4,4) sono linearmente indipendenti; infatti (1,0,0) = ‚Ñé(1,3,0) + ùëò(3,4,4) √® valida
solo se ‚Ñé = ùëò = 0 e quindi ho un assurdo, di conseguenza (1,0,0) non dipende dagli altri tue, il
ragionamento √® valido anche per gli altri due elementi, quindi √® un sistema indipendente.
4) (1, ‚àí
1
2 , 0), (4,3,1), (1,2,
1
2) + linearmente dipendente potendo scrivere il terzo come combinazione
lineare degli altri due; infatti: (4,3,1) = 2 (1, ‚àí
1
2 , 0) + 2(1,2,
1
2)
5) Spazio vettori geometrici applicati in ùëÇ: due vettori sono dipendenti se e
solamente se sono proporzionali (giacciono sulla stessa retta) quindi tre vettori
‚Éó ùëÇùëÉ‚Éó‚Éó‚Éó‚Éó , ùëÇùëÑ
‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó , ùëÇùëÖ‚Éó‚Éó‚Éó‚Éó‚Éó  sono dipendenti se e solo se giacciono su di uno stesso piano:
‚Éó ùëÇùëÉ‚Éó‚Éó‚Éó‚Éó  = ùõºùëÇùëÑ‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó  + ùõΩùëÇùëÖ‚Éó‚Éó‚Éó‚Éó‚Éó  (vedi figura).
Relazione tra un sistema pi√π piccolo ed uno pi√π grande:
‚ÄúyvMy W yew, ‚Äî
‚ÄúArwdi
Vv
Vv
=a
‚Ü¶ ‚Ñé1 = ‚Ñé2 = ‚Ñé3 = 0.
af
Se il vettore pi√π piccolo √® linearmente dipendente (quindi posso scriverlo come combinazione lineare di
vettori non tutti nulli) allora posso scrivere il sistema pi√π grande: ‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùëõùë£ùëõ + 0ùë§1 +‚ãØ+ 0ùë§ùëõ = 0;
mentre, se il sistema pi√π grande √® indipendente allora anche il sistema pi√π piccolo √® indipendente.
Proposizione: siano ùë£1,‚Ä¶ , ùë£ùëõ vettori linearmente indipendenti allora ogni vettore ùë£ che dipende da ùë£1,‚Ä¶ , ùë£ùëõ
vi dipende in maniera univoca. Ovvero posso ottenere ogni combinazione lineare in un singolo modo.
Dimostrazione: Bisogna verificare che ‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùëõùë£ùëõ = ùëò1ùë£1 +‚ãØ+ ùëòùëõùë£ùëõ, e che ‚Ñéùëñ = ùëòùëñ, posso scrivere:
(‚Ñé1 ‚àí ùëò1)ùë£1 +‚ãØ+ (‚Ñéùëõ ‚àí ùëòùëõ)ùë£ùëõ = 0, essendo linearmente indipendenti, l‚Äôunica combinazione lineare √®
quella di scalari tutti nulli, ne segue ‚Ñéùëñ ‚àí ùëòùëñ = 0 e dunque ‚Ñéùëñ = ùëòùëñ.
16
3

Proposizione: Siano ùêª, ùêæ ‚â§ ùëâ non triviali (0 ‚â† ‚Ñé ‚àà ùêª e 0 ‚â† ùëò ‚àà ùêæ), ùêª ‚à© ùêæ = {0}, allora i vettori ‚Ñé e ùëò sono
indipendenti.
Dimostrazione: supponiamo per assurdo che ‚Ñé e ùëò siano dipendenti, quindi ùõº‚Ñé + ùõΩùëò = 0 con ùõº, ùõΩ ‚àà ‚Ñù non
entrambi nulli, supponiamo ùõº diverso da zero (uno dei due deve esserlo al fine della dipendenza) quindi
posso scrivere ‚Ñé = (‚àíùõº‚àí1ùõΩ )ùëò ‚àà ùêª ‚à© ùêæ, ora, poich√© ‚Ñé deve essere diverso dal vettore nullo per ipotesi, ed
essendo che ùêª ‚à© ùêæ contiene solo il vettore nullo (sempre per ipotesi) abbiamo trovato un assurdo.
Proposizione: Siano ùêª1,‚Ä¶ , ùêªùëõ sottospazi non banali di ùëâ, 0 ‚â† ùë£1 ‚àà ùêª1,‚Ä¶ , 0 ‚â† ùë£ùëõ ‚àà ùêªùëõ, il sistema costituito
da ùë£1,‚Ä¶ , ùë£ùëõ √® indipendente. (i sottospazi sono in somma diretta)
Dimostriamo per assurdo che i vettori siano linearmente dipendenti:
‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùëõùë£ùëõ = 0
(‚Ñé1,‚Ä¶ , ‚Ñéùëõ) ‚â† (0,‚Ä¶ ,0)
]
‚Ñé1‚â†0
‚áí    ùë£1 =
(‚àí‚Ñé1
‚àí1‚Ñé2)ùë£2 +‚ãØ+ (‚àí‚Ñé1
‚àí1‚Ñéùëõ)ùë£ùëõ ‚àà ùêª1 ‚à© (ùêª2 +‚ãØ+ ùêªùëõ) = {0} e questo significa che i sottospazi sono in
somma diretta che per definizione √® il singleton dell‚Äôelemento neutro, ci√≤ denota un assurdo che contraddice
il fatto che qeusti oggetti siano linearmente dipendenti.
Prendiamo ad esempio i seguenti insiemi: ùêª1 = {(0, ùë•)|ùë• ‚àà ‚Ñù}, ùêª2 = {(ùë•, 0)|ùë• ‚àà ‚Ñù}, ùêª3 = ‚å©(1,1)‚å™;
sappiamo che ùêª1 e ùêª2sono in somma diretta quindi formano un sistema indipendente, supponiamo (0,1) e
(1,0) allora (0,1) (1,0) (1,1) sono dipendenti per gli scalari 1,1, ‚àí1 rispettivamente.
Proposizione: Siano ùêª1 e ùêª2 sottospazi di ùëâ in somma diretta, sia ùë£ ‚àà ùêª1‚®Åùêª2 allora ‚àÉ! ùë£1 ‚àà ùêª1, ùë£2 ‚àà ùêª2 ‚à∂
ùë£ = ùë£1 + ùë£2
Dimostrazione: supponiamo di poter scrivere ùë£ in due modi: ùë£ = ùë£1 + ùë£2 = ùë£1
‚Ä≤ + ùë£2
‚Ä≤ ‚áí (ùë£1 ‚àí ùë£1
‚Ä≤ ) +
(ùë£2 ‚àí ùë£2
‚Ä≤ ) = 0; essendo il primo oggetto una differenza di due oggetti in ùêª1 quell‚Äôoggetto appartiene ad ùêª1,
allo stesso modo ùë£2 ‚àí ùë£2
‚Ä≤ ‚àà ùêª2. Supponendo i due oggetti entrambi diversi da zero avremo un assurdo poich√©
per ipotesi i vettori sono in somma diretta (la loro somma √® pari al singleton del vettore nullo); di conseguenza
l‚Äôunica soluzione √® che ùë£1 = ùë£1
‚Ä≤ e ùë£2 = ùë£2
‚Ä≤ , come volevasi dimostrare.
Osservazione: la precedente proposizione ci fa capire che la somma diretta implica l‚Äôunicit√† di scrittura (posso
scrivere ogni vettore appartenente alla somma diretta in un unico modo). Viceversa: ‚àÄùëî ‚àà ùêª1 + ùêª2 e ‚àÉ! ùë£1 ‚àà
ùêª1, ùë£2 ‚àà ùêª2 ‚à∂ ùëî = ùë£1 + ùë£2 ‚áí ùêª1 ‚à© ùêª2 = {0} (quindi la mia somma √® somma diretta)
Dim.: ùë£ ‚àà ùêª1 ‚à© ùêª2 ‚áí ùë£ = ùë£‚èü
‚ààùêª1
+ 0‚èü
‚ààùêª2
= 0‚èü
‚ààùêª1
+ ùë£‚èü
‚ààùêª2
; ovvero l‚Äôunicit√† di scrittura ùë£ = 0 e 0 = ùë£
Mettendo insieme le precedenti proposizioni posso scrivere la seguente proposizione: siano ùêª1, ùêª2 ‚â§ ùëâ, se
ùêª1 ‚à© ùêª2 = {0} ‚áî ‚àÄùë£ ‚àà ùêª1 + ùêª2, ‚àÉ! ‚Ñé1 ‚àà ùêª1, ‚Ñé2 ‚àà ùêª2 ‚à∂ ùë£ = ‚Ñé1 + ‚Ñé2. La quale posso generalizzarla in:
ùëØùüè,‚Ä¶ , ùëØùíè ùíîùíêùíèùíê ùíäùíè ùíîùíêùíéùíéùíÇ ùíÖùíäùíìùíÜùíïùíïùíÇ ‚ü∫ ‚àÄùíó ‚àà ùëØùüè +‚ãØ+ ùëØùíè, ‚àÉ! ùíâùíä ‚àà ùëØùíä ‚à∂ ùíó = ùíâùüè +‚ãØ+ ùíâùíè
Dimostrazione: l‚Äôimplicazione ‚áí √® gi√† stata dimostrata, rimane da dimostrare quindi l‚Äôimplicazione ‚áê.
Prendiamo un elemento ùë£ ‚àà ùêª1 ‚à© (ùêª2 +‚ãØ+ ùêªùëõ) e dimostriamo che sia uguale all‚Äôelemento neutro (non
consideriamo gli altri sottospazi generati poich√© si procede in modo analogo). Ricordando che ‚àÉ‚Ñé2 ‚àà ùêª2,
‚Ä¶ , ‚Ñéùëõ ‚àà ùêªùëõ
‚à∂ ùë£ = ‚Ñé2 +‚ãØ+ ‚Ñéùëõ posso scrivere ùë£ = ùë£‚èü
‚ààùêª1
e quindi per l‚Äôunicit√† di scrittura comporta che ùë£ = 0
Proposizione: Supponiamo di avere ùë£1, ùë£2, ‚Ä¶ , ùë£ùë° indipendente e ùë£ ‚àà ùëâ che non dipende dai ùë£ùëñ allora se
aggiungo il vettore ùë£ al precedente sistema ottengo un sistema indipendente, quindi ùë£1,‚Ä¶ , ùë£ùë°, ùë£ indipendete
Dimostrazione: prendiamo una combinazione lineare e poniamola uguale al vettore nullo: ‚Ñé1ùë£1 + ‚Ñé2ùë£2 +
‚ãØ+ ‚Ñéùë°ùë£ùë° + ‚Ñéùë£ = 0, in questo modo avremo due possibili casi. Caso ‚Ñé ‚â† 0: ùë£ = (‚àí‚Ñé‚àí1‚Ñé1)ùë£1 +‚ãØ+
(‚àí‚Ñé‚àí1‚Ñéùë°)ùë£ùë° il che √® assurdo essendo che ùë£ non dipende dagli altri vettori. Mentre per il caso ‚Ñé = 0: ‚Ñé1ùë£1 +
‚Ñé2ùë£2 +‚ãØ+ ‚Ñéùë°ùë£ùë° + ‚Ñéùë£ = 0 ‚áí ‚Ñé1 = ‚Ñé2 = ‚ãØ = ‚Ñéùë° = 0 (per definizione).
17
+ 0‚èü
‚ààùêª2
+‚ãØ+ 0‚èü
‚ààùêªùëõ
= 0 + ùë£ = 0‚èü
‚ààùêª1
+ ‚Ñé2‚èü
‚ààùêª2
+‚ãØ+ ‚Ñéùëõ‚èü
‚ààùêªùëõ

Corollario: Sia ùë£1,‚Ä¶ , ùë£ùë° indipendente, ùë£ ‚àà ùëâ, se ùë£1,‚Ä¶ , ùë£ùë°, ùë£ √® dipendente allora ùë£ dipende da ùë£1,‚Ä¶ , ùë£ùë°.
La dimostrazione √® una sorta di negazione della proposizione precedente ((ùëù ‚áí ùëû) ‚áî (¬¨ùëû ‚áí ¬¨ùëù)).
Esempio: Partiamo da (1,2,0) che √® indipendente ‚áí
(1,2,0), (3,2,0) ancora indipendente poich√© (3,2,0)
non dipende da (1,2,0). Allora, per lo stesso motivo,
si ha che (1,2,0), (3,2,0), (0,0,1) √® indipendente.
(42,9) , (3)49)
L pmolig. (mm prep)
Anhp,
Vettori finitamente generabili
Uno spazio vettoriale ùëâ √® detto finitamente generabile se esistono ùë£1, ‚Ä¶ , ùë£ùëõ
‚à∂ ùëâ = ‚å©ùë£1,‚Ä¶ , ùë£ùëõ ‚å™, dove
‚å©ùë£1,‚Ä¶ , ùë£ùëõ ‚å™ √® detto sistema di generatori. Quindi uno spazio vettoriale √® finitamente generabile se ogni
elemento di V pu√≤ essere scritto come combinazione lineare di elementi di ùë£1, ‚Ä¶ , ùë£ùëõ.
Altre definizioni associate:
‚Ä¢ Un sistema di generatori indipendente √® detto base.
‚Ä¢ Un riferimento √® una base ordinata
ùëí1,‚Ä¶ , ùëíùëõ ùëèùëéùë†ùëí ‚áí (ùëí1, ‚Ä¶ , ùëíùëõ) ùëüùëñùëìùëíùëüùëñùëöùëíùëõùë°ùëú
Esempi:
1) ‚Ñù2 (1,0) (0,1)
I.
(ùë•, ùë¶) = ùë•(1,0) + ùë¶(0,1) formano un sistema di generatori indipendenti e dunque (1,0) e (0,1)
sono una base per ‚Ñù2.
II.
III.
(1,0), (2,0) non √® un sistema di generatori poich√© (ùë•, ùë¶) = ‚Ñé1(1,0) + ‚Ñé2(2,0) = (‚Ñé1 + 2‚Ñé2, 0)
dunque (0,1) ‚àâ ‚å©(1,0), (2,0)‚å™
(
u
(1,0), (1,1) √® un sistema di generatori perch√© (ùë•, ùë¶) = ‚Ñé(1,0) + ùëò(1,1) ‚áí {
‚Ñé + ùëò = ùë•
ùëò = ùë¶
(1 1
0 1
|
ùë•
ùë¶) ‚áí ‚àÉ! ùë†ùëúùëôùë¢ùëßùëñùëúùëõùëí ‚àÄ(ùë•, ùë¶) ‚àà ‚Ñù2 (sono anche indipendenti)
2) I vettori (1,0,0), (0,2,0), (0,0, ‚àí1), (0,0,0) sono un sistema di generatori per ‚Ñù3? Prendiamo un generico
sistema (ùë•, ùë¶, ùëß) = ‚Ñé1(1,0,0) + ‚Ñé2(0,2,0) + ‚Ñé3(0,0,‚àí1) + ‚Ñé4(0,0,0) dunque avr√≤ un sistema {
(
(
‚Ñé1 = ùë•
2‚Ñé2 = ùë¶
‚àí‚Ñé3 = ùëß
con soluzione unica e questo vorrebbe dire che il sistema (avendo unicit√†) √® un sistema indipendente di
generatori. √à dunque una possibile base di ‚Ñù3 √® (1,0,0), (0,2,0), (0,0,‚àí1)
3) Spazio vettoriale dei vettori geometrici applicati in ùëÇ √® finitamente generabile
poich√© supponendo di prendere un vettore ùëÇùëÖ‚Éó‚Éó‚Éó‚Éó‚Éó  in qualunque parte del piano
(vedi figura), essa pu√≤ essere ottenuta come combinazione lineare di ùëÇùëÉ‚Éó‚Éó‚Éó‚Éó‚Éó  e ùëÇùëÑ‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó .
a
amis os >
Vig
a
vy
P
‚ÄúLOB,
4) Lo spazio di polinomi ‚Ñùùëõ[ùë•] √® finitamente generabile? Supponiamo di avere gli ùëõ + 1 polinomi
1, ùë•, ùë•2, ‚Ä¶ , ùë•ùëõ, dunque ogni polinomio di grado al pi√π ùëõ posso scriverlo come combinazione lineare
ùëéùëõùë•ùëõ + ùëéùëõ‚àí1ùë•ùëõ‚àí1 +‚ãØ+ ùëé0 = ùëéùëõùë•ùëõ +‚ãØ+ ùëé01; di conseguenza √® finitamente generabile.
5) Lo spazio dei polinomi ‚Ñù[ùë•] non √® finitamente generabile poich√© se lo fosse esisterebbero
ùëù1(ùë•),‚Ä¶ , ùëùùëõ(ùë•) ‚à∂ ‚Ñù[ùë•] = ‚å©ùëùùëñ(ùë•)‚å™ ci√≤ vuol dire che ùëÖ[ùë•] = {ùõº1ùëù1(ùë•) +‚ãØ+ ùõºùëõùëùùëõ(ùë•)|ùõºùëñ ‚àà ‚Ñù} ma ci√≤
significherebbe che se prendo il grado massimo dei polinomi, la combinazione lineare associata non
supererebbe mai il suo grado, ci√≤ significa che un grado superiore al suddetto non apparterrebbe a : ‚Ñù[ùë•]
6) (1,0) √® indipendente ma non √® una base poich√© non posso generare altro elemento oltre allo zero nel
secondo elemento.
18
dunque
(00,4)
am olf.

Teorema di esistenza delle basi: Sia ùëâ uno spazio vettoriale non nullo, se √® finitamente generabile allora ùëâ
ammette basi.
Dimostrazione: sia ùë£1,‚Ä¶ , ùë£ùëõ un sistema di generatori, se ùë£1,‚Ä¶ , ùë£ùëõ √® indipendete allora √® base. Se invece il
sistema √® dipendente allora ‚àÉùëñ ‚à∂ ùë£ùëñ dipenda dai rimanenti, supponiamo che sia ùë£1, allora esso pu√≤ essere
rimosso dal sistema, dunque avr√≤ ùë£2, ‚Ä¶ , ùë£ùëõ
che sar√† ancora un sistema di generatori poich√© ùëâ =
‚å©ùë£1,‚Ä¶ , ùë£ùëõ‚å™ ‚â§ ‚å©ùë£2,‚Ä¶ , ùë£ùëõ‚å™ ‚àã ùë£2, ‚Ä¶ , ùë£ùëõ ‚áí ‚å©ùë£2,‚Ä¶ , ùë£ùëõ‚å™ = ùëâ. Iterando questo procedimento trover√≤ sempre
almeno un sistema indipendente e quindi una base.
Osservazione: Da ogni sistema di generatori si pu√≤ estrarre una base
Ad esempio: (1,0), (2,0), (3,4), (5,6), (1,1) √® un sistema di generatori per ‚Ñù2, essendo (ùë•, ùë¶) = ùë¶(1,1) +
(ùë• ‚àí ùë¶)(1,0) + 0(2,0) + 0(3,4) + 0(5,6); da questo sistema di generatori si pu√≤ estrarre una base, (2,0) √®
proporzionale a (1,0) e si pu√≤ rimuovere, allo stesso modo (3,4), (5,6) essendo entrambi scrivibili come
‚Ñé(1,1) ‚àí 1(1,0), di conseguenza rimarr√≤ con i vettori (1,0)(1,1), che formano una base.
Dimensione di uno spazio vettoriale
Lemma di Steinitz: Se ho ùëö vettori linearmente indipendenti contenuti in un sottospazio generato da ùëõ
vettori allora il numero di generatori √® maggiore o uguale del numero di vettori indipendenti.
ùë£1, ‚Ä¶ , ùë£ùëö ‚àà ‚å©ùë§1,‚Ä¶ , ùë§ùëõ‚å™ ‚áí ùëö ‚â§ ùëõ
Corollario (diretta conseguenza del lemma di Steinitz): Sia ùëâ ‚â† {0} spazio vettoriale, tutte le basi hanno lo
stesso ordine detto dimensione di ùëâ. (la dimensione √® un concetto ben definito per il lemma di Steinitz)
Dimostrazione: Siano ùë£1, ‚Ä¶ , ùë£ùëõ e ùë§1,‚Ä¶ , ùë§ùëö due basi, essendo ùë£1, ‚Ä¶ , ùë£ùëõ una base allora ùë£1,‚Ä¶ , ùë£ùëõ √® un sistema
di vettori linearmente indipendente ed √® contenuto in ùëâ = ‚å©ùë§1,‚Ä¶ , ùë§ùëö‚å™ essendo ùë§1,‚Ä¶ , ùë§ùëö base e quindi
anche un sistema di generatori. Per il lemma di Steinitz avr√≤ che ùëõ ‚â§ ùëö, ovviamente posso scrivere anche il
contrario, ovvero ùë§1, ‚Ä¶ , ùë§ùëö ‚àà ùëâ = ‚å©ùë£1,‚Ä¶ , ùë£ùëõ‚å™, di conseguenza avr√≤ anche ùëö ‚â§ ùëõ, ovvero ùëö = ùëõ.
dim (ùëâ) √® la cardinalit√† di una base, se ùëâ = {0} ‚üπ dim(ùëâ) = 0 (poich√© l‚Äôinsieme vuoto si pu√≤ vedere come
un sistema indipendente di vettori costituito dal solo elemento neutro) mentre se ùëâ non √® finitamente
generabile ‚áí dim(ùëâ) = ‚àû.
Proposizioni della dimensione (ùëΩ ‚â† {ùüé} ùíáùíäùíèùíäùíïùíÇùíéùíÜùíèùíïùíÜ ùíàùíÜùíèùíÜùíìùíÇùíÉùíäùíçùíÜ, ùêùùê¢ùê¶ ùëΩ = ùíè):
‚Ä¢ ùë£1,‚Ä¶ , ùë£ùëö indipendente ‚áí ùëö ‚â§ ùëõ (il numero di oggetti non pu√≤ superare la dimensione di ùëâ)
Dim.: Sia ùëí1,‚Ä¶ , ùëíùëõ una base di ùëâùëõ allora essendo ùë£1,‚Ä¶ , ùë£ùëö un sistema di vettori linearmente indipendente
ùë£1,‚Ä¶ , ùë£ùëö ‚àà ‚å©ùëí1,‚Ä¶ , ùëíùëõ‚å™ = ùëâùëõ ‚áí ùëö ‚â§ ùëõ per il lemma di Steinitz
‚Ä¢ ùë£1,‚Ä¶ , ùë£ùëõ indipendente ‚áí ùë£1,‚Ä¶ , ùë£ùëõ formano una base (quindi anche un sistema di generatori)
Basta dimostrare che ùë£1,‚Ä¶ , ùë£ùëõ sia anche un sistema di generatori, per farlo supponiamo per assurdo che
ùë£1,‚Ä¶ , ùë£ùëõ non siano un sistema di generatori quindi ‚å©ùë£1,‚Ä¶ , ùë£ùëõ‚å™ < ùëâ ma allora ‚àÉùë£ ‚àà ùëâ ‚àñ ‚å©ùë£1,‚Ä¶ , ùë£ùëõ‚å™ e
quindi ùë£ non dipende da ùë£1,‚Ä¶ , ùë£ùëõ e quindi ùë£, ùë£1,‚Ä¶ , ùë£ùëõ √® un sistema indipendente di cardinalit√† ùëõ + 1
che √® assurdo poich√© contraddice la proposizione precedente.
‚Ä¢ ùë£1,‚Ä¶ , ùë£ùëõ sistema di generatori ‚áí ùë£1,‚Ä¶ , ùë£ùëõ formano una base (quindi √® anche indipendente)
Si dimostra per assurdo poich√© se non fossero indipendenti potremmo estrarre una base di cardinalit√†
minore di ùëõ (in modo analogo alla dimostrazione precedente)
‚Ä¢ ùë£1,‚Ä¶ , ùë£ùëö indipendenti ‚áí ‚àÉùë§ùëö+1, ‚Ä¶ , ùë§ùëõ: ùë£1, ‚Ä¶ , ùë£ùëö, ùë§ùëö+1,‚Ä¶ , ùë§ùëõ formano una base (Ogni sistema di
vettori linearmente indipendenti si pu√≤ completare in una base)
Dim.: se ùëö = ùëõ allora √® base per la seconda preposizione. Se ùëö < ùëõ ‚áí ‚å©ùë£1, ‚Ä¶ , ùë£ùëö‚å™ < ùëâ, ma allora
‚àÉùë§ùëö+1 ‚àà ùëâ ‚àñ ‚å©ùë£1,‚Ä¶ , ùë£ùëö‚å™ ‚áí ùë£1, ‚Ä¶ , ùë£ùëö, ùë§ùëö+1 √® indipendente; se √® base ho concluso, altrimenti continuo
sino a trovare la base ùë£1, ‚Ä¶ , ùë£ùëö, ùë§ùëö+1,‚Ä¶ , ùë§ùëõ (finir√≤ poich√© per la prima proposizione ho un numero finito
di vettori linearmente indipendenti, ovvero ùëõ).
19

Proposizione: Sia ùëâ spazio vettoriale, i seguenti enunciati sono tra loro equivalenti per ùë£1,‚Ä¶ , ùë£ùëõ:
1) Base
2) Massimale rispetto all‚Äôindipendenza (se preso un sistema pi√π grande allora perde la propriet√† di
indipendenza)
3) Minimale rispetto all‚Äôessere un sistema di generatori (se preso un sistema pi√π piccolo allora perde la
propriet√† di essere un sistema di generatori)
4) Sistema indipendente di cardinalit√† massima (tutti gli altri sistemi indipendenti hanno cardinalit√†
minore o uguale di n)
5) Sistema di generatori di cardinalit√† minima (tutti gli altri sistemi di generatori hanno cardinalit√†
maggiore o uguale di n)
Dimostrazione: 1 ‚áí 2: supponiamo ci sia un sistema di vettore contenente ùë£1,‚Ä¶ , ùë£ùëõ, che essendo base
implica ùë£1,‚Ä¶ , ùë£ùëõ, ùë§1,‚Ä¶ , ùë§ùëö
dipendente. 2 ‚áí 1:
ùë£1,‚Ä¶ , ùë£ùëõ massimale indipendente, usando il
completamento ad una base perderei il concett√≤ di massimale se esistesse una base con cardinalit√† > ùëõ.
Segue da quest‚Äôultima preposizione anche che 4 ‚áí 1, mentre la 1 ‚áí 4 √® analoga alla 1 ‚áí 2. 1 ‚áí 3: essendo
ùë£1,‚Ä¶ , ùë£ùëõ base, se non sar√† minimale rispetto alla generazione ci sar√† un sottosistema proprio di ùë£1,‚Ä¶ , ùë£ùëõ
che √® ancora un sistema di generatori, da cui possiamo allora estrarre una base e trarne dimensioni diverse.
3 ‚áí 1: ùë£1,‚Ä¶ , ùë£ùëõ minimale rispetto alla generazione allora estraiamo una base che deve coincidere con
ùë£1,‚Ä¶ , ùë£ùëõ. 1 ‚áí 5: essendo ùë£1,‚Ä¶ , ùë£ùëõ base se per assurdo esistesse un sistema di generatori ùë§1,‚Ä¶ , ùë§ùëö con
ùëö < ùëõ allora √® possibile estrarre da ùë§1,‚Ä¶ , ùë§ùëö una base che avr√† perci√≤ cardinalit√† < ùëõ (assurdo). La 5 ‚áí 1
si dimostra banalmente dalla 3 ‚áí 1. Per doppia implicazione abbiamo dimostrato che: 1 ‚áî 2 ‚áî 3 ‚áî 4 ‚áî 5
Proposizione: Sia ùëä ‚â§ ùëâùëõ allora:
1) ùëä √® finitamente generabile e dimùëä ‚â§ ùëõ
Dim.: Se per assurdo ùëä non fosse finitamente generabile allora se ùë§1 ‚àà ùëä, ùëä ‚â† ‚å©ùë§1‚å™ e quindi
esisterebbe un ùë§2 ‚àà ùëä ‚àñ ‚å©ùë§1‚å™ ‚áí ùë§1, ùë§2 indipendenti. Segue sempre che ùëä ‚â† ‚å©ùë§1, ùë§2‚å™, di
conseguenza possiamo iterare questo processo fino all‚Äôassurdo di trovare ùëõ + 1 vettori indipendenti.
Presa una base di ùëä(‚â† {0}) ùë§1,‚Ä¶ , ùë§ùë° allora ùë§1, ‚Ä¶ , ùë§ùë° sono indipendenti anche in ùëâ, quindi ùë° ‚â§ ùëõ
2) dimùëä = ùëõ ‚áî ùëä = ùëâùëõ
La ‚áê √® ovvia poich√© se ùëâùëõ = ùëä allora dimùëä = dimùëâ = ùëõ. L‚Äôimplicazione ‚áí si dimostra prendendo
una base di ùëä, sia ùë§1,‚Ä¶ , ùë§ùëõ che essendo base √® anche un sistema indipendente di cardinalit√† ùëõ in
ùëâ e quindi base per ùëâ
Corollario: Siano ùëä, ùëç ‚â§ ùëâùëõ allora se
1) ùëä ‚â§ ùëç ‚áí dimùëä ‚â§ dimùëç
2) ùëä ‚â§ ùëç e dimùëä = dimùëç ‚áí ùëä = ùëç
Le dimostrazioni sono analoghe a quelle precedenti. Basti considerare ùëç come ùëâ
Esempio: <(1,0,0),(0,1,0)> e <(0,1,0),(0,0,1)> sono due sottospazi che hanno la stessa dimensione ma non
sono contenuti uno nell‚Äôaltro; eccetto per la dimensione R^1 sono infiniti gli esempi di questo tipo. Si prenda
ad esempio ‚Ñù2 avremo ‚å©(1,0)‚å™ ‚å©(1,1)‚å™ ‚å©(1,2)‚å™ ‚Ä¶ ‚àû sottospazi della stessa dimensione.
Si prendano ùêª1,‚Ä¶ , ùêªùëõ ‚â§ ùëâ sottospazi in somma diretta allora dim(ùêª1‚®Å‚Ä¶‚®Åùêªùëõ) = dimùêª1 +‚ãØ+ dimùêªùëõ.
Dimostrazione: Siano ùëí11,‚Ä¶ , ùëí1ùëö1 base di ùêª1, ùëí21,‚Ä¶ , ùëí2ùëö2 base di ùêª2, ‚Ä¶ , ùëíùëõ1,‚Ä¶ , ùëíùëõùëöùëõ base di ùêªùëõ (il primo
pedice identifica lo spazio vettoriale, il secondo il numero dell‚Äôelemento). Dimostriamo che l‚Äôunione delle
basi ùëí11,‚Ä¶ , ùëí1ùëö1, ùëí21,‚Ä¶ , ùëí2ùëö2,‚Ä¶ , ùëíùëõ1,‚Ä¶ , ùëíùëõùëöùëõ
sia essa stessa una base: che sia un sistema di generatori √®
scontato perch√© comunque si prendano un vettore del sottospazio somma esso si pu√≤ ottenere mediante il
sistema ùëí11,‚Ä¶ , ùëí1ùëö1,‚Ä¶ , ùëíùëõ1,‚Ä¶ , ùëíùëõùëöùëõ. Ora ci resta da dimostrare che formi anche un sistema indipendente
per ùêª1‚®Å‚Ä¶‚®Åùêªùëõ, ovvero che ‚Ñé11ùëí11 +‚ãØ+ ‚Ñé1ùëö1ùëí1ùëö1 +‚ãØ+ ‚Ñéùëõ1ùëíùëõ1 +‚ãØ+ ‚Ñéùëõùëöùëõ ùëíùëõùëöùëõ = 0, isolando i
vettori nel seguente modo ‚Ñé11ùëí11 +‚ãØ+ ‚Ñé1ùëö1ùëí1ùëö1 + (‚Ä¶+ ‚Ñéùëõ1ùëíùëõ1 +‚ãØ+ ‚Ñéùëõùëöùëõùëíùëõùëöùëõ)
posso scrivere
‚èü                    
‚ààùêª2+‚ãØ+ùêªùëõ
20

‚Ñé11ùëí11 +‚ãØ+ ‚Ñé1ùëö1ùëí1ùëö1 = ‚ãØ‚àí ‚Ñéùëõ1ùëíùëõ1 +‚ãØ‚àí ‚Ñéùëõùëöùëõùëíùëõùëöùëõ ed in questa forma √® evidente che quest‚Äôoggetto
scrivibile in due modi (come se stesso e come la combinazione lineare soprascritta) appartiene all‚Äô insieme
ùêª1 ‚à© (ùêª2 +‚ãØ+ ùêªùëõ) = {0} (per definizione fa il singleton dell‚Äôelemento neutro). Ci√≤ significa che l‚Äôoggetto
a sinistra √® l‚Äôelemento neutro e dunque ‚Ñé11ùëí11 +‚ãØ+ ‚Ñéùëõùëöùëõùëíùëõùëöùëõ = 0 ‚áí ‚Ñé11 = ‚ãØ = ‚Ñé1ùëõ1 = 0. Isolando ora
gli altri insiemi seguendo lo stesso ragionamento avr√≤ come da tesi ‚Ñé11 = ‚ãØ = ‚Ñé1ùëõ1 = ‚ãØ = ‚Ñéùëõùëöùëõ = 0.
Relazione di Grassmann
Essa ci dice praticamente la dimensione del sottospazio somma quando i due sottospazi non sono
necessariamente in somma diretta (vale anche per ùëõ sottospazi).
ùëØ, ùë≤ ‚â§ ùëΩùíè
ùêùùê¢ùê¶ ùëØ + ùêùùê¢ùê¶ ùë≤ = ùêùùê¢ùê¶(ùëØ ‚à© ùë≤) + ùêùùê¢ùê¶(ùëØ + ùë≤)
Dimostrazione: se ùêª ‚à© ùêæ = {0} ‚áí ùêª‚®Åùêæ ed √® dimostrata banalmente per la proposizione precedente.
Supponiamo dunque ùêª ‚à© ùêæ ‚â† {0} ‚áí dimùêª ‚à© ùêæ = ùë°, prendiamo una base di ùêª ‚à© ùêæ ed andiamo a
completarla sia come base di ùêª che di ùêæ, avremo dunque che esistono ùëí1,‚Ä¶ , ùëíùë° base di ùêª ‚à© ùêæ,
ùëí1,‚Ä¶ , ùëíùë°, ‚Ñéùë°+1,‚Ä¶ , ‚Ñéùëü base di ùêª e ùëí1,‚Ä¶ , ùëíùë°, ùëòùë°+1,‚Ä¶ , ùëòùë† base di ùêæ (ovviamente pu√≤ anche succedere che i ùë°
vettori siano una base anche di ùêª e/o di ùêæ). Mettendo insieme i precedenti oggetti e considerandolo un
singolo insieme avr√≤ ùëí1,‚Ä¶ , ùëíùë°, ‚Ñéùë°+1,‚Ä¶ , ‚Ñéùëü, ùëòùë°+1, ‚Ä¶ , ùëòùë† sono ùëü + ùë† ‚àí ùë° elementi (vedi figura) ora dobbiamo
dimostrare che ùëü + ùë† ‚àí ùë° = dimùêª + dimùêæ ‚àí dimùêª ‚à© ùêæ sia
effettivamente dim(ùêª + ùêæ). Dobbiamo dimostrare dunque che
sia una base, quindi che sia un sistema di generatori e linearmente
indipendente. 1) sistema di generatori: per definizione esiste ùë£ ‚àà
ùêª + ùêæ ‚áí ‚àÉ‚Ñé ‚àà ùêª, ùëò ‚àà ùêæ ‚à∂ ùë£ = ‚Ñé + ùëò dove ‚Ñé √® combinazione
lineare di ùëí1,‚Ä¶ , ùëíùë°, ‚Ñéùë°+1,‚Ä¶ , ‚Ñéùëü e ùëò combinazione lineare di
ùëí1,‚Ä¶ , ùëíùë°, ùëòùë°+1,‚Ä¶ , ùëòùë† e dunque avr√≤ che ùë£ √® combinazione lineare
di ùëí1,‚Ä¶ , ùëíùë°, ‚Ñéùë°+1,‚Ä¶ , ‚Ñéùëü, ùëòùë°+1,‚Ä¶ , ùëòùë†. 2) sistema linearmente indipendente: prendiamo una combinazione
lineare generica ùëé1ùëí1 +‚ãØ+ ùëéùë°ùëíùë° + ùëéùë°+1‚Ñéùë°+1 +‚ãØ+ ùëéùëü‚Ñéùëü + ùëèùë°+1ùëòùë°+1 +‚ãØ+ ùëèùë†ùëòùë† = 0 e scriviamo (come
con la dimostrazione precedente) , ùë• = ùëé1ùëí1 +‚ãØ+ ùëéùë°ùëíùë° + ùëéùë°+1‚Ñéùë°+1 +‚ãØ+ ùëéùëü‚Ñéùëü
= ‚àíùëèùë°+1ùëòùë°+1 +‚ãØ‚àí ùëèùë†ùëòùë†
‚èü                          
ùêª
dunque ùë• ‚àà ùêª ‚à© ùêæ = ‚å©ùëí1,‚Ä¶ , ùëíùë°‚å™ e in particolare ùë•
= ùëê1ùëí1 +‚ãØ+ ùëêùë°ùëíùë°
= ‚àíùëèùë°+1ùëòùë°+1 +‚ãØ‚àí ùëèùë†ùëòùë†
‚èü              
ùêæ
ottenendo cos√¨ che ùëê1ùëí1 +
‚ãØ+ ùëêùë°ùëíùë° + ùëèùë°+1ùëòùë°+1 +‚ãØ+ ùëèùë†ùëòùë† = 0 da cui (essendo una base) posso dedurre ùëê1 = ‚ãØ = ùëêùë° = ùëèùë°+1 = ‚ãØ =
ùëèùë† = 0 ma allora ùëé1ùëí1 +‚ãØ+ ùëéùë°ùëíùë° + ùëéùë°+1‚Ñéùë°+1 +‚ãØ+ ùëéùëü‚Ñéùëü = 0 ‚áí ùëé1 = ‚ãØ = ùëéùë° = ùëéùë°+1 = ‚ãØ = ùëéùëü = 0.
Osservazione: Grassmann si usa usualmente per calcolare la dimensione dell‚Äôintersezione poich√© nella
maggior parte dei casi la dimensione della somma √® ‚Äúvisivamente‚Äù calcolabile.
Proposizione: Sia ùëâùëõ uno spazio vettoriale di dimensione ùëõ e ‚Ñõ = (ùëí1,‚Ä¶ , ùëíùëõ) un riferimento (base ordinata,
quindi ho l‚Äôunicit√† di scrittura), avr√≤: ‚àÄùë£ ‚àà ùëâùëõ ‚àÉ! ‚Ñé1,‚Ä¶ , ‚Ñéùëõ ‚àà ‚Ñù ‚à∂ ùë£ = ‚Ñé1ùëí1 +‚ãØ+ ‚Ñéùëõùëíùëõ (considero un
riferimento cos√¨ posso definire una n-upla nell‚Äôordine assegnato), e questa n-upla (‚Ñé1,‚Ä¶ , ‚Ñéùëõ) si chiamer√†
n-upla dei componenti nel riferimento ‚Ñõ.
21

Per passare da componenti in un riferimento ‚Ñõ a componenti di un altro riferimento ‚Ñõ‚Ä≤ si eseguono
(meccanicamente) i seguenti passi:
DCm) > Neh)2) wf ot Vu
'
24, Obed 6,16)
veVv
Ey7 @2c)4 48Wc! CryXn) ae O
:
'
.
:
'
CaF O C4 than2)
YER Gas Xan =(matt
Wo
Cy ky eta) ia
'
heOia)Si HOpthem ) S,
2
FX,+ TR
ylel
1
le
Le,
1 ;
Sa My rate XaBam pany RPXp Bit
Az(s;,) X> (!
\/%
‚Äò) x(\ XS AX
x
Ni molto ol: foray
Forma canonica
Sia ‚Ñùùëõ, si definisce base canonica o riferimento canonico di uno spazio vettoriale l‚Äôinsieme delle seguenti
basi: ùëí1 = (1,0,‚Ä¶ ,0), ùëí2 = (0,1,‚Ä¶ ,0), ‚Ä¶ , ùëíùëõ = (0,0,‚Ä¶ ,1); (per la base canonica non √® rilevante l‚Äôordine) di
conseguenza ogni vettore appartenente allo spazio vettoriale si pu√≤ scrivere come combinazione lineare della
base canonica. Altri esempi:
‚Ä¢ Base canonica di ‚Ñùùëõ[ùë•]: 1, ùë•, ùë•2,‚Ä¶ , ùë•ùëõ
‚Ä¢ Base dello spazio delle matrici ‚Ñù2,3: (1 0 0
0 0 0
(0 0 0
0 0 1
) , (0 1 0
0 0 0
) , (0 0 1
0 0 0
) (concetto analogo per quanto riguarda matrici ‚Ñùùëõ,ùëö
Metodi per estrarre una base da un sottospazio generato
Sia, ad esempio, il seguente sottospazio ùêª = ‚å©(1,0,1,2), (‚àí1,2,3,0), (3, ‚àí2,‚àí1,4), (‚àí2,4,6,0)‚å™ ‚â§ ‚Ñù4, tra i
generatori di questo sottospazio generato √® evidente la proporzionalit√† tra il secondo e il quarto vettore,
dunque ùêª = ‚å©(1,0,1,2), (‚àí1,2,3,0), (3,‚àí2, ‚àí1,4)‚å™, mentre il terzo √® dipendente dai primi due essendo
(3, ‚àí2,‚àí1,4) = 2(1,0,1,2) ‚àí 1(‚àí1,2,3,0), quindi rimarr√† ùêª = ‚å©(1,0,1,2), (‚àí1,2,3,0)‚å™, di conseguenza
essendo dimùêª = 2 qualunque coppia non proporzionale dei quattro vettore definiti all‚Äôinizio sono una base
per ùêª. Pi√π precisamente: tutti i sistemi indipendenti di ordine 2 sono una base per ùêª.
22
) , (0 0 0
1 0 0
) , (0 0 0
0 1 0
),
one forenele gli
hed
tossegye

Un altro metodo √® quello di posizionare i vettori in una matrice e trasformarla in una matrice equivalente a
gradini: (
1
‚àí1
3
‚àí2
0
2
‚àí2
4
1
3
‚àí1
6
2
0
4
0
)~(
1
0
0
0
0
2
0
0
1
4
0
0
2
2
0
0
) in tal modo non ho alterato il sottospazio generato dalle righe e ci√≤
significa che il sottospazio generato dalle righe della matrice a gradini √® lo stesso di quello generato dalla
matrice iniziale, ed √® evidente che la dimensione di ùêª sia 2 (le righe nulle sono ininfluenti ed √® evidente la
non proporzionalit√† tra la prima e la seconda riga), pi√π precisamente dimùêª = numero di pivot di un sistema
a gradini equivalente.
Corollario: Due matrici a gradini equivalenti hanno lo stesso numero di pivot.
4. Matrici e Sistemi lineari
Determinante di una matrice quadrata
Il determinante esiste solo e solamente per matrici quadrate; il determinante si definisce per ricorsione
sull‚Äôordine della matrice in questione: per ùëõ = 1 la matrice ùê¥ = (ùëé) ha determinante det ùê¥ = ùëé; supposto di
averlo definito per matrici di ordine ùëõ ‚àí 1 definiamolo per matrici di ordine ùëõ, al fine di poterlo fare si
definiscono i seguenti concetti:
‚Ä¢ ‚àÄùëéùëñùëó di ùê¥ la matrice complementare di ùëéùëñùëó √® ùê¥(ùëñ, ùëó) e si definisce nel seguente modo:
(praticamente si cancella la riga ùëñ e la colonna ùëó dalla matrice ùê¥)
‚Ä¢ Complemento algebrico di ùëéùëñùëó: ùê¥ùëñùëó = (‚àí1)ùëñ+ùëó det ùê¥(ùëñ, ùëó);
√à possibile usare il determinante nella definizione di complemento algebrico poich√© essendo ùê¥(ùëñ, ùëó)
una matrice complementare ha ordine minore di ùê¥ (abbiamo supposto di aver dato la definizione di
determinante per matrici di ordine ùëõ ‚àí 1)
ùëé11ùê¥11 + ùëé12ùê¥12 +‚ãØ+ ùëé1ùëõùê¥1ùëõ = ùëé21ùê¥21 + ùëé22ùê¥22 +‚ãØ+ ùëé2ùëõùê¥2ùëõ = ‚ãØ = ùëéùëõ1ùê¥ùëõ1 + ùëéùëõ2ùê¥ùëõ2 +‚ãØ+
ùëéùëõùëõùê¥ùëõùëõ = ùêùùêûùê≠ ùë® = ùëé1ùëõùê¥1ùëõ +‚ãØ+ ùëéùëõùëõùê¥ùëõùëõ = ‚ãØ = ùëé12ùê¥12 +‚ãØ+ ùëéùëõ2ùê¥ùëõ2 = ùëé11ùê¥11 +‚ãØ+ ùëéùëõ1ùê¥ùëõ1
Esempio: det (ùëé
ùëè
ùëê
ùëë
) = ùëéùê¥11 + ùëèùê¥12 = ùëéùëë ‚àí ùëèùëê infatti se ùê¥ = (1 2
3 4
) ‚áí det ùê¥ = 4 ‚àí 6 = ‚àí2
Matrici 3x3: regola di Sorrus: Prendiamo il determinante di una generica matrice 3x3 |(
ùëé11 ùëé12 ùëé13
ùëé21 ùëé22 ùëé23
ùëé31 ùëé32 ùëé33
ad essa si duplica prima e seconda colonna e li posizioniamo dopo la seconda colonna:
(
ùëé11 ùëé12 ùëé13
ùëé21 ùëé22 ùëé23
ùëé31 ùëé32 ùëé33
|
ùëé11 ùëé12
ùëé21 ùëé22
ùëé31 ùëé32
) dopodich√© il determinante della mia matrice di partenza sar√† uguale al
prodotto tra le diagonali (quelle in verde) meno il prodotto tra le antidiagonali (quelle in nero), ovvero:
Art
"
64810,
Q),0,4 QO.
‚Äî
%
a ata &.0
4u%‚Äô4
Sp Ub 44
23
\|
/|
)|,
nN
_¬ªes_
a ONS

Osservazione: Se una matrice ha una riga o colonna tutta nulla allora il suo determinante √® zero
Propriet√† del determinante:
1) det ùê¥ = det ùê¥ùë°
2) Se una riga dipende dalle rimanenti allora il determinante √® zero
(Corollario: se il determinante √® diverso da zero allora le righe sono indipendenti)
3) Teorema di Cauchy-Binet: ùê¥, ùêµ ‚àà ‚Ñùùëõ
det(ùê¥ùêµ) = det(ùê¥) ‚ãÖ det(ùêµ)
4) Se moltiplico una singola riga (o colonna) per uno scalare ‚Ñé allora il determinante della matrice risultante
√® ‚Ñé ‚ãÖ det ùê¥ (con ‚Ñé ‚àà ‚Ñù e ùê¥ ‚àà ‚Ñùùëõ)
5) Se scambiamo due righe o due colonne il determinante cambia segno
6) Se aggiungo ad una riga (o colonna) un multiplo di un‚Äôaltra riga (o colonna) il determinante non cambia
7) Sia ùê¥ una matrice quadrata e sia ùê∑ una matrice a gradini equivalente ad ùê¥ allora |ùê¥| ‚â† 0 ‚áî |ùê∑| ‚â† 0
Definizione: ùê¥ ‚àà ‚Ñùùëõ √® detta invertibile se e solo se ‚àÉùêµ ‚àà ‚Ñùùëõ: ùê¥ùêµ = ùêºùëõ = ùêµùê¥
Proposizione: Una matrice ùê¥ √® invertibile se e solamente se ha determinante diverso da zero e la sua
inversa √® ùê¥‚àí1 =
|ùê¥| ( ‚ãÆ ‚ã± ‚ãÆ ) (matrice trasposta dei complementi algebrici)
1
ùê¥11 ‚ãØ ùê¥ùëõ1
ùê¥1ùëõ ‚ãØ ùê¥ùëõùëõ
Corollario: Siano ùê¥, ùêµ ‚àà ‚Ñùùëõ se ùê¥ùêµ = ùêºùëõ allora sia ùê¥ che ùêµ sono invertibili e ùêµ = ùê¥‚àí1 (inversa di ùê¥)
Dimostrazione: Applicando Cauchy-Binet: det(ùê¥) det(ùêµ) = det(ùê¥ùêµ) = det(ùêºùëõ) = 1 ne segue det ùê¥ ‚â† 0 e
anche det ùêµ ‚â† 0 e dunque ùê¥ e ùêµ sono invertibili, ma allora ùê¥ùêµ = ùêºùëõ ‚áí (ùê¥ùêµ)ùêµ‚àí1 = ùêºùëõùêµ‚àí1 ‚áí ùê¥ = ùêµ‚àí1,
ovviamente anche ùêµ = ùê¥‚àí1 essendo l‚Äôinversa dell‚Äôinversa la matrice stessa.
Definizione: Sia ùê¥ ‚àà ‚Ñùùëõ,ùëö (si noti la generalit√† della matrice, non √® valido solo per una matrice quadrata)
definisco rango di riga la dimensione del sottospazio generato dalle righe, e rango di colonna la dimensione
del sottospazio generato dalle colonne.
A> (¬∞-2) ) (A)eo>", (A)
On, 2, Q,, vee
Qig Gir Say
he Qe
ae
em|@ [
Qs an
Mm, an
am
Minore di una matrice
Sia 0 < ‚Ñé ‚â§ min{ùëö, ùëõ}, fissato ‚Ñé allora prender√≤ le ‚Ñé-righe e le ‚Ñé-colonne e cancello tutte le altre, la
sottomatrice quadrata rimanente √® definita come la matrice minore di ordine ‚Ñé. Ad esempio:
24

Sia ùê¥ ‚àà ‚Ñùùëõ,ùëö una matrice rettangolare:
ùëéùëñ1, ‚Ä¶ , ùëéùëñ‚Ñé ùëüùëñùëî‚Ñéùëí
ùëéùëó1, ‚Ä¶ , ùëéùëó‚Ñé ùëêùëúùëôùëúùëõùëõùëí
] ‚áí ùê¥(ùëñ1, ‚Ä¶ , ùëñ‚Ñé; ùëó1,‚Ä¶ , ùëó‚Ñé), cos√¨ facendo (poich√©
il determinante √® possibile solo per matrici quadrate) posso definire il determinante del minore, e lo indico
con |ùê¥|(ùëñ1,‚Ä¶ , ùëñ‚Ñé; ùëó1,‚Ä¶ , ùëó‚Ñé).
Un minore si dice di ordine massimo se il suo ordine coincide con il minimo del numero di righe e colonne di
quella matrice rettangolare (min{ùëõ, ùëö}); una matrice non quadrata ha sempre pi√π di un minore di ordine
massimo, mentre una matrice quadrata ha un solo minore di ordine massimo ed √® la matrice stessa.
Supponiamo che ùê¥(ùëñ1,‚Ä¶ , ùëñ‚Ñé; ùëó1,‚Ä¶ , ùëó‚Ñé) non sia di ordine massimo, allora esister√† un
indice di riga ùëñ ‚â† ùëñ1,‚Ä¶ , ùëñ‚Ñé ed un indice di colonna ùëó ‚â† ùëó1,‚Ä¶ , ùëó‚Ñé cos√¨ da poter comporre
un minore ùê¥(ùëñ, ùëñ1, ‚Ä¶ , ùëñ‚Ñé; ùëó, ùëó1,‚Ä¶ , ùëó‚Ñé); un minore cos√¨ composto si definisce orlato del
minore. √à ovviamente possibile orlare un orlato.
Definizione: Un minore si dice fondamentale se il suo determinante √® diverso da 0 e se il determinante di
ogni suo orlato √® uguale a 0.
Non ci sono sempre minori fondamentali, ad esempio la matrice nulla non ha minori fondamentali, ma, se la
matrice non √® nulla allora esiste sempre almeno un minore fondamentale. Il minore fondamentale non √®
necessariamente unico.
Se un minore di ordine massimo ha determinante diverso da zero allora esso √® un minore fondamentale
(banalmente: l‚Äôimplicazione √® vera se non posso fare un orlato, e dunque sar√† un minore fondamentale).
Teorema degli orlati: Sia ùê¥ ‚àà ‚Ñùùëõùëö una matrice rettangolare e ùê¥(ùëñ1,‚Ä¶ , ùëñ‚Ñé; ùëó1,‚Ä¶ , ùëó‚Ñé) √® un minore
fondamentale allora le righe ùëñ1, ‚Ä¶ , ùëñ‚Ñé, ùëó1,‚Ä¶ , ùëó‚Ñé sono un base del sottospazio generato da tutte le righe
(formano un sistema di vettori indipendenti).
Osservazioni: il teorema degli orlati ci dice che trovato un minore fondamentale (ed esiste sempre) non solo
le righe (o colonne) formano un sistema di vettori indipendenti ma essi sono anche una base di tutte le righe
(o colonne) della matrice. Un‚Äôaltra conseguenza che tutti i minori fondamentali hanno lo stesso ordine.
Corollari derivanti dal teorema degli orlati:
‚Ä¢ ùëü(ùê¥) si chiama semplicemente rango della matrice poich√© il rango di riga √® sempre uguale al rango
di colonna, inoltre il rango di ùê¥ √® uguale al numero di pivot di una matrice a gradini equivalente per
righe.
‚Ä¢ Tutti i minori fondamentali hanno lo stesso ordine
‚Ä¢ Il determinante di una matrice quadrata √® diversa da zero se e solo se le righe (o colonne) sono
indipendenti. ùê¥ ‚àà ‚Ñùùëõ
|ùê¥| ‚â† 0 ‚áî righe (o colonne) sono indipendenti
‚Ä¢ ùê¥ ‚àà ‚Ñùùëõ
Non √® necessario dimostrare che |ùê¥| ‚â† 0 ‚áí righe indipendenti. Per l‚Äôimplicazione ‚áê supponiamo
che le righe siano indipendenti, allora ùëüùëü(ùê¥) = ùëõ, e prendiamo un minore fondamentale di ùê¥,
quest‚Äôultima essendo una matrice quadrata ha un unico minore fondamentale di ordine ùëõ, questo
minore fondamentale √® ùê¥ stesso e dunque per definizione |ùê¥| ‚â† 0
det ùê¥ = 0 ‚áî righe (o colonne) dipendenti
25
v
4:
co
aune
omYe

Criteri di compatibilit√† di sistemi di equazioni lineari
ùëÜ: {
ùëé11ùë•1 + ùëé12ùë•2 +‚ãØ+ ùëé1ùëöùë•ùëö = ùëê1
ùëé21ùë•1 + ùëé22ùë•2 +‚ãØ+ ùëé2ùëöùë•ùëö = ùëê2
‚ãÆ
ùëéùëõ1ùë•1 + ùëéùëõ2ùë•2 +‚ãØ+ ùëéùëõùëöùë•ùëö = ùëêùëõ
essere scritta come ùê∂ = (
ùëé11
‚ãÆ
ùëéùëõ1
) ùë•1 +‚ãØ+ (
ùëé1ùëö
‚ãÆ
ùëéùëõùëö
) ùë•ùëö; quindi se c‚Äô√® una soluzione la colonna dei termini noti
√® combinazione dei sistemi lineari delle colonne della matrice incompleta.
Primo criterio di compatibilit√† (utile per dimostrare il teorema di Rouch√©-Capelli): ùëÜ √® compatibile se e
soltanto se la colonna dei termini noti √® combinazione lineare delle colonne della matrice incompleta.
Teorema di Rouch√©-Capelli: ùëÜ √® compatibile ‚áî matrice completa ed incompleta hanno lo stesso rango. (il
sistema ammette soluzione se e solamente se il rango della matrice completa √® uguale al rango della matrice
incompleta).
Dimostrazione: ‚áê ‚à∂ ùëü(ùê¥) = ùëü(ùê¥‚Ä≤) = ‚Ñé; avendo lo stesso rango dimostriamo che il sistema sia compatibile.
Prendiamo ‚Ñé colonne indipendenti di ùê¥, che sono indipendenti anche come colonne di ùê¥‚Äô e quindi queste ‚Ñé
colonne di ùê¥ sono una base anche per la matrice completa. Di conseguenza la colonna dei termini noti ùê∂ si
scrive come combinazione lineare delle colonne di ùê¥ ed ho dimostrato l‚Äôimplicazione per il primo criterio di
compatibilit√†. Dimostriamo ora la ‚áí ‚à∂ Supponiamo che ùëÜ sia compatibile quindi bisogna mostrare che i ranghi
siano uguali, Prendiamo ‚Ñé colonne indipendenti di ùê¥ e prendiamo il sottospazio generato da tutte le colonne
della matrice incompleta: ùëâ = ‚å©ùëé1,‚Ä¶ , ùëéùëõ‚å™ = ‚å©ùëéùëñ1, ‚Ä¶ , ùëéùëñ‚Ñé‚å™, ùê∂ ‚àà ùëâ, il sottospazio generato da tutte le colonne
della matrice completa √® uguale al sottospazio di tutte le colonne della matrice incompleta poich√© l‚Äôunica
colonna che potrebbe aggiungere un elemento al sottospazio generato da tutte le colonne sono le incognite
ùë•, che possiamo rimuovere poich√© dipendente dalle altre colonne essendo S compatibile. Ma allora ‚Ñé =
ùëü(ùê¥) = ùëü(ùê¥‚Äô) come volevasi dimostrare.
Esercizio: Sistema a tre equazioni e tre incognite: {
matriciale (
2 1 1
1 ‚àí2 2
3 1 1
| ) e determiniamo un minore fondamentale per la matrice incompleta: 2
4
1
5
ùë†ùëñ ùë£ùëé ùëéùëë
‚Üí     
ùëúùëüùëôùëéùëüùëí
(2 1
1 ‚àí2
‚èü      
det(‚àí4‚àí1)=‚àí5
) ‚Üí (
2 1 1
1 ‚àí2 2
3 1 1
)
‚èü        
‚àí4+6+1‚àí(‚àí6+1+4)=4
√® essa stessa ed ha rango 3, il rango della matrice completa √® anche 3 (min{3,4}) di conseguenza per il
teorema di Rouch√©-Capelli questo sistema √® compatibile.
N.B.: Il teorema di Rouch√©-Capelli si comporta molto bene anche per la determinazione dei valori di un
parametro ‚Ñé per cui il sistema ùëÜ sia compatibile o meno, prendiamo ad esempio il seguente sistema: ùëÜ =
{
2ùë• + ùë¶ + ‚Ñéùëß = 4
ùë• ‚àí 2ùë¶ + 2ùëß = 1
3ùë• + ùë¶ + ùëß = 5
(
2 1 ‚Ñé
1 ‚àí2 2
3 1 1
4
1
5
e prendiamo la matrice incompleta ùê¥ = (
2 1 ‚Ñé
1 ‚àí2 2
3 1 1
) e la matrice completa ùê¥‚Ä≤ =
); determiniamone ora i determinanti in funzione del parametro ‚Ñé: 2 ‚Üí (2 1
1 ‚àí2
) ‚ü∂ ùê¥ ‚à∂
|ùê¥| = ‚àí4 + 6 + ‚Ñé ‚àí (‚àí6‚Ñé + 1 + 4) = 7‚Ñé ‚àí 3, quindi |ùê¥| = 0 ‚áî ‚Ñé =
3
7 (nel caso ci fossero pi√π orlati si deve
prendere il valore comune di ‚Ñé per cui tutti gli orlati abbiano determinante nullo) . Per ‚Ñé =
3
7 si ha ùëü(ùê¥) = 2
di conseguenza il minore fondamentale della matrice incompleta
2ùë• + ùë¶ + ùëß = 4
ùë• ‚àí 2ùë¶ + 2ùë• = 1
3ùë• + ùë¶ + ùëß = 5
, scriviamo questo sistema nella forma
ùê¥ùëã = ùê∂, osserviamo che la colonna dei termini noti ùê∂ pu√≤
26
ma No

ma ùëü(ùê¥‚Ä≤) = 3 poich√© |(
2 1 4
1 ‚àí2 1
3 1 5
\l
/|
)| ‚â† 0 e quindi il sistema √® incompatibile per ‚Ñé =
sistema sar√† compatibile essendo ùëü(ùê¥) = ùëü(ùê¥‚Ä≤) = 3.
Risoluzione di sistemi lineari in situazioni particolari
Metodo migliore ed alternativo rispetto a quello visto per quanto riguarda la riduzione a gradini di un sistema.
Prendiamo un sistema di ùëõ equazioni ed ùëö incognite ùëÜ ‚à∂
{
ùëé11ùë•1 +‚ãØ+ ùëé1ùëöùë•ùëö = ùëê1
‚ãÆ
ùëéùëõ1ùë•1 +‚ãØ+ ùëéùëõùëöùë•ùëö = ùëêùëõ
questo sistema sia compatibile (che si pu√≤ capire con Rouch√©-Capelli) e calcoliamone le soluzioni. Prima di
tutto calcoliamo un minore fondamentale della matrice incompleta ùê¥ che sar√† anche un minore
fondamentale della matrice completa ùê¥‚Ä≤ (essendo il sistema compatibile) ed eliminiamone le righe al di fuori
del minore fondamentale (che possiamo trascurare perch√© tutte le righe al difuori del minore fondamentale
sono dipendenti dalle altre). Quelle che restano sono equazioni indipendenti e di conseguenza avremo il
numero di equazioni minore o uguale al numero di incognite. Vediamo i vari casi vari casi:
1) ùíè equazioni ed ùíè incognite: sappiamo gi√† che esiste un‚Äôunica soluzione per la riduzione a gradini, per√≤
calcoliamola in maniera pi√π veloce; se ci sono ùëõ equazioni ed ùëõ incognite sappiamo che la matrice
incompleta √® una matrice quadrata, e poich√© le righe della matrice quadrata sono indipendenti poich√©
fanno sempre parte del minore fondamentale di partenze, la matrice ùê¥ ha determinante diverso da zero,
per cui ùê¥ùëã = ùê∂ ‚áî ùëã = ùê¥‚àí1ùê∂ (possiamo prendere l‚Äôinverso di ùê¥ perch√© sappiamo che √® diverso da 0),
date queste caratteristiche il sistema di incognite ùëã √® un sistema ben determinato; vado a sviluppare
ùê¥11 ‚ãØ ùê¥ùëõ1
l‚Äôuguaglianza precedente: (
ùë•1
‚ãÆ
ùë•ùëõ
ùëê1ùê¥11+‚ãØ+ùëêùëõùê¥ùëõ1
|ùê¥|
) = |ùê¥| ( ‚ãÆ ‚ã± ‚ãÆ )(
1
ùê¥1ùëõ ‚ãØ ùê¥ùëõùëõ
, ‚Ä¶ , ùë•ùëõ =
ùëê1ùê¥1ùëõ+‚ãØ+ùëêùëõùê¥ùëõùëõ
|ùê¥|
ausiliaria ùêµ1 = (
ùëê1
‚ãÆ
ùëêùëõ
ùëé12 ‚ãØ ùëé1ùëõ
\/\
J\ /
ùëê1
‚ãÆ
ùëêùëõ
), pi√π precisamente avremo che ùë•1 =
, per capire cosa rappresentino questi ùë•ùëñ prendiamo la matrice
ùëéùëõ2 ‚ãØ ùëéùëõùëõ
‚ãÆ ‚ã± ‚ãÆ ) che differisce da ùê¥ per la prima colonna (dove al posto della prima
colonna di ùê¥ ha i termini noti), sviluppando il determinante rispetto la prima colonna avremo |ùêµ1| =
ùëê1ùê¥11 + ùëê2ùê¥21 +‚ãØ+ ùëêùëõùê¥ùëõ1 (che per definizione di complemento algebrico sono uguali sia per la
matrice ùêµ1 che per ùê¥), analogamente si trovano i casi ùë•2,‚Ä¶ , ùë•ùëõ con le matrici ùêµ2, ‚Ä¶ , ùêµùëõ, di conseguenza
posso applicare direttamente il procedimento ùë•1 =
|ùêµ1|
|ùê¥| , ‚Ä¶ , ùë•ùëõ =
sistema, le precedenti formule sono note come Regola di Cramer.
2) ùíè equazioni < ùíé incognite: Sia il sistema ùëÜ ‚à∂ {
ùëé11ùë•1 +‚ãØ+ ùëé1ùëöùë•ùëö = ùëê1
‚ãÆ
ùëéùëõ1ùë•1 +‚ãØ+ ùëéùëõùëöùë•ùëö = ùëêùëõ
riferimento da esempio ùëÜ = {
2ùë• + ùë¶ + ùëß + 2ùë° = 0
ùë• + ùë¶ + ùëß + ùë° = 0
e prendiamo (come nel caso precedente) un minore
fondamentale e portiamo a destra le incognite fuori dalle colonne del minore fondamentale, dunque
avremo ([2 1
1 1 1 1 0
] 1 2
| 0
) ovvero {
2ùë• + ùë¶ = ‚àíùëß ‚àí 2ùë°
ùë• + ùë¶ = ‚àíùëß ‚àí ùë°
. Consideriamo fissate le incognite esterne, quindi
avremo un sistema di due equazioni e due incognite {
2ùë• + ùë¶ = ùëê1
ùë• + ùë¶ = ùëê2
in cui le due equazioni in questione
sono indipendenti poich√© appartenenti alle righe di un minore fondamentale, di conseguenza ci troviamo
nel caso precedente di ùëõ equazioni ed ùëõ incognite; applichiamo Cramer: ùë• = ùëê1 ‚àí ùëê2 e ùë¶ = 2ùëê2 ‚àí ùëê1; ora
sostituendo avremo ùë• = ùëê1 ‚àí ùëê2 = ‚àíùëß ‚àí 2ùë° + ùëß + ùë° = ‚àíùë° e ùë¶ = 2ùëê2 ‚àí ùëê1 = 2(‚àíùëß ‚àí ùë°) + ùëß + 2ùë° = ‚àíùëß
dunque l‚Äôinsieme delle soluzioni sar√† ùëÜ = {(‚àíùë°, ‚àíùëß, ùëß, ùë°)|ùëß, ùë° ‚àà ‚Ñù} e sostanzialmente ùëÜ √® indeterminato
e ha ‚àû2 soluzioni.
27
, usiamo come sistema di
|ùêµùëõ|
|ùê¥|
per trovarmi le soluzioni del
e supponiamo che
3
7, mentre per ‚Ñé ‚â†
3
7 il

Sistemi omogenei
Come gi√† visto in precedenza un sistema omogeno √® un sistema che ha per incognite tutti zeri, ovvero un
sistema del tipo ùëÜ ‚à∂ ùê¥ùëã = 0. Pi√π precisamente ùëÜ ‚à∂ {
ùëé11ùë•1 +‚ãØ+ ùëé1ùëöùë•ùëö = 0
‚ãÆ
ùëéùëõ1ùë•1 +‚ãØ+ ùëéùëõùëöùë•ùëö = 0
Proposizione: l‚Äôinsieme delle soluzioni rappresenta un sottospazio vettoriale di ‚Ñùùëö, ovvero ùëÜ ‚â§ ‚Ñùùëö
Dimostrazione: Per determinare che sia un sottospazio vettoriale bisogna dimostrare che ùëÜ ‚â† ‚àÖ, che √® stabile
rispetto alla somma e che √® stabile rispetto al prodotto. Ogni sistema omogeneo ammette almeno la
soluzione banale (0, ‚Ä¶ ,0) ‚àà ùëÜ e quindi ùëÜ non √® vuoto. Per verificare che sia stabile rispetto alla somma
prendiamo due soluzioni ùëå1, ùëå2 ‚àà ùëÜ (sono n-uple) che essendo soluzioni del sistema omogeneo possiamo
scrivere ùê¥ùëå1 = 0 ed ùê¥ùëå2 = 0 ‚áí ùê¥(ùëå1 + ùëå2) = ùê¥ùëå1 + ùê¥ùëå2 = 0 + 0 = 0 (stabile rispetto alla somma). La
stabilit√† rispetto il prodotto si dimostra prendendo un ùúÜ ‚àà ‚Ñù; allora si ha che ùê¥(ùúÜùëå1) = ùúÜ(ùê¥ùëå1) = ùúÜ0 = 0.
Trovare una base per il sottospazio delle soluzioni di un sistema omogeneo: sia ùëÜ = {
2ùë• + ùë¶ + ùëß + 2ùë° = 0
ùë• + ùë¶ + ùëß + ùë° = 0
il
nostro sistema omogeneo, allora (come fatto precedentemente) posso scrivere il mio sistema nel seguente
modo: {
2ùë• + ùë¶ = ‚àíùëß ‚àí 2ùë°
ùë• + ùë¶ = ‚àíùëß ‚àí ùë° , avr√≤ quindi il sistema di soluzioni generico ùëÜ = {(ùõº(ùëß, ùë°), ùõΩ(ùëß, ùë°), ùëß, ùë°)|ùëß, ùë° ‚àà ‚Ñù},
allora potendo scegliere ùëß e ùë° in maniera arbitraria scegliamo i valori che rendano i vettori indipendenti,
dunque
ùëß = 1, ùë° = 0 ‚Üí (ùõº(1,0), ùõΩ(1,0), 1,0)
ùëß = 0, ùë° = 1 ‚Üí (ùõº(0,1), ùõΩ(0,1), 0,1)
(usualmente si pone a 1 una variabile e 0 sulle altre prendendo
tutte le possibilit√†, in questo caso ho solo due scelte). In questo modo i precedenti vettori sono anche un
sistema di generatori, prendiamo infatti una generica soluzione (ùë•, ùë¶, ùëß, ùë°‚èü) ‚àà ùëÜ, avr√≤ (ùõº(ùëß, ùë°), ùõΩ(ùëß, ùë°), ùëß, ùë°‚èü)
che sono dunque due soluzioni del sistema, ma (come visto precedentemente) per Cramer esiste un‚Äôunica
soluzione per il sistema {
2ùë• + ùë¶ = ‚àíùëß ‚àí 2ùë°
ùë• + ùë¶ = ‚àíùëß ‚àí ùë° , questo vuol dire che le quadruple coincidono in tutto per tutto
(ogni volta che le quadruple di soluzioni coincidono sui valori al di fuori del minore fondamentale devono
coincidere su tutte le incognite) quindi esiste un'unica soluzione e allora ùõº(ùëß, ùë°) = ùë• e ùõΩ(ùëß, ùë°) = ùë¶. Abbiamo
cos√¨ dimostrato che ùëÜ √® un sottospazio vettoriale, per dimostrare che sia un sistema di generatori prendiamo
ùëß(ùõº(1,0), ùõΩ(1,0), 1,0) + ùë°(ùõº(0,1), ùõΩ(0,1), 0,1) = (‚àó,‚àó, ùëß, ùë°) ‚àà ùëÜ ed ha gli stessi identici valori della soluzione
(ùë•, ùë¶, ùëß, ùë°) allora per Cramer (‚àó,‚àó, ùëß, ùë°) = (ùë•, ùë¶, ùëß, ùë°) e quindi questa quadrupla √® combinazione lineare dei
due vettori (ùõº(1,0), ùõΩ(1,0), 1,0) e (ùõº(0,1), ùõΩ(0,1), 0,1).
Relazione tra sistema generale ed omogeneo associato: in termini matriciali ùëÜ ‚à∂ ùê¥ùëã = ùê∂ e ùëÜ0 ‚à∂ ùê¥ùëã = 0 esiste
una relazione tra il sistema generale ùëÜ ed il sistema omogeneo ùëÜ0 per il teorema seguente:
Teorema: Fissato ùëå ‚àà ùëÜ come soluzione del mio sistema completo ùëÜ allora:
1) ‚àÄùëç ‚àà ùëÜ ‚àÉùëç0 ‚àà ùëÜ0 ‚à∂ ùëç = ùëå + ùëç0 (posso ottenere la soluzione ùëç a partire da ùëå e da una soluzione ùëç0
del mio sistema omogeneo)
2) ‚àÄùëç0 ‚àà ùëÜ0 ‚à∂ ùëå + ùëç0 ‚àà ùëÜ (soluzione particolare del sistema ùëÜ, il punto 1 e 2 sono uno il viceversa
dell‚Äôaltro)
Dimostrazione: Punto 2: ùê¥(ùëå + ùëç0) = ùê¥ùëå + ùê¥ùëç0 = ùê∂ + 0 = ùê∂ quindi soddisfa il sistema generale ùëÜ ed allora
ùëå + ùëç0 appartiene ad ùëÜ. Per dimostrare il punto 1 prendiamo una qualunque soluzione ùëç di ùëÜ e sapendo che
anche ùëå ‚àà ùëÜ scriviamo ùê¥(ùëç ‚àí ùëå) = ùê¥ùëç ‚àí ùê¥ùëå = ùê∂ ‚àí ùê∂ = 0 ‚áí ùëç ‚àí ùëå ‚àà ùëÜ0 e dunque ùëç = ùëå + (ùëç ‚àí ùëå‚èü  
ùëç0
).
Questo teorema ci dice che se voglio descrivere l‚Äôinsieme delle soluzioni del mio sistema generale S quello
che io devo andare a fare √® trovarmi una singola soluzione per S dopodich√© vado a considerare il sistema
omogeneo associato, di quest‚Äôultimo vado a trovare l‚Äôinsieme delle soluzioni; e tutte le altre soluzioni si
28

possono descrivere come somma dell‚Äôunica soluzione che mi sono trovato pi√π una qualunque combinazione
lineare degli elementi della base dello spazio delle soluzioni del sistema omogeneo.
Esempio: Sia ùëÜ = {
2ùë• + ùë¶ + ùëß + 2ùë° = 1
ùë• + ùë¶ + ùëß + ùë° = 1
il sistema generico e ùëÜ0 = {
2ùë• + ùë¶ + ùëß + 2ùë° = 0
ùë• + ùë¶ + ùëß + ùë° = 0
il sistema
omogeneo associato, che abbiamo gi√† precedentemente calcolato e quindi sappiamo che l‚Äôinsieme di
soluzioni di ùëÜ0 √® ùëÜ0 = ‚å©(0, ‚àí1,1,0), (‚àí1,0,0,1)‚å™, cerchiamo ora una soluzione per ùëÜ (poniamo ùëß = ùë° = 0):
ùëå ‚àà ùëÜ ‚à∂
{
2ùë• + ùë¶ = 1
ùë• + ùë¶ = 1 ‚áí ùë• = 0, ùë¶ = 1 e allora abbiamo trovato che la quadrupla ùëå = (0,1,0,0) ‚àà ùëÜ √®
soluzione del sistema generale ùëÜ; dunque possiamo descrivere l‚Äôinsieme delle soluzioni ùëÜ = ùëå + ùëÜ0 ovvero
come somma di ùëå ed una soluzione di ùëÜ0 (cos√¨ abbiamo descritto tutte le soluzioni di ùëÜ).
Risoluzione sistema omogeneo con ùíè ‚àí ùüè equazioni indipendenti ed ùíè incognite
Sia il seguente sistema omogeneo generico: ùëÜ ‚à∂
{
ùëé11ùë•1 + ùëé12ùë•2 +‚ãØ+ ùëé1ùëõùë•ùëõ = 0
‚ãÆ
, il fatto che
ùëé(ùëõ‚àí1),1ùë•1 + ùëé(ùëõ‚àí1),2ùë•2 +‚ãØ+ ùëé(ùëõ‚àí1),ùëõùë•ùëõ = 0
si abbia ùëõ ‚àí 1 equazioni indipendenti vuol dire sostanzialmente che la matrice incompleta (anche quella
completa poich√© l‚Äôaggiunta della colonna di zeri √® ininfluente) ha tutte le righe che sono indipendenti, di
conseguenza avremo per la matrice ùê¥ = (
ùëé11
‚ãÆ
‚ãØ ùëé1ùëõ
‚ãÆ
‚ã±
ùëé(ùëõ‚àí1),1 ‚ãØ ùëé(ùëõ‚àí1),ùëõ
) il rango massimo: ùëü(ùê¥) = ùëõ ‚àí 1. Inoltre,
grazie alla teoria sino ad ora studiata quello che si trova √® che la dimensione delle soluzioni del sistema
dimùëÜ = 1 (essendo un sistema omogeneo la dimensione del sottospazio generato √® sostanzialmente il
numero di incognite meno il rango di un minore fondamentale della matrice incompleta), di conseguenza per
poter descrivere ùëÜ ci basta trovare una soluzione non nulla e tutti i vettori ad esso proporzionali saranno
tutti e soli vettori soluzioni di questo sistema.
In questo caso particolare, per trovare una soluzione del sistema si usa una formula che si ricava nel modo
seguente: prendiamo tutti i minori di ordine ùëõ ‚àí 1 di ùê¥ (le sottomatrici quadrate di ordine ùëõ ‚àí 1):
ùê¥1, ùê¥2, ‚Ä¶ , ùê¥ùëõ e lasciamo fuori una singola colonna per avere ùëõ ‚àí 1 righe (dunque ho ùëõ possibilit√†), di queste
colonne abbiamo la certezza che ce ne sia almeno una con determinante non nullo (poich√© il minore
fondamentale √® ùê¥ stesso, essendo le righe indipendenti) quindi avr√≤ per ùúÜ1 = det ùê¥1 , ‚Ä¶ , ùúÜùëõ = det ùê¥ùëõ
almeno un ùúÜùëñ ‚â† 0; allora la n-upla dei determinanti dei minori presi a segni alterni √® effettivamente una
soluzione del mio sistema, quindi (ùúÜ1,‚àíùúÜ2,‚Ä¶ , (‚àí1)ùëõ‚àí1ùúÜùëõ) ‚àà ùëÜ. Inoltre, poich√© quest‚Äôultima √® una n-upla
non tutta nulla, l‚Äôoggetto (ùúÜ1,‚àíùúÜ2,‚Ä¶ , (‚àí1)ùëõ‚àí1ùúÜùëõ) costituir√† una base per le soluzioni di ùëÜ e questo significa
che tutte le altre soluzioni le posso ottenere moltiplicando uno scalare per quella n-upla. Ora ci resta da
dimostrare che (ùúÜ1, ‚àíùúÜ2,‚Ä¶ , (‚àí1)ùëõ‚àí1ùúÜùëõ) sia effettivamente soluzione per ùëÜ, a tal scopo utilizziamo la
seguente matrice ausiliaria ùê∂1 = (
ùëé11 ‚Ä¶ ùëé1ùëõ
ùëé11
‚ãØ ùëé1ùëõ
‚ãÆ
‚ã±
‚ãÆ
ùëé(ùëõ‚àí1),1 ‚ãØ ùëé(ùëõ‚àí1),ùëõ
), che differisce dalla matrice ùê¥ per la prima riga
duplicata, e questo significa che det ùê∂1 = 0 per il teorema degli orlati. Ora possiamo sviluppare il
determinante della matrice rispetto al prima riga, avremo quindi ùúÜ1ùëé11 ‚àí ùúÜ2ùëé12 +‚ãØ+ (‚àí1)ùëõ‚àí1ùúÜùëõùëé1ùëõ = 0,
ma questa equazione rappresenta nient‚Äôaltro che la prima equazione del sistema ùëÜ (basta sostituire i ùúÜùëñ con
gli ùë•ùëñ), quindi non solo ho soddisfatto la prima equazioni del sistema ma questi oggetti rappresentano anche
la n-upla che volevamo dimostrare appartenente alle soluzioni di ùëÜ, e quindi ho dimostrato che la n-upla
(ùúÜ1,‚àíùúÜ2,‚Ä¶ , (‚àí1)ùëõ‚àí1ùúÜùëõ) √® quantomeno soluzione della prima equazione di ùëÜ, questo processo si pu√≤ iterare
con altre matrici ausiliari e risulter√† che (ùúÜ1,‚àíùúÜ2,‚Ä¶ , (‚àí1)ùëõ‚àí1ùúÜùëõ) √® effettivamente soluzione per ùëÜ.
Esempio: ùëÜ ‚à∂
{
2ùë• + 2ùë¶ + 3ùëß = 0
2ùë• + ùë¶ + 4ùëß = 0
ùê¥ = (2 2 3
2 1 4
) si ha che ùëÜ = ‚å© ( |2 3
1 4
| , ‚àí |2 3
2 4
| , |2 2
2 1
| ) ‚å™
29

5. Applicazioni Lineari
Definizione
Un‚Äôapplicazione lineare non √® nient‚Äôaltro che un‚Äôapplicazione che soddisfa delle propriet√†, dunque siano
ùëâ, ùëâ‚Ä≤ spazi vettoriali, ùëì ‚à∂ ùëâ ‚Üí ùëâ‚Ä≤ viene detta lineare se soddisfa le seguenti propriet√†:
1) Linearit√† rispetto la somma: ‚àÄùë£, ùë§ ‚àà ùëâ, ùëì(ùë£ + ùë§) = ùëì(ùë£) + ùëì(ùë§)
l‚Äôimmagine della somma √® uguale alla somma delle immagini
2) Linearit√† rispetto al prodotto: ‚àÄùúÜ ‚àà ‚Ñù e ‚àÄùë£ ‚àà ùëâ, ùëì(ùúÜùë£) = ùúÜùëì(ùë£)
analogo del concetto di omomorfismo in algebra
Le applicazioni lineari sono conosciute anche con la nomenclatura di omomorfismi (lineari), altre
nomenclature sono:
‚Ä¢ Se ùëâ = ùëâ‚Ä≤ si parla di endomorfismo
‚Ä¢ Se ùëì √® iniettiva di monomorfismo
‚Ä¢ Se ùëì √® suriettiva di epimorfismo
‚Ä¢ Se ùëì √® biettiva di isomorfismo
‚Ä¢ L‚Äôendomorfismo biettivo si dice automorfismo
Esempi
1) ùëì: ùëâ ‚Üí ùëâ, l‚Äôapplicazione che porta ogni vettore nel vettore nullo (ùë£ ‚Ü¶ 0), √® un endomorfismo detto
endomorfismo nullo (applicazione nulla). √à un applicazione lineare poich√© le due propriet√† sono
banalmente verificate (analogo ragionamento per la funzione identica (ùë£ ‚Ü¶ ùë£))
2) ùëì: (ùë•, ùë¶) ‚àà ‚Ñù2 ‚Üí (ùë•, ùë• + ùë¶, ùë¶) ‚àà ‚Ñù3 √® un applicazione lineare poich√© per (ùë•, ùë¶), (ùë•‚Ä≤, ùë¶‚Ä≤) ‚àà ‚Ñù2 risulta
ùëì((ùë•, ùë¶) + (ùë•‚Ä≤, ùë¶‚Ä≤)) = ùëì(ùë• + ùë•‚Ä≤, ùë¶ + ùë¶‚Ä≤) = (ùë• + ùë•‚Ä≤, ùë• + ùë•‚Ä≤ + ùë¶ + ùë¶‚Ä≤, ùë¶ + ùë¶‚Ä≤) = (ùë•, ùë• + ùë¶, ùë¶) +
(ùë•‚Ä≤, ùë•‚Ä≤ + ùë¶‚Ä≤, ùë¶‚Ä≤) = ùëì(ùë•, ùë¶) + ùëì(ùë•‚Ä≤, ùë¶‚Ä≤) (lineare rispetto la somma) mentre per (ùë•, ùë¶) ‚àà ‚Ñù2 e ‚Ñé ‚àà ‚Ñù si ha
ùëì(‚Ñé(ùë•, ùë¶)) = ùëì(‚Ñéùë•, ‚Ñéùë¶) = (‚Ñéùë•, ‚Ñéùë• + ‚Ñéùë¶, ‚Ñéùë¶) = ‚Ñé(ùë•, ùë• + ùë¶, ùë¶) = ‚Ñé ‚ãÖ ùëì(ùë•, ùë¶) (lineare rispetto il prodotto)
3) ùëì: ‚Ñù2 ‚Üí ‚Ñù, l‚Äôapplicazione √® la proiezione della prima coordinata ((ùë•, ùë¶) ‚Ü¶ ùë•), √® un applicazione lineare,
e le propriet√† sono banalmente verificate. L‚Äôapplicazione non √® iniettiva poich√© ad esempio (1,2) e (1,3)
hanno la stessa immagine, ma √® suriettiva poich√© ‚àÄùë¶ ‚àà ‚Ñù , ‚àÉùë• ‚àà ‚Ñù ‚à∂ (ùë•, ùë¶) ‚Üí ùë•; di conseguenza si tratta
di un epimorfismo (non monomorfismo).
4) ùëì: ‚Ñù2[ùë•] ‚Üí ‚Ñù3, questa applicazione va dallo spazio dei polinomi di grado al pi√π due in ‚Ñù3, ed associa
ùëéùë•2 + ùëèùë• + ùëê ‚üº (ùëé, ùëè, ùëê). Questa applicazione √® lineare: ùëì((ùëéùë•2 + ùëèùë• + ùëê) + (ùëé‚Ä≤ùë•2 + ùëè‚Ä≤ùë• + ùëê‚Ä≤)) =
ùëì((ùëé + ùëé‚Ä≤)ùë•2 + (ùëè + ùëè‚Ä≤)ùë• + (ùëê + ùëê‚Ä≤)) = (ùëé + ùëé‚Ä≤, ùëè + ùëè‚Ä≤, ùëê + ùëê‚Ä≤) = (ùëé, ùëè, ùëê) + (ùëé‚Ä≤, ùëè‚Ä≤, ùëê‚Ä≤) = ùëì(ùëéùë•2 +
ùëèùë• + ùëê) + ùëì(ùëé‚Ä≤ùë•2 + ùëè‚Ä≤ùë• + ùëê‚Ä≤), ed anche ùëì(ùë¢(ùëéùë•2 + ùëèùë• + ùëê)) = ùëì((ùë¢ùëé)ùë•2 + (ùë¢ùëè)ùë• + (ùë¢ùëê)) =
(ùë¢ùëé, ùë¢ùëè, ùë¢ùëê) = ùë¢(ùëé, ùëè, ùëê) = ùë¢ ‚ãÖ ùëì(ùëéùë•2 + ùëèùë• + ùëê). √à inoltre sia iniettiva che suriettiva, di conseguenza
quest‚Äôapplicazione √® un isomorfismo (la propriet√† di isomorfismo ci sar√† utile in seguito)
5) ùëì: ‚Ñù ‚Üí ‚Ñù con ùë• ‚Ü¶ ùë•2 non √® un applicazione lineare, infatti ùëì(1 + 2) = ùëì(3) = 9 ‚â† ùëì(1) + ùëì(2) = 5
6) Sia una matrice ùê¥ ‚àà ‚Ñùùëöùëõ di dimensioni arbitrarie definiamo la seguente applicazione lineare (ci sar√†
molto utile) ùëì: ùëã ‚àà ‚Ñùùëõ ‚Üí ùê¥ùëã ‚àà ‚Ñùùëö, √® sicuramente lineare poich√© ùëì(ùëã1 + ùëã2) = ùê¥(ùëã1 + ùëã2) = ùê¥ùëã1 +
ùê¥ùëã2 = ùëì(ùëã1) + ùëì(ùëã2) e ùëì(‚Ñéùëã) = ùê¥(‚Ñéùëã) = ‚Ñé(ùê¥ùëã) = ‚Ñé ‚ãÖ ùëì(ùëã)
Esempio con matrice concreta: ùëì: (ùë•, ùë¶, ùëß) ‚àà ‚Ñù3 ‚üº (3 1 2
/
XN
2 0 1
\f \
) ( ) = (
ùë•
7\ J
ùë¶
ùëß
3ùë• + ùë¶ + 2ùëß
2ùë• + ùëß
) ‚àà ‚Ñù2
7) L‚Äôapplicazione (ùëé
ùëê
2ùëì ((0 0
\*
0 0
ùëè
ùëë
) ‚àà ‚Ñù22 ‚üº (1 1
ùëé
)) = 2 (1 1
7]
XN
0 0
) = (2 2
7
XN
0 0
)
7
30
ùëè
) ‚àà ‚Ñù2 non √® lineare perch√© ùëì (2 (0 0
0 0
)) = (1 1
0 0
) ‚â†

Proposizioni
‚Ä¢ Sia ùëì: ùëâ ‚Üí ùëâ‚Ä≤ un applicazione lineare, allora valgono le seguenti proposizioni:
I. ‚àÄ‚Ñé, ùëò ‚àà ‚Ñù, ‚àÄùë£, ùë§ ‚àà ùëâ, ùëì(‚Ñéùë£ + ùëòùë§) = ‚Ñé ‚ãÖ ùëì(ùë£) + ùëò ‚ãÖ ùëì(ùë§)
ùëì(0ùëâ) = 0ùëâ‚Ä≤
II.
III.
IV.
ùëì(‚Ñé1ùë£1 + ‚Ñé2ùë£2 +‚ãØ+ ‚Ñéùëõùë£ùëõ) = ‚Ñé1 ‚ãÖ ùëì(ùë£1) +‚ãØ+ ‚Ñéùëõ ‚ãÖ ùëì(ùë£ùëõ)
ùëì(‚àíùë£) = ‚àíùëì(ùë£)
Dimostrazione: (I) applichiamo la linearit√† rispetto la somma e poi la linearit√† rispetto il prodotto su
ciascun addento: ùëì(‚Ñéùë£ + ùëòùë§) = ùëì(‚Ñéùë£) + ùëì(ùëòùë§) = ‚Ñé ‚ãÖ ùëì(ùë£) + ùëò ‚ãÖ ùëì(ùë§).
(II) la dimostrazione che
l‚Äôelemento neutro viene mandato nell‚Äôelemento neutro √® la seguente (si sfrutta la propriet√†
dell‚Äôelemento neutro): ùëì(0) = ùëì(0 + 0) = ùëì(0) + ùëì(0) ‚áí ùëì(0) = ùëì(0) ‚àí ùëì(0) = 0. (III) si dimostra
per induzione su ùëõ: abbiamo gi√† dimostrato l‚Äôasserto vero per ùëõ = 1 nel punto (I), supponiamolo vero
per ùëõ ‚àí 1 e scriviamo ùëì (‚Ñé1ùë£1 + ‚Ñé2ùë£2 +‚ãØ
‚èü          + ‚Ñéùëõùë£ùëõ) = ùëì(‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùëõ‚àí1ùë£ùëõ‚àí1) + ‚Ñéùëõ ‚ãÖ ùëì(ùë£ùëõ) = ‚Ñé1 ‚ãÖ
ùëì(ùë£1) +‚ãØ+ ‚Ñéùëõ‚àí1 ‚ãÖ ùëì(ùë£ùëõ‚àí1) + ‚Ñéùëõ ‚ãÖ ùëì(ùë£ùëõ). (IV) ùëì(‚àíùë£) = ùëì(‚àí1ùë£) = ‚àí1ùëì(ùë£) = ‚àíùëì(ùë£)
‚Ä¢ Sia ùëì: ùëâ ‚Üí ùëâ‚Ä≤ un applicazione lineare
I.
ùëì conserva la dipendenza: ùë£1,‚Ä¶ , ùë£ùë° ‚àà ùëâ dipendente ‚áí ùëì(ùë£1), ‚Ä¶ , ùëì(ùë£ùë°) dipendente in ùëâ‚Ä≤
(mentre la dipendenza lineare viene conservata l‚Äôindipendenza non √® detto che lo sia)
Dim.: ùë£1,‚Ä¶ , ùë£ùë° sono dipendenti quindi esistono ùëñ scalari non tutti nulli tale che la combinazione
lineare corrispondente √® uguale all‚Äôelemento neutro: ‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùë°ùë£ùë° = 0; applicando ora la
funzione ùëì ad ambo i membri di questa uguaglianza avremo che ùëì(‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùë°ùë£ùë°) = ‚Ñé1 ‚ãÖ
ùëì(ùë£1) +‚ãØ+ ‚Ñéùëõ ‚ãÖ ùëì(ùë£ùëõ) = ùëì(0) = 0 (per la (III) e ùëôùëé (II) della proposizione precedente).
II.
Se un vettore appartiene al sottospazio generato da un certo insieme ùëÜ allora l‚Äôimmagine di quel
vettore appartiene al sottospazio generato dall‚Äôimmagine di quel sottoinsieme (conseguenza
della I): ùë£ ‚àà ‚å©ùëÜ‚å™ ‚áí ùëì(ùë£) ‚àà ‚å©ùëì(ùëÜ)‚å™
Dim.: Consideriamo l‚Äôinsieme finito ùë£ = ‚Ñé1ùë†1 +‚ãØ+ ‚Ñéùëõùë†ùëõ con ùëÜ = {ùë†1,‚Ä¶ , ùë†ùëõ}, applicando ùëì ad
entrambi i membri: ùëì(ùë£) = ‚Ñé1 ‚ãÖ ùëì(ùë†1) +‚ãØ+ ‚Ñéùëõ ‚ãÖ ùëì(ùë†ùëõ) e trovo che ùëì(ùë£) ‚àà ‚å©ùëì(ùë†1), ‚Ä¶ , ùëì(ùë†ùëõ)‚å™
III.
IV.
ùëä ‚â§ ùëâ ‚áí ùëì(ùëä) ‚â§ ùëâ‚Ä≤ (l‚Äôimmagine di ogni sottospazio del dominio risulta essere un sottospazio
del codominio, se l‚Äôapplicazione non √® lineare allora √® solo sottoinsieme del codominio).
Dim.: Sappiamo per certo che ùëä ‚â† ‚àÖ poich√© essendo sottospazio contiene almeno l‚Äôelemento
neutro, ma allora anche l‚Äôimmagine ùëì(ùëä) ‚â† ‚àÖ poich√© contiene l‚Äôimmagine dell‚Äôelemento
neutro. Dunque, ci resta da dimostrare che sia stabile rispetto alla somma e al prodotto. Siano
ùë£‚Ä≤, ùë§‚Ä≤ ‚àà ùëì(ùëä) ‚áí ‚àÉùë£, ùë§ ‚àà ùëä ‚à∂ ùëì(ùë£) = ùë£‚Ä≤, ùëì(ùë§) = ùë§‚Ä≤; a questo punto dobbiamo dimostrare
che ùë£‚Ä≤ + ùë§‚Ä≤ ‚àà ùëì(ùëä), a tal proposito dobbiamo trovare un oggetto all‚Äôinterno di ùëä tale che la
sua immagine sia proprio ùë£‚Ä≤ + ùë§‚Ä≤; ossia ùë£ + ùë§: ùëì(ùë£) + ùëì(ùë§) = ùëì(ùë£ + ùë§) = ùë£‚Ä≤ + ùë§‚Ä≤ ‚àà ùëì(ùëä).
Allo stesso modo, per il prodotto: ‚Ñéùë£ = ‚Ñé ‚ãÖ ùëì(ùë£) = ùëì(‚Ñéùë£) = ‚Ñéùë£‚Ä≤ ‚àà ùëì(ùëä).
ùëä = ‚å©ùë§1,‚Ä¶ , ùë§ùëõ‚å™ ‚áí ùëì(ùëä) = ‚å©ùëì(ùë§1),‚Ä¶ , ùëì(ùë§ùëõ)‚å™ (se ùëä √® generato da ùë§1,‚Ä¶ , ùë§ùëõ
allora un
insieme di generatori per il sottospazio immagine viene dato dalle immagini dei generatori)
Dim.: ùë§1,‚Ä¶ , ùë§ùëõ ‚àà ùëä ‚áí ùëì(ùë§1),‚Ä¶ , ùëì(ùë§ùëõ) ‚àà ùëì(ùëä). Allora, se ci appartengono tutti gli oggetti del
tipo ùëì(ùë§ùëñ) anche il sottospazio generato da tutti gli ùëì(ùë§ùëñ) √® contenuto all‚Äôinterno di ùëì(ùëä), in
simboli: ‚å©ùëì(ùë§ùëñ), ‚Ä¶ , ùëì(ùë§ùëõ)‚å™ ‚àà ùëì(ùëä); abbiamo cos√¨ dimostrato una delle due inclusioni. A questo
punto dimostriamo che ogni elemento di ùëì(ùëä) ‚àà ‚å©ùëì(ùë§1),‚Ä¶ , ùëì(ùë§ùëõ)‚å™, ovvero che ogni elemento
ùë§‚Ä≤ ‚àà ùëì(ùëä) si pu√≤ scrivere come combinazione lineare degli elementi ùëì(ùë§1), ‚Ä¶ , ùëì(ùë§ùëõ). A tal
proposito sia ùë§‚Ä≤ ‚àà ùëì(ùëä) ‚áí ‚àÉùë§ ‚àà ùëä ‚à∂ ùë§‚Ä≤ = ùëì(ùë§), essendo ùë§ ‚àà ùëä = ‚å©ùë§1,‚Ä¶ , ùë§ùëõ‚å™ sappiamo
che certi scalari ‚Ñéùëñ ‚àà ‚Ñù ‚à∂ ùë§ = ‚Ñé1ùë§1 +‚ãØ+ ‚Ñéùëõùë§ùëõ; ma allora si ha ùë§‚Ä≤ = ùëì(‚Ñé1ùë§1 +‚ãØ+ ‚Ñéùëõùë§ùëõ) =
‚Ñé1 ‚ãÖ ùëì(ùë§1) +‚ãØ+ ‚Ñéùëõ ‚ãÖ ùëì(ùë§ùëõ) e dunque ùë§‚Ä≤ si pu√≤ scrivere come combinazione lineare dei ùëì(ùë§ùëñ)
e di conseguenza, per doppia inclusione, ùëì(ùëä) = ‚å©ùëì(ùë§1),‚Ä¶ , ùëì(ùë§ùëõ)‚å™.
31

Concetto di Ker e sue propriet√†
Definizione: Sia ùëì: ùëâ ‚Üí ùëâ‚Ä≤ un applicazione lineare, denotiamo il sottospazio immagine di ùëì con Im ùëì ed √®
l‚Äôinsieme {ùëì(ùë£) | ùë£ ‚àà ùëâ} = ùëì(ùëâ) = Im ùëì; un altro insieme utile √® il cosiddetto ker (nucleo, dal tedesco
kernel) che √® l‚Äôinsieme di tutti gli oggetti del dominio che vengono portati nell‚Äôelemento neutro, ovvero
ùê§ùêûùê´ ùíá = {ùíó ‚àà ùëΩ ‚à∂ ùíá(ùíó) = ùüé}. Inoltre, il ker √® anche un sottospazio vettoriale.
Proposizione: ker ùëì ‚â§ ùëâ
Im ùëì ‚â§ ùëâ‚Ä≤
Dimostrazione: Im ùëì poich√© coincide con ùëì(ùëâ) √® sottospazio vettoriale per la proposizione vista
precedentemente. ker ùëì ‚â† ‚àÖ poich√© 0 ‚àà ker ùëì per definizione. Siano ora ùë£, ùë§ ‚àà ker ùëì ‚áí ùë£ + ùë§ ‚àà ker ùëì ci√≤
significa che ùëì(ùë£) = 0 = ùëì(ùë§) ‚áí ùëì(ùë£ + ùë§) = ùëì(ùë£) + ùëì(ùë§) = 0 + 0 = 0 ‚áí ùëì(ùë£ + ùë§) ‚àà ker ùëì; allo stesso
modo per ùúÜ ‚àà ‚Ñù si ha ùëì(ùúÜùë£) = ùúÜ ‚ãÖ ùëì(ùë£) = ùúÜ0 = 0 ‚áí ùëì(ùúÜùë£) ‚àà ker ùëì.
Corollario: Sia ùëì: ùëâ ‚Üí ùëâ‚Ä≤ lineare, con ùëâ spazio vettoriale finitamente generato. Se prendo una base ùëí1,‚Ä¶ , ùëíùëõ
di ùëâ allora Im ùëì = ‚å©ùëì(ùëí1),‚Ä¶ , ùëì(ùëíùëõ)‚å™ (√® generato ma non √® detto che sia essa stessa base).
Essendo un corollario della proposizione IV vista precedentemente la dimostrazione √® inutile.
Proposizione: Sia ùëì: ùëâ ‚Üí ùëâ‚Ä≤ lineare allora ùüè) ùëì ùë†ùë¢ùëüùëñùëíùë°ùë°ùëñùë£ùëé ‚áî Im ùëì = ùëâ‚Ä≤
ùüê) ùëì ùëñùëõùëñùëíùë°ùë°ùëñùë£ùëé ‚áî ker ùëì = {0}
Dimostrazione: la prima √® ovvia e precedentemente osservata. La seconda ci dice che una funzione √® iniettiva
se il ker √® il singleton dell‚Äôelemento neutro e lo si dimostra per doppia implicazione. ‚áí: sappiamo che 0 ‚àà
ker ùëì ‚áí {0} ‚äÜ ker ùëì, supponiamo che ùë£ ‚àà ker ùëì ‚áí ùëì(ùë£) = 0 ma anche ùëì(0) = 0 dunque per l‚Äôiniettivit√†
abbiamo che ùë£ = 0 e questo comporta anche l‚Äôinclusione di ker ùëì ‚äÜ {0}. ‚áê: prendiamo due oggetti ùëì(ùë£) e
ùëì(ùë§) ‚àà ker ùëì, dunque sar√† ùëì(ùë£) = ùëì(ùë§) ‚áí ùëì(ùë£) ‚àí ùëì(ùë§) = 0 ‚áí ùëì(ùë£ ‚àí ùë§) = 0 ‚áí ùë£ ‚àí ùë§ = 0 ‚áí ùë£ = ùë§.
Proposizione (correlazione tra indipendenza e iniettivit√† di una funzione): Sia ùëì: ùëâ ‚Üí ùëâ‚Ä≤ un monomorfismo
(omomorfismo iniettivo) succede che l‚Äôindipendenza viene conservata, dunque: ùë£1, ‚Ä¶ , ùë£ùëõ ‚àà ùëâ indipendenti
‚áí ùëì(ùë£1), ‚Ä¶ , ùëì(ùë£ùëõ) indipendenti. (In particolare: base di ùëâ ‚áí base di ùëì(ùëâ))
Dimostrazione: Prendiamo una combinazione lineare dei vettori immagine ed eguagliamola all‚Äôelemento
neutro: ‚Ñé1 ‚ãÖ ùëì(ùë£1) +‚ãØ+ ‚Ñéùëõ ‚ãÖ ùëì(ùë£ùëõ) = 0, affinch√© questi vettori siano indipendenti bisogna dimostrare che
tutti gli scalari siano necessariamente pari a zero. Per la propriet√† di linearit√† di ùëì si ha che il primo membro
si pu√≤ rileggere come ùëì(‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùëõùë£ùëõ) e quindi ‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùëõùë£ùëõ ‚àà ker ùëì, ed appartenendo al ker deve
essere pari all‚Äôelemento neutro e dunque per l‚Äôiniettivit√† abbiamo ‚Ñé1ùë£1 +‚ãØ+ ‚Ñéùëõùë£ùëõ = 0 ed essendo questi
vettori indipendenti allora ‚Ñé1 = ‚ãØ = ‚Ñéùëõ = 0 e quindi ùëì(ùë£1),‚Ä¶ , ùëì(ùë£ùëõ) sono indipendenti.
Teorema: Se ùëì: ùëâùëõ ‚Üí ùëâ allora la seguente uguaglianza √® vera: dim(ker ùëì) + dim(Im ùëì) = ùëõ
Dimostrazione: Se il ker ùëì = ùëâùëõ ‚áí ùëõ + 0 = ùëõ essendo Im ùëì pari al singleton dell‚Äôelemento neutro. Mentre
per ker ùëì = {0} significa che ùëì √® iniettiva ed allora presa una base del dominio ùëí1,‚Ä¶ , ùëíùëõ di ùëâ allora (per la
precedente proposizione) ùëì(ùëí1),‚Ä¶ , ùëì(ùëíùëõ) √® base di Im ùëì e quindi avendo trovato una base di dimensione ùëõ
si ha che dim(ker ùëì) + dim(Im ùëì) = 0 + ùëõ = ùëõ. Supponiamo ora che il {0} < ker ùëì < ùëâùëõ: sia ùë£1, ‚Ä¶ , ùë£ùë° base
di ker ùëì e completiamola in una base di ùëâùëõ aggiungendo almeno un vettore: ùë£1, ‚Ä¶ , ùë£ùë°, ùë£ùë°+1,‚Ä¶ , ùë£ùëõ base di ùëâùëõ.
Quello da dimostrare √® che dim(Im ùëì) = ùëõ ‚àí ùë° che √® il numero di vettori usati per completare la base di ùëâùëõ,
quindi bisogna dimostrare che ùëì(ùë£ùë°+1),‚Ä¶ , ùëì(ùë£ùëõ) sia una base di Im ùëì: prendiamo il sistema di generatori
per Im ùëì = ‚å©ùëì(ùë£1), ‚Ä¶ , ùëì(ùë£ùë°), ùëì(ùë£ùë°+1),‚Ä¶ , ùëì(ùë£ùëõ)‚å™, ma essendo ùëì(ùë£1), ‚Ä¶ , ùëì(ùë£ùë°) pari all‚Äôelemento neutro
possono essere rimossi dal sistema di generatori e dunque si ha che Im ùëì = ‚å©ùëì(ùë£ùë°+1),‚Ä¶ , ùëì(ùë£ùëõ)‚å™; rimane da
dimostrare che ùëì(ùë£ùë°+1),‚Ä¶ , ùëì(ùë£ùëõ) sono indipendenti: ‚Ñéùë°+1 ‚ãÖ ùëì(ùë£ùë°+1) +‚ãØ+ ‚Ñéùëõ ‚ãÖ ùëì(ùë£ùëõ) = ùëì(‚Ñéùë°+1ùë£ùë°+1 +
‚ãØ+ ‚Ñéùëõùë£ùëõ) = 0 ‚áí ‚Ñéùë°+1ùë£ùë°+1 +‚ãØ+ ‚Ñéùëõùë£ùëõ ‚àà ker ùëì e quindi questo vettore pu√≤ essere riscritto come
combinazione lineare della base ùë£1, ‚Ä¶ , ùë£ùë° e quindi ‚àÉ‚Ñé1,‚Ä¶ , ‚Ñéùë° ‚àà ‚Ñù ‚à∂ ‚Ñéùë°+1ùë£ùë°+1 +‚ãØ+ ‚Ñéùëõùë£ùëõ = ‚Ñé1ùë£1 +‚ãØ+
‚Ñéùë°ùë£ùë°; portando a sinistra entrambi i membri avremo che : ‚Ñéùë°+1ùë£ùë°+1 +‚ãØ+ ‚Ñéùëõùë£ùëõ ‚àí ‚Ñé1ùë£1 +‚ãØ‚àí ‚Ñéùë°ùë£ùë° = 0 ed
essendo ùë£1, ‚Ä¶ , ùë£ùë°, ùë£ùë°+1,‚Ä¶ , ùë£ùëõ base si ha che ‚Ñé1 = ‚ãØ = ‚Ñéùë° = ‚Ñéùë°+1 = ‚Ñéùëõ = 0. E questo dimostra che questi
vettori sono indipendenti ed un sistema di generatori e dunque una base di dimensione ùëõ ‚àí ùë°.
32

Isomorfismo coordinato
L‚Äôisomorfismo coordinato √® quel concetto per il quale uno spazio vettoriale ha una forma un po‚Äô pi√π semplice,
ad esempio sia ùëâùëõ uno spazio vettoriale di dimensione ùëõ e prendiamo un riferimento ‚Ñõ = (ùëí1,‚Ä¶ , ùëíùëõ) allora
posso considerare la seguente applicazione ùê∂‚Ñõ ‚à∂ ùë£ = ‚Ñé1ùëí1 +‚ãØ+ ‚Ñéùëõùëíùëõ ‚àà ùëâùëõ ‚Ü¶ (‚Ñé1, ‚Ñé2, ‚Ä¶ , ‚Ñéùëõ) ‚àà ‚Ñùùëõ, questa
applicazione porta un vettore ùë£ in una n-upla determinata, avendo il vettore ùë£ l‚Äôunicit√† di scrittura. ùê∂‚Ñõ √®
dunque ben definita ed √® anche lineare (dimostrare la sua linearit√† per esercizio), inoltre si ha che ker ùê∂‚Ñõ =
{ùë£ ‚àà ùëâùëõ
‚à∂ ùê∂‚Ñõ(ùë£) = (0, ‚Ä¶ ,0)} = {0} e quindi ùë£ = 0ùëí1 +‚ãØ+ 0ùëíùëõ = 0 di conseguenza si ha che l‚Äôisomorfismo
coordinato √® iniettivo. Dimostriamo anche la sua suriettivit√†: Im ùê∂‚Ñõ = ‚å©ùê∂‚Ñõ(ùëí1),‚Ä¶ , ùê∂‚Ñõ(ùëíùëõ)‚å™ dove ùê∂‚Ñõ(ùëí1) √® il
primo vettore del riferimento canonico di ‚Ñùùëõ e cos√¨ via per gli altri, e dunque avremo che Im ùê∂‚Ñõ =
‚å©(1,0,‚Ä¶ ,0),‚Ä¶ , (0,‚Ä¶ ,0,1)‚å™ = ‚Ñùùëõ e quindi l‚Äôapplicazione √® suriettiva.
Teorema: Ogni spazio vettoriale finitamente generato (diverso da 0) di dimensione ùëõ √® isomorfo ad ‚Ñùùëõ
Proposizione: Sia ùëì: ùëâ ‚Üí ùëâ‚Ä≤ isomorfismo lineare (applicazione lineare biettiva, quindi ha un‚Äôinversa) allora
l‚Äôinversa ùëì‚àí1: ùëâ‚Ä≤ ‚Üí ùëâ √® anch‚Äôessa un isomorfismo lineare.
Dimostrazione: Iniziamo col dimostrare che ùëì‚àí1 sia lineare rispetto alla somma, siano ùë£‚Ä≤, ùë§‚Ä≤ ‚àà ùëâ‚Ä≤ (dominio
di ùëì‚àí1 e codominio di ùëì) dobbiamo dimostrare che ùëì‚àí1(ùë£‚Ä≤ + ùë§‚Ä≤) = ùëì‚àí1(ùë£‚Ä≤) + ùëì‚àí1(ùë§‚Ä≤), poich√© ùë£‚Ä≤, ùë§‚Ä≤ ‚àà
ùëâ‚Ä≤ e data la suriettivit√† di ùëì possiamo dire che ‚àÉùë£, ùë§ ‚àà ùëâ ‚à∂ ùë£‚Ä≤ = ùëì(ùë£) e ùë§‚Ä≤ = ùëì(ùë§); a questo punto possiamo
scrivere ùëì‚àí1(ùë£‚Ä≤ + ùë§‚Ä≤) = ùëì‚àí1 (ùëì(ùë£) + ùëì(ùë§)) = ùëì‚àí1 (ùëì(ùë£ + ùë§)) = ùë£ + ùë§ = ùëì‚àí1(ùë£‚Ä≤) + ùëì‚àí1(ùë§‚Ä≤); ora in
modo analogo dimostriamo la linearit√† rispetto al prodotto: sia ùúÜ ‚àà ‚Ñù si ha ùëì‚àí1(ùúÜùë£‚Ä≤) = ùëì‚àí1 (ùúÜ ‚ãÖ ùëì(ùë£)) =
ùëì‚àí1 (ùëì(ùúÜùë£)) = ùúÜùë£ = ùúÜ ‚ãÖ ùëì‚àí1(ùë£‚Ä≤). L‚Äôaltra implicazione si dimostra praticamente con la proposizione stessa.
Corollario: Sia ùëì: ùëâ ‚Üí ùëâ‚Ä≤ un isomorfismo lineare allora:
I.
ùë£1,‚Ä¶ , ùë£ùëõ sono indipendente se e solamente se le loro corrispettive immagini sono indipendenti.
In simboli: ùë£1,‚Ä¶ , ùë£ùëõ indipendenti ‚áî ùëì(ùë£1), ‚Ä¶ , ùëì(ùë£ùëõ) indipendenti.
L‚Äôimplicazione da sinistra a destra √® vera per la correlazione tra indipendenza e iniettivit√† di una
funzione, da destra a sinistra invece (essendo ùëì un isomorfismo) posso per la proposizione
precedente dire che le immagini di ùëì(ùë£1),‚Ä¶ , ùëì(ùë£ùëõ) sono anch‚Äôesse indipendenti.
II.
III.
ùë£1,‚Ä¶ , ùë£ùëõ dipendenti ‚áî ùëì(ùë£1), ‚Ä¶ , ùëì(ùë£ùëõ) dipendenti.
In generale la dipendenza lineare viene conservata, in pi√π se √® isomorfismo allora vale anche il
contrario, la dimostrazione √® semplicemente la negazione della precedente.
Sia ùëä ‚â§ ùëâ allora ùë§1,‚Ä¶ , ùë§ùëõ base di ùëä ‚áî ùëì(ùë£1), ‚Ä¶ , ùëì(ùë£ùëõ) base di ùëì(ùëä).
L‚Äôimplicazione da sinistra a destra √® dimostrata in una proposizione precedente, per l‚Äôaltra
implicazione utilizziamo di nuovo il corollario, per cui sapendo che ùëì √® un isomorfismo basta
applicare ùëì‚àí1 per dimostrare l‚Äôimplicazione, essendo ùëì‚àí1 anch‚Äôesso un isomorfismo lineare.
Esempio: Sia ùëä ‚â§ ‚Ñù2[ùë•] dove ùëä = ‚å©ùë•2 + 2ùë•, ùë• ‚àí 1, 2ùë•2 + 3ùë•, ùë•2 + 3ùë• ‚àí 2‚å™, l‚Äôidea √® quella di utilizzare
la coordinazione associata allo spazio dei polinomi ‚Ñù2[ùë•] che √® ‚Ñù3, prendendo come riferimento ùë•2 + ùë• + 1
e di conseguenza posso scrivere ùëì(ùëä) = ‚å©(1,2,0), (0,1, ‚àí1), (2,3,0), (1,3, ‚àí2)‚å™; utilizzando la III del
precedente corollario sappiamo che ùëì(ùëä) √® anch‚Äôesso un isomorfismo e che √® generato dalle terne descritte
precedentemente, a questo punto, per trovare una base del sottospazio ùëä basta trovare una base di ùëì(ùëä)
e invertirla.
Teorema: Un applicazione lineare ùëì: ùëâùëõ ‚Üí ùëâùëö
‚Ä≤ risulta essere ‚Äúnota‚Äù quando sono noti i corrispondenti dei
vettori di una base (significa che conosco come agiscono tutti i vettori del dominio ogni volta che conosce
come agisce sui vettori di una base)
Dimostrazione: Supponiamo che ùëí1,‚Ä¶ , ùëíùëõ sia una base del dominio ùëâùëõ e supponiamo che siano noti le basi
ùëì(ùëí1), ‚Ä¶ , ùëì(ùëíùëõ), allora conosco anche l‚Äôapplicazione lineare valutata in un vettore ùë£ ‚àà ùëâùëõ poich√© posso
33

scrivere ùë£ come combinazione della base ùëí1,‚Ä¶ , ùëíùëõ e dunque ùëì(ùë£) = ùëì(‚Ñé1ùëí1 +‚ãØ+ ‚Ñéùëõùëíùëõ) = ‚Ñé1 ‚ãÖ ùëì(ùëí1) +
‚ãØ+ ‚Ñéùëõ ‚ãÖ ùëì(ùëíùëõ); e di conseguenza √® noto.
Forma canonica delle applicazioni lineari e matrice di passaggio
Prendiamo un applicazione lineare ùëì: ùëâùëõ ‚Üí ùëâùëö
‚Ä≤ ed un riferimento del dominio e uno del codominio,
rispettivamente ‚Ñõ = (ùëí1, ‚Ä¶ , ùëíùëõ) di ùëâùëõ e ‚Ñõ‚Ä≤ = (ùëí1
‚Ä≤ ,‚Ä¶ , ùëíùëö
‚Ä≤ ) di ùëâùëö. Per ogni vettore del dominio scriviamo la su
immagine come combinazione lineare dei ùëí1
‚Ä≤ ,‚Ä¶ , ùëíùëö
‚Ä≤
‚Ä≤ +‚ãØ+ ùëéùëöùëõùëíùëö
ùëé1ùëõùëí1
, quindi: ùëì(ùëí1) = ùëé11ùëí1
‚Ä≤ +‚ãØ+ ùëéùëö1ùëíùëö
‚Ä≤
,‚Ä¶ , ùëì(ùëíùëõ) =
‚Ä≤ . A questo punto possiamo creare una matrice con i coefficienti nel seguente modo:
ùê¥ = ( ‚ãÆ ‚ã± ‚ãÆ ) ‚àà ‚Ñùùëö,ùëõ ; questa matrice √® molto utile per poter descrivere la funzione ùëì e prende il
ùëé11 ‚ãØ ùëé1ùëõ
ùëéùëö1 ‚ãØ ùëéùëöùëõ
nome di matrice associata ad ùíá nei riferimenti ùì° ed ùì°‚Ä≤; usualmente descritta con il simbolo ùëÄ‚Ñõ‚Ñõ‚Ä≤(ùëì). Il
procedimento cos√¨ descritto per la definizione √® anche il metodo in cui va calcolata la matrice associata. In
particolare, se la funzione ùëì √® un endomorfismo (dominio uguale al codominio) e ‚Ñõ = ‚Ñõ‚Ä≤ allora sar√† ùëÄ‚Ñõ(ùëì).
Propriet√† della matrice associata: Sia ùë£ ‚àà ùëâùëõ, se definiamo con ùëã la colonna delle componenti di ùë£ nel
riferimento del dominio ‚Ñõ e con ùëã‚Ä≤ i vettori colonna delle componenti di ùëì(ùë£) in ‚Ñõ‚Ä≤ allora ùëã e ùëã‚Ä≤ sono legati
dalla seguente relazione: ùëã‚Ä≤ = ùê¥ùëã (Quindi se mi viene fornita la matrice associata e conosco le componenti
del vettore nel riferimento del dominio allora posso ottenere le componenti nel riferimento di arrivo a
prescindere da come √® definita la funzione ùëì).
Dimostrazione: ùëã‚Ä≤ = (ùë•1
‚Ä≤ ,‚Ä¶ , ùë•ùëö
‚Ä≤ ) ‚àà ‚Ñùùëö e ùëã = (ùë•1,‚Ä¶ , ùë•ùëõ) ‚àà ‚Ñùùëõ (sono entrambi colonne, ma per praticit√† le
scrivo in riga per considerarli come oggetti in ‚Ñùùëö e ‚Ñùùëõ) poich√© lo spazio del codominio e del dominio sono
rispettivamente ùëö ed ùëõ. Sia ùë£ ‚àà ùëâùëõ per ipotesi (ùëã componenti di ùë£ in ‚Ñõ) posso scriverla come combinazione
lineare: ùë£ = ùë•1ùëí1 +‚ãØ+ ùë•ùëõùëíùëõ. A questo punto applico ùëì ad entrambi i membri dell‚Äôuguaglianza, risulter√†
dunque ùëì(ùë£) = ùë•1 ‚ãÖ ùëì(ùëí1) +‚ãØ+ ùë•ùëõ ‚ãÖ ùëì(ùëíùëõ), ma essendo ùëì(ùëí1) = ùëé11ùëí1
‚Ä≤ +‚ãØ+ ùëéùëö1ùëíùëö
‚Ä≤
,‚Ä¶ , ùëì(ùëíùëõ) =
‚Ä≤ +‚ãØ+ ùëéùëöùëõùëíùëö
ùëé1ùëõùëí1
‚Ä≤
otteniamo ùëì(ùë£) = ùë•1(ùëé11ùëí1
‚Ä≤ +‚ãØ+ ùëéùëö1ùëíùëö
‚Ä≤ ) +‚ãØ+ ùë•ùëõ(ùëé1ùëõùëí1
‚Ä≤ +‚ãØ+ ùëéùëöùëõùëíùëö
‚Ä≤ ) =
(ùë•1ùëé11 +‚ãØ+ ùë•ùëõùëé1ùëõ)ùëí1
‚Ä≤ +‚ãØ+ (ùë•1ùëéùëö1 +‚ãØ+ ùë•ùëõùëéùëöùëõ)ùëíùëö
‚Ä≤
anche ùëì(ùë£) = ùë•1
‚Ä≤ ùëí1
‚Ä≤ +‚ãØ+ ùëíùëö
‚Ä≤
e poich√© ho l‚Äôunicit√† di scrittura e posso scrivere
‚Ä≤ risulter√† che ùë•1
ùë•ùëö
‚Ä≤ = ùë•1ùëé11 +‚ãØ+ ùë•ùëõùëé1ùëõ,‚Ä¶ , ùë•ùëö
‚Ä≤ = ùë•1ùëéùëö1 +‚ãØ+ ùë•ùëõùëéùëöùëõ;
abbiamo cos√¨ scoperto che ùê¥ùëã = ( ‚ãÆ ‚ã± ‚ãÆ )(
ùëé11 ‚ãØ ùëé1ùëõ
ùëéùëö1 ‚ãØ ùëéùëöùëõ
\f \ f
J\ J
ùë•1
‚ãÆ
ùë•ùëõ
) = (
ùë•1ùëé11 +‚ãØ+ ùë•ùëõùëé1ùëõ
‚ãÆ
\ùë•1ùëéùëö1 +‚ãØ+ ùë•ùëõùëéùëöùëõ
) = ( ‚ãÆ ) = ùëã‚Ä≤.
‚Ä≤
ùë•1
‚Ä≤
ùë•ùëö
Osservazione: la propriet√† della matrice associata √® in realt√† una propriet√† caratterizzante la matrice, nel
senso che se ho una matrice ùê¥ di ùëõ righe ed ùëö colonne e questa matrice √® tale da avere la seguente propriet√†:
ùëã‚Ä≤ = ùê¥ùëã dove ùëã = ùê∂‚Ñõ(ùë£) (colonna delle componenti di ùë£ nel riferimento ‚Ñõ) e ùëã‚Ä≤ = ùê∂‚Ñõ‚Ä≤ (ùëì(ùë£)) per ogni ùë£
allora la matrice ùê¥ = ùëÄ‚Ñõ‚Ñõ‚Ä≤(ùëì).
Proposizione: La matrice (quadrata) di passaggio da ‚Ñõ ad ‚Ñõ‚Ä≤ √® invertibile (la sua inversa risulta essere proprio
la matrice di passaggio da ‚Ñõ‚Ä≤ ad ‚Ñõ).
Dimostrazione: ‚Ñõ ‚Üí ‚Ñõ‚Ä≤ ricordando che la matrice di passaggio si costruisce per colonne prendendo le
componenti del vecchio riferimento (in questo caso ‚Ñõ) nel nuovo riferimento. Quindi se ‚Ñõ = (ùëí1,‚Ä¶ , ùëíùëõ) la
matrice ùê¥ la costruisco con ùê∂‚Ñõ‚Ä≤(ùëì(ùëí1) ) per la prima colonna, ‚Ä¶ e ùê∂‚Ñõ‚Ä≤(ùëì(ùëíùëõ)) per la n-sima colonna. La
matrice ùê¥ √® invertibile perch√© essendo i vettori ùëí1,‚Ä¶ , ùëíùëõ
indipendenti ed essendo la coordinazione un
isomorfismo anche le loro immagini saranno indipendenti; quindi, i vettori rappresentati le colonne di ùê¥ sono
indipendenti, di conseguenza tutte le colonne di ùê¥ sono indipendenti ed essendo il rango di ùê¥ massimo (per
il teorema degli orlati) segue che det(ùê¥) ‚â† 0, quindi ùê¥ √® invertibile.
Corollario: Se ùëÉ √® la matrice di passaggio da ‚Ñõ ‚Üí ‚Ñõ‚Ä≤, allora ùëÉ‚àí1 √® di passaggio da ‚Ñõ‚Ä≤ ‚Üí ‚Ñõ.
Dimostrazione: ùëÉ √® di passaggio, allora prendo le componenti ùëã = ùê∂‚Ñõ(ùë£) e ùëã‚Ä≤ = ùê∂‚Ñõ‚Ä≤(ùëì(ùë£)) con ùë£ ‚àà ùëâùëõ, e
quindi ùëã‚Ä≤ = ùëÉùëã, moltiplicando entrambi i membri per ùëÉ‚àí1 si ottiene ùëÉ‚àí1ùëã‚Ä≤ = ùëã. Scriviamo in maniera
34

generica la matrice ùëÉ‚àí1 = ( ‚ãÆ ‚ã± ‚ãÆ ) e andiamo a dimostrare che ùëÉ‚àí1 sia effettivamente la matrice
ùëé11 ‚ãØ ùëé1ùëõ
ùëéùëõ1 ‚ãØ ùëéùëõùëõ
di passaggio da ‚Ñõ‚Ä≤ ‚Üí ‚Ñõ. Siano ‚Ñõ = (ùëí1,‚Ä¶ , ùëíùëõ) e ‚Ñõ‚Ä≤ = (ùëí1
‚Ä≤ ,‚Ä¶ , ùëíùëõ
‚Ä≤ ), allora la matrice di passaggio da ‚Ñõ‚Ä≤ ‚Üí ‚Ñõ
(chiamiamola ùêµ) sar√† ùêµ = ( ‚ãÆ ‚ã± ‚ãÆ ) essendo
ùë•11 ‚ãØ ùë•1ùëõ
ùë•ùëõ1 ‚ãØ ùë•ùëõùëõ
‚Ä≤ = ùë•11ùëí1 +‚ãØ+ ùë•ùëõ1ùëíùëõ
ùëí1
‚ãÆ
‚Ä≤ = ùë•1ùëõùëí1 +‚ãØ+ ùë•ùëõùëõùëíùëõ
ùëíùëõ
le componenti del vettore ùëí1
‚Ä≤ che nel riferimento ‚Ñõ sono ùë•11ùëí1 +‚ãØ+ ùë•ùëõ1ùëíùëõ, d‚Äôaltro canto le componenti di
‚Ä≤
ùëí1
nel riferimento ‚Ñõ‚Ä≤ sono ( ) allora la avr√≤: ùëÉ (
1
0
‚ãÆ
0
( ‚ãÆ ‚ã± ‚ãÆ )( ) = (
ùëé11 ‚ãØ ùëé1ùëõ
ùëéùëõ1 ‚ãØ ùëéùëõùëõ
1
0
‚ãÆ
0
ùë•11
ùë•21
‚ãÆ
ùë•ùëõ1
) = ( ) ‚áí (
1
0
‚ãÆ
0
‚èü          
ùëã‚Ä≤=ùëÉùëã
ùë•11
ùë•21
‚ãÆ
ùë•ùëõ1
) = ùëÉ‚àí1 ( ) ‚áí (
1
0
‚ãÆ
0
‚èü            
ùëÉ‚àí1ùëã‚Ä≤=ùëã
ùëé11
ùëé21
‚ãÆ
ùëéùëõ1
) di conseguenza la prima colonna di ùëÉ‚àí1 √® uguale alla prima colonna di ùêµ,
iterando questo procedimento per i vettori ùëí2
‚Ä≤ ,‚Ä¶ , ùëíùëõ
‚Ä≤ risulter√† che ogni colonna di ùëÉ‚àí1 sar√† uguale alla
rispettiva colonne di ùêµ dunque, come volevasi dimostrare, la matrice di passaggio da ‚Ñõ‚Ä≤ ‚Üí ‚Ñõ √® proprio ùëÉ‚àí1.
6. Matrici simili e diagonalizzazione (spazi vettoriali)
Matrici simili
La definizione di matrici simili riguarda matrici quadrate, dunque: siano ùê¥, ùê¥‚Ä≤ ‚àà ‚Ñùùëõ diremo che ùê¥ √® simile ad
ùê¥‚Ä≤, in simboli ùê¥ ‚àº ùê¥‚Ä≤, se e soltanto se esiste una matrice quadrata ùëÉ con determinante diverso da zero (quindi
invertibile) tale che la ùëÉ‚àí1ùê¥ùëÉ = ùê¥‚Ä≤. Definizione in simboli: ùê¥ ‚àº ùê¥‚Ä≤ ‚áî ‚àÉùëÉ ‚à∂ |ùëÉ| ‚â† 0 e ùëÉ‚àí1ùê¥ùëÉ = ùê¥‚Ä≤ (il
prodotto ùëÉ‚àí1ùê¥ùëÉ √® chiamato in ambito algebrico anche coniugato di ùê¥ mediante ùëÉ).
La similitudine √® una relazione d‚Äôequivalenza, cio√® riflessiva, simmetrica e transitiva:
‚Ä¢ Riflessiva: ùê¥ ‚àº ùê¥ infatti ùêºùëõùê¥ùêºùëõ = ùê¥ essendo |ùêºùëõ| = 1 e ùêºùëõ
‚àí1 = ùêºùëõ
ùëÉ‚àí1ùê¥ùëÉ
=
‚áì
‚Ä¢ Simmetrica: se ùê¥ ‚àº ùê¥‚Ä≤ allora ùê¥‚Ä≤ ‚àº ùê¥: infatti
ùëÉ(ùëÉ‚àí1ùê¥ùëÉ)ùëÉ‚àí1 =
‚áì
ùê¥‚Ä≤
ùëÉùê¥‚Ä≤ùëÉ‚àí1
ùê¥
= (ùëÉ‚àí1)‚àí1ùê¥‚Ä≤ùëÉ‚àí1
‚Ä¢ Transitiva: se ùê¥ ‚àº ùêµ ùëí ùêµ ‚àº ùê∂ ‚áí ùê¥ ‚àº ùê∂: infatti per ùê¥ ‚àº ùêµ esiste un ùëÉ invertibile tale che ùëÉ‚àí1ùê¥ùëÉ = ùêµ, per
ùêµ ‚àº ùê∂ esiste invece un ùëÉ1 invertibile tale che ùëÉ1
‚àí1ùêµùëÉ1 = ùê∂, ma questo vuol dire che per sostituzione si
ha ùê∂ = ùëÉ1
‚àí1ùêµùëÉ1 = ùëÉ1
‚àí1(ùëÉ‚àí1ùê¥ùëÉ)ùëÉ1 = (ùëÉ1
‚àí1ùëÉ‚àí1)ùê¥(ùëÉùëÉ1) = (ùëÉùëÉ1)‚àí1ùê¥(ùëÉùëÉ1) e quindi ùê¥ ‚àº ùê∂ essendo ùëÉùëÉ1
una matrice invertibile con determinante non nullo per il teorema di Cauchy-Binet.
Teorema: Sia ùëì: ùëâùëõ ‚Üí ùëâùëõ un endomorfismo e siano ‚Ñõ e ‚Ñõ‚Ä≤ riferimenti di ùëâùëõ allora ùê¥ = ùëÄ‚Ñõ(ùëì) ‚àº ùëÄ‚Ñõ‚Ä≤(ùëì) = ùê¥‚Ä≤
(mette in relazione le matrici associate tra due riferimenti di un endomorfismo, ùê¥ ‚àº ùê¥‚Ä≤).
Dimostrazione: Sia ùëÉ la matrice di passaggio dal riferimento ‚Ñõ al riferimento ‚Ñõ‚Ä≤ allora per ùë£ ‚àà ùëâùëõ ho ùëã =
ùê∂‚Ñõ(ùë£) e ùëã‚Ä≤ = ùê∂‚Ñõ‚Ä≤(ùë£), mentre per l‚Äôimmagine di ùë£ ho ùëå = ùê∂‚Ñõ (ùëì(ùë£)) e ùëå‚Ä≤ = ùê∂‚Ñõ‚Ä≤ (ùëì(ùë£)). Ora, poich√© ùëÉ √® la
matrice di passaggio da ‚Ñõ a ‚Ñõ‚Ä≤ ho le seguenti relazioni: ùëã‚Ä≤ = ùëÉùëã e ùëå‚Ä≤ = ùëÉùëå; mentre per la propriet√† di
matrice associata ad una funzione ho ùëå = ùê¥ùëã e ùëå‚Ä≤ = ùê¥‚Ä≤ùëã‚Ä≤ a questo punto posso eseguire i seguenti passaggi:
ùëå‚Ä≤ = ùëÉùëå = ùê¥‚Ä≤ùëã‚Ä≤ = ùê¥‚Ä≤ùëÉùëã ‚áí ùëå = ùëÉ‚àí1ùê¥‚Ä≤ùëÉ‚èü    
ùëç
ùëã (moltiplicando entrambi i membri a sinistra per ùëÉ‚àí1); il prodotto
ùëç √® una matrice tale che ogni volta che moltiplico a destra per le componenti di un certo vettore nel
35
ùë•11
ùë•21
‚ãÆ
ùë•ùëõ1
) =
; a questo punto, prendiamo
| a ‚Ñ¢~
ao ‚Ñ¢~
nN
NL S

riferimento ‚Ñõ ottengo le componenti in ùëÖ‚Ä≤, quindi ùëç soddisfa la propriet√† di matrice associata. Ma questa
propriet√† definisce univocamente la matrice associata in un riferimento, e allora ùëÉ‚àí1ùê¥‚Ä≤ùëÉ = ùê¥ ‚áí ùê¥‚Ä≤ ‚àº ùê¥.
Diagonalizzazione di un endomorfismo
Sia ùê¥ = (
ùëé11 ‚ãØ ùëé1ùëõ
ùëéùëõ1 ‚ãØ ùëéùëõùëõ
‚ãÆ ‚ã± ‚ãÆ ) ‚àà ‚Ñùùëõ ne vado a fare il determinante: det(ùê¥ ‚àí ùë°ùêºùëõ) dove ùë° √® un‚Äôincognita. Il
determinante di questa matrice √® detto polinomio caratteristico della matrice ùê¥, ne consegue che con
equazione caratteristica intendiamo det(ùê¥ ‚àí ùë°ùêºùëõ) = 0. ùê¥ ‚àí ùë°ùêºùëõ √® sostanzialmente una matrice del seguente
tipo ùê¥ ‚àí ùë°ùêºùëõ = (
ùëé11 ‚àí ùë° ‚ãØ ùëé1ùëõ
‚ãÆ
‚ã±
‚ãÆ
ùëéùëõ1
‚ãØ ùëéùëõùëõ ‚àí ùë°
).
Lemma: Matrici simili hanno lo stesso polinomio caratteristico.
Dimostrazione: Se ùê¥ ‚àº ùê¥‚Ä≤ allora esiste un ùëÉ invertibile tale che ùê¥‚Ä≤ = ùëÉ‚àí1ùê¥ùëÉ, tentiamo a questo punto di
confutare il polinomio caratteristico di ùê¥‚Ä≤, quindi det(ùê¥‚Ä≤ ‚àí ùë°ùêºùëõ) = det(ùëÉ‚àí1ùê¥ùëÉ ‚àí ùë°ùêºùëõ). Osserviamo che la
matrice identica ùêºùëõ = ùëÉ‚àí1ùëÉ, ne consegue det(ùê¥‚Ä≤ ‚àí ùë°ùêºùëõ) = det(ùëÉ‚àí1ùê¥ùëÉ ‚àí ùë°ùëÉ‚àí1ùëÉ) = det[ùëÉ‚àí1(ùê¥ùëÉ ‚àí ùë°ùëÉ)], √®
possibile mettere in evidenza ùëÉ‚àí1 a sinistra grazie alla distributivit√† del prodotto righe per colonne sulla
somma di matrici, e per lo stesso motivo metto in evidenza a destra ùëÉ: det(ùê¥‚Ä≤ ‚àí ùë°ùêºùëõ) = det[ùëÉ‚àí1(ùê¥ ‚àí ùë°ùêºùëõ)ùëÉ],
applicando Cauchy-Binet det[ùëÉ‚àí1(ùê¥ ‚àí ùë°ùêºùëõ)ùëÉ] = det(ùëÉ‚àí1) det(ùê¥ ‚àí ùë°ùêºùëõ) det ùëÉ, essendo numeri reali posso
spostarli a piacere, dunque det(ùëÉ‚àí1) det(ùëÉ) det(ùê¥ ‚àí ùë°ùêºùëõ) = det(ùê¥ ‚àí ùë°ùêºùëõ) essendo det(ùëÉ‚àí1) l‚Äôinverso di
det(ùëÉ) e quindi det(ùëÉ‚àí1ùëÉ) = det(ùêºùëõ) = 1. Abbiamo cos√¨ trovato che det(ùê¥‚Ä≤ ‚àí ùë°ùêºùëõ) = det(ùê¥ ‚àí ùë°ùêºùëõ).
Diamo le seguenti definizioni:
‚Ä¢ Sia ùëì: ùëâùëõ ‚Üí ùëâùëõ un endomorfismo, fissato un riferimento ‚Ñõ ne prendiamo la matrice associata ùê¥ = ùëÄ‚Ñõ(ùëì)
e definisco il polinomio caratteristico di ùíá come il polinomio caratteristico di ùê¥ (quindi della matrice
associata al suo riferimento). Il polinomio caratteristico √® ben definito, dunque non dipende dal
riferimento scelto perch√© le matrici associate sono simili per un teorema visto nel capitolo precedente.
‚Ä¢ Un endomorfismo ùëì: ùëâùëõ ‚Üí ùëâùëõ √® detto diagonalizzabile se esiste almeno un riferimento ‚Ñõ in cui la matrice
associata ùëÄ‚Ñõ(ùëì) √® diagonale (matrice quadrata dove solo la diagonale pu√≤ avere valori diversi da zero).
‚Ä¢ Una matrice ùê¥ ‚àà ‚Ñùùëõ √® detta diagonalizzabile se √® simile ad una diagonale.
Proposizione (correlazione delle precedenti definizioni): Sia ùëì: ùëâùëõ ‚Üí ùëâùëõ un endomorfismo diagonalizzabile
allora ogni matrice associata ad ùëì (non √® detto che sia diagonale) √® diagonalizzabile.
Dimostrazione: Poich√© l‚Äôendomorfismo √® diagonalizzabile ‚àÉ‚Ñõ di ùëâùëõ
‚à∂ ùëÄ‚Ñõ(ùëì) √® diagonale. A questo punto
‚àÄ‚Ñõ‚Ä≤ di ùëâùëõ succede che ùëÄ‚Ñõ‚Ä≤(ùëì) ‚àº ùëÄ‚Ñõ(ùëì) ‚áí ùëÄ‚Ñõ‚Ä≤(ùëì) √® diagonalizzabile per definizione, essendo simile ad una
matrice diagonale.
Proposizione: Sia ùëì: ùëâùëõ ‚Üí ùëâùëõ un endomorfismo e sia ‚Ñõ un riferimento di ùëâùëõ con ùëÄ‚Ñõ(ùëì) diagonalizzabile allora
ùëì √® effettivamente diagonalizzabile.
Dimostrazione: Per semplicit√† ùê¥ = ùëÄ‚Ñõ(ùëì); so che questa matrice, essendo diagonalizzabile, √® simile ad una
matrice diagonale e quindi esiste una matrice ùëÉ invertibile tale che ùëÉ‚àí1ùê¥ùëÉ = ùê∑ con ùê∑ una certa matrice
diagonale. Sia ‚Ñõ = (ùëí1,‚Ä¶ , ùëíùëõ) cominciamo con definire le componenti del riferimento ‚Ñõ di ùëí1,‚Ä¶ , ùëíùëõ ovvero
ùëã1 = ùê∂‚Ñõ(ùëí1) = (1,0,‚Ä¶ ,0),‚Ä¶ , ùëãùëõ = ùê∂‚Ñõ(ùëíùëõ) = (0,‚Ä¶ ,0,1); definiamo con ùêπùëù: ùëã ‚àà ‚Ñùùëõ ‚Ü¶ ùëÉùëã ‚àà ‚Ñùùëõ
un
applicazione lineare che porta l‚Äôoggetto ùëã (vettore colonna) in un altro vettore colonna ùëÉùëã, questa tipologia
di applicazione √® un isomorfismo lineare poich√© ùêπùëù √® invertibile ed ha inversa ùêπùëÉ‚àí1: ùëã ‚Üí ùëÉ‚àí1ùëã. Inoltre,
sapendo che ùëã1, ‚Ä¶ , ùëãùëõ sono linearmente indipendenti anche le immagini ùëÉùëã1, ‚Ä¶ , ùëÉùëãùëõ sono linearmente
indipendenti, questi n vettori sono n-uple, essendo oggetti di ‚Ñùùëõ, a questo punto vado a fare la
controimmagine nell‚Äôisomorfismo coordinato a riferimento ‚Ñõ di questi n vettori, vado praticamente a
prendere ùê∂‚Ñõ
‚àí1(ùëÉùëã1) = ùëí1
‚Ä≤ ,‚Ä¶ , ùê∂‚Ñõ
‚àí1(ùëÉùëãùëõ) = ùëíùëõ
‚Ä≤ che costituiscono ancora a loro volta vettori indipendenti e
poich√© sono in numero pari alla dimensione dello spazio vettoriale questi vettori formano una base, quindi
36

posso ordinarli secondo i loro pedici e quindi avr√≤ effettivamente il riferimento ‚Ñõ‚Ä≤ = (ùëí1
‚Ä≤ ,‚Ä¶ , ùëíùëõ
‚Ä≤ ). Prendiamo
ora la matrice di passaggio ùêµ da ‚Ñõ‚Ä≤ ad ‚Ñõ, quello che succede (secondo quanto visto nella dimostrazione del
teorema delle matrici simili) √® che la matrice ùêµ‚àí1ùê¥ùêµ = ùê¥‚Ä≤, di conseguenza se dimostriamo che ùêµ = ùëÉ
avremmo dimostrato che ùê¥‚Ä≤ = ùëÄ‚Ñõ‚Ä≤(ùëì) ‚àº ùëÄ‚Ñõ(ùëì) = ùê¥ e quindi la proposizione. La matrice di passaggio si
costruisce per colonne quindi osserviamo la prima colonna di ùêµ, ovvero ùê∂‚Ñõ(ùëí1
‚Ä≤) = ùëÉùëã1 = ùëÉ (
( \
\)
1
0
‚ãÆ
0
),
proseguendo per le restanti colonne troveremo che tutte le colonne di ùêµ coincidono a tutte le colonne di ùëÉ,
e cio√® che ùêµ = ùëÉ e quindi ùê¥‚Ä≤ = ùê∑.
Corollario: Se ùëì: ùëâùëõ ‚Üí ùëâùëõ √® un endomorfismo diagonalizzabile nel riferimento ‚Ñõ allora anche la matrice
associata lo √® (quindi la diagonalizzabilit√† di un endomorfismo pu√≤ essere visto con la diagonalizzabilit√† di
ùëÄ‚Ñõ(ùëì), questo corollario mette praticamente insieme le due proposizioni precedenti).
Esempio: Sia ùê¥ = (
1 0 0
0 2 0
0 0 3
) una matrice diagonale e sia ùêπùê¥: ùëã ‚àà ‚Ñù3 ‚Ü¶ ùê¥ùëã ‚àà ‚Ñù3 endomorfismo, esso √®
diagonalizzabile poich√© nel riferimento naturale la matrice associata ad ùêπùê¥ √® proprio ùê¥. Un altro esempio √®
dato dall‚Äôendomorfismo identico (idùëâ) che √® banalmente diagonalizzabile poich√© in un qualunque riferimento
la matrice associata ùëÄ‚Ñõ(idùëâ) = ùêºùëõ che √® una matrice per sua natura diagonale. Anche l‚Äôendomorfismo nullo
√® diagonalizzabile poich√© in qualunque riferimento la matrice associata √® la matrice nulla (che √® diagonale).
Definizione pratica: Sia ùëì: ùëâùëõ ‚Üí ùëâùëõ un endomorfismo allora un certo elemento ùë£ ‚àà ùëâùëõ si dice autovettore di
autovalore ùùÄ (numero reale) se e solo se ùë£ ‚â† 0 e ùëì(ùë£) = ùúÜùë£.
Proposizione (Il concetto di autovettore e autovalore √® ben definito): Se ùë£ √® un autovettore di autovalori ùúÜ1
e ùúÜ2 allora ùúÜ1 = ùúÜ2 (un autovettore pu√≤ avere un unico e solo autovalore).
Dimostrazione: ùëì(ùë£) = ùúÜ1ùë£ essendo autovettore di autovalore ùúÜ1, ma anche autovalore ùúÜ2 quindi ùëì(ùë£) =
ùúÜ1ùë£ = ùúÜ2ùë£ ‚áí (ùúÜ1 ‚àí ùúÜ2)ùë£ = 0 ‚áí ùúÜ1 ‚àí ùúÜ2 = 0 ‚áí ùúÜ1 = ùúÜ2 poich√© ùë£ ‚â† 0 per definizione.
Teorema: un endomorfismo lineare ùëì: ùëâùëõ ‚Üí ùëâùëõ √® diagonalizzabile ‚áî esiste una base formata da autovettori
(rapporto preliminare tra diagonalizzabilit√† ed esistenza degli autovettori).
Dimostrazione: cominciamo col dimostrare che se ùëì √® diagonalizzabile allora esiste una base di autovettori.
ùëì √® diagonalizzabile significa che esiste un riferimento in cui la matrice √® diagonale, supponiamo che ‚Ñõ =
(ùëí1,‚Ä¶ , ùëíùëõ) sia il riferimento per cui ùëÄ‚Ñõ(ùëì) = (
ùëé1 0 ‚Ä¶
0
‚ãÆ
0
ùëé2
‚ã±
0
‚ãÆ
0
‚Ä¶ 0 ùëéùëõ
), scriviamo le immagini del riferimento
come combinazione lineare delle colonne, ovvero ùëì(ùëí1) = ùëé1ùëí1 + 0ùëí2 +‚ãØ+ 0ùëíùëõ = ùëé1ùëí1,‚Ä¶ , ùëì(ùëíùëõ) = ùëéùëõùëíùëõ
questo significa che ùëí1 √® un autovettore poich√© ùëí1 √® non nullo e la sua immagine √® proporzionale a se stesso,
allo stesso modo ùëí2,‚Ä¶ , ùëíùëõ sono autovettori e dunque il riferimento ‚Ñõ √® formata da autovettori e quindi esiste
effettivamente una base formata da autovettori e la base in questione √® proprio quella data dal riferimento
in cui la funzione ha una matrice associata diagonale. Resta da dimostrare l‚Äôaltra implicazione: sia ‚Ñõ =
(ùëí1,‚Ä¶ , ùëíùëõ) con ùëí1,‚Ä¶ , ùëíùëõ autovettori, questo vuol dire che
‚àÉùúÜ1 ‚àà ‚Ñù ‚à∂ ùëì(ùëí1) = ùúÜ1ùëí1
‚ãÆ
‚àÉùúÜùëõ ‚àà ‚Ñù ‚à∂ ùëì(ùëíùëõ) = ùúÜùëõùëíùëõ
sono gli autovettori a non dover essere nulli, non gli autovalori), di conseguenza la matrice associata ad ùëì nel
riferimento ‚Ñõ √® ùëÄ‚Ñõ(ùëì) = (
ùúÜ1 0 ‚Ä¶
0
‚ãÆ
0
ùúÜ2
‚ã±
0
‚ãÆ
0
‚Ä¶ 0 ùúÜùëõ
).
37
(si tenga presente che

Proposizione: prendiamo un endomorfismo ùëì: ùëâùëõ ‚Üí ùëâùëõ e fissiamone un riferimento ‚Ñõ = (ùëí1,‚Ä¶ , ùëíùëõ) con
ùëÄ‚Ñõ(ùëì) = ùê¥ = (ùëéùëñùëó) allora esiste un autovettore che ammette ‚Ñé come autovalore se e solamente se
det(ùê¥ ‚àí ‚Ñéùêºùëõ) = 0 (quindi per poter trovare gli autovalori bisogna risolvere l‚Äôequazione caratteristica ).
ùë£ √® autovettore di ùëì ‚áî ùëã = ùê∂‚Ñõ(ùë£) √® una soluzione non banale dell‚Äôequazione matriciale (ùê¥ ‚àí ‚Ñéùêºùëõ)ùëã = 0.
Dimostrazione: ‚Ñé √® un autovalore significa che esiste un ùë£ ‚â† 0 tale che ùëì(ùë£) = ‚Ñéùë£, in termini di coordinate
il tutto si traduce nel seguente modo: esiste un ùëç = ùê∂‚Ñõ(ùë£) ‚â† 0 tale che ùê¥ùëç = ‚Ñéùëç, dove ùëç sono le componenti
di ùë£. Ma allora ùê¥ùëç = ‚Ñéùëç ‚áí ùê¥ùëç ‚àí ‚Ñéùêºùëõùëç = 0 ‚áí (ùê¥ ‚àí ‚Ñéùêºùëõ)ùëç = 0, abbiamo cos√¨ trovato che ‚Ñé √® una autovalore
se e solamente se esiste un ùë£ le cui componenti sono diverse da zero e soddisfano l‚Äôequazione (ùê¥ ‚àí ‚Ñéùêºùëõ).
Si osservi l‚Äôequazione (ùê¥ ‚àí ‚Ñéùêºùëõ)ùëã = 0, dove ùëã √® una colonna di incognite che ha 2 soluzioni (matrice nulla o
ùëç) ma questo significa che il sistema di equazione omogeneo ha ‚àû soluzioni per i sistemi di equazioni (che
ha una, nessuna o infinite soluzioni). A questo punto si pu√≤ assumere che il rango di ùê¥ ‚àí ‚Ñéùêºùëõ non √® massimo
(avendo infinite soluzioni) e quindi il det(ùê¥ ‚àí ‚Ñéùêºùëõ) = 0. Abbiamo cos√¨ dimostrato entrambe le proposizioni
poich√© sostanzialmente le implicazioni scritte sopra sono in realt√† equivalenze.
Definizione: sia ‚Ñé autovalore dell‚Äôendomorfismo ùëì: ùëâùëõ ‚Üí ùëâùëõ definisco come autospazio relativo ad ùíâ e lo
denoto ùëâ(‚Ñé) = {ùë£ ‚àà ùëâùëõ|ùëì(ùë£) = ‚Ñé(ùë£)} (non necessariamente ùë£ deve essere non nullo).
Proposizioni:
1) ùëâ(‚Ñé) ‚â§ ùëâùëõ e dimùëâ(‚Ñé) ‚â• 1
Dim.: se ‚Ñé √® un autovalore vuol dire che esiste un vettore non nullo ùë£ ‚àà ùëâ(‚Ñé), di conseguenza ùëâ(‚Ñé) ‚â† ‚àÖ
e la dimensione √® maggiore o uguale ad uno poich√© c‚Äô√® almeno un vettore non nullo, resta da dimostrare
che ùëâ(‚Ñé) sia sottospazio vettoriale e che quindi sia stabile rispetto al somma e il prodotto: siano ùë£, ùë§ ‚àà
ùëâ(‚Ñé) allora ùëì(ùë£) = ‚Ñéùë£ e ùëì(ùë§) = ‚Ñéùë§ (il fatto che siano autovettori o meno non √® rilevante) ne consegue
ùëì(ùë£ + ùë§) = ùëì(ùë£) + ùëì(ùë§) = ‚Ñéùë£ + ‚Ñéùë§ = ‚Ñé(ùë£ + ùë§). Ragionamento analogo per il prodotto, sia ùúÜ ‚àà ‚Ñù si
ha ùëì(ùúÜùë£) = ùúÜ ‚ãÖ ùëì(ùë£) = ùúÜ ‚ãÖ (‚Ñéùë£) = ‚Ñé ‚ãÖ (ùúÜùë£).
2) L‚Äôautospazio √® isomorfo: ùëâ(‚Ñé) ‚âÉ {ùëã|(ùê¥ ‚àí ‚Ñéùêºùëõ)ùëã = 0} = ùëÜ
Dim.: Basta considerare la coordinazione associata e ridurre il domino all‚Äôautospazio, cos√¨ posso
prenderne le componenti che per la proposizione precedente devono appartenere all‚Äôinsieme delle
soluzioni ùëÜ, in simboli ùë£ ‚àà ùëâ(‚Ñé) ‚Ü¶ ùê∂‚Ñõ(ùë£) ‚àà ùëÜ. Viceversa, se prendo un elemento di ùëÜ: se √® nullo allora
appartiene a ùëâ(‚Ñé), altrimenti √® un autovettore e allora sempre per la proposizione precedente posso
trovare un vettore appartenente all‚Äôautospazio che abbia quelle immagini.
3) L‚Äôintersezione di due autospazi con autovalori diversi √® triviale (cio√® fa il singleton dell‚Äôelemento neutro):
ùëâ(‚Ñé) ‚à© ùëâ(ùëò) = {0} se ‚Ñé ‚â† ùëò (in particolare vuol dire che i due sottospazi sono in somma diretta)
Dim.: Prendiamo un qualunque vettore ùë£ ‚àà ùëâ(‚Ñé) ‚à© ùëâ(ùëò) e supponiamo che sia diverso dal vettore nullo,
quindi √® sia autovettore di autovalore ‚Ñé che autovettore di autovalore ùëò, ma poich√© un autovettore pu√≤
avere un unico e solo autovalore si arriverebbe all‚Äôassurdo che ‚Ñé = ùëò, di conseguenza ùë£ = 0
4) Autospazi relativi ad autovalori distinti ùëâ(‚Ñé1),‚Ä¶ , ùëâ(‚Ñéùë°) con ‚Ñéùëñ ‚â† ‚Ñéùëó sono in somma diretta
Dim. per induzione: ùë° = 2 √® vera per la proposizione precedente, supposta vera per ùë° ‚àí 1 dimostriamo
che l‚Äôintersezione di un sottospazio con il sottospazio generato da tutti gli altri sia triviale: ùë£ ‚àà ùëâ(‚Ñé1) ‚à©
‚å©ùëâ(‚Ñé2), ‚Ä¶ , ùëâ(‚Ñéùë°)‚å™ ma ‚å©ùëâ(‚Ñé2),‚Ä¶ , ùëâ(‚Ñéùë°)‚å™ significa che ùëâ(‚Ñé2)‚®Å‚Ä¶‚®Åùëâ(‚Ñéùë°), allora per ùë£ so che ùëì(ùë£) = ‚Ñé1ùë£
poich√© appartiene a ùëâ(‚Ñé1), ma so anche che appartiene al sottospazio generato dai ùë° ‚àí 1 rimanenti e
quindi posso scriverlo come ùë£ = ùë£2 +‚ãØ+ ùë£ùë° e quindi ùëì(ùë£) = ùëì(ùë£2) +‚ãØ+ ùëì(ùë£ùë°) ma essendo ùëì(ùë£2)
oggetto di ùëâ(‚Ñé2) posso scriverlo ùëì(ùë£2) = ‚Ñé2ùë£2‚Ä¶ allora ùëì(ùë£) = ‚Ñé2ùë£2 +‚ãØ+ ‚Ñéùë°ùë£ùë° ma √® anche vero che
posso scrivere ùëì(ùë£) = ‚Ñé1ùë£2 +‚ãØ+ ‚Ñé1ùë£ùë° di conseguenza ‚Ñé1 = ‚Ñé2,‚Ä¶ , ‚Ñé1 = ‚Ñéùë° (essendoci l‚Äôunicit√† di
scrittura per la somma diretta) ma questo contraddice l‚Äôipotesi che gli autovalori siano tutti distinti.
5) Autovettori di autovalori distinti sono indipendenti
Dim.: Siano ùúÜ1,‚Ä¶ , ùúÜùë° autovalori degli autovettori ùë£1, ‚Ä¶ , ùë£ùë°, rispettivamente; supponiamo per assurdo che
gli autovettori non siano indipendenti, cio√® ad esempio ùë£1 ‚àà ‚å©ùë£2,‚Ä¶ , ùë£ùë°‚å™ ma questo significa che ùëâ(ùúÜ1) ‚à©
38

‚å©ùëâ(ùúÜ2),‚Ä¶ , ùëâ(ùúÜùë°)‚å™ ‚â† {0}, e ci√≤ contraddice l‚Äôipotesi che i vettori siano in somma diretta per la propriet√†
precedente.
Corollario: Se l‚Äôequazione caratteristica di un endomorfismo ùëì: ùëâùëõ ‚Üí ùëâùëõ ha ùëõ radici (numero massimo) reali e
distinte allora ùëì √® diagonalizzabile (l‚Äôaltra implicazione non sussiste).
Dimostrazione: Se ha ùëõ radici reali e distinte significa che ciascuna di queste radici per il polinomio
caratteristico √® un autovalore, e dunque esistono ùëõ autovalori ùúÜ1,‚Ä¶ , ùúÜùëõ. Di questi autovalori vado a prendere
uno degli autovettori corrispondenti che lo caratterizzano, supponiamo siano ùë£1,‚Ä¶ , ùë£ùëõ. Questi vettori,
essendo autovettori di autovalori distinti, sono linearmente indipendenti, di conseguenza sono una base di
ùëâùëõ, ma per un teorema precedente se esiste una base formata da autovettori allora ùëì √® diagonalizzabile.
Alcune definizioni:
‚Ä¢ Molteplicit√† algebrica: sia ùëù(ùë•) = ùëéùëõùë•ùëõ + ùëéùëõ‚àí1ùë•ùëõ‚àí1 +‚ãØ+ ùëé1ùë• + ùëé0 = 0 un polinomio esso ha sempre
nel campo dei numeri complessi ‚ÑÇ al pi√π ùëõ soluzioni. Allora posso fattorizzare questo polinomi nel
seguente modo: (ùë• ‚àí ùõº1)ùëõùõº1 ‚ãÖ ‚Ä¶ ‚ãÖ (ùë• ‚àí ùõºùë°)ùëõùõºùë° dove ùëõùõºùëñ √® la molteplicit√† algebrica della radice ùõºùë°. Inoltre,
gli ùõºùëñ sono tutte e sole le soluzioni del polinomio in questione. ùë° ‚â§ ùëõ succede che la molteplicit√† algebrica
delle soluzioni ùëõùõº1 +‚ãØ+ ùëõùõºùë° = ùëõ.
‚Ä¢ Molteplicit√† geometrica: Sia ùëâ(‚Ñé) un autospazio relativo ad ‚Ñé; dimùëâ(‚Ñé) √® detta molteplicit√† geometrica
di ‚Ñé, in simboli ùëöùëî(‚Ñé)
‚Ä¢ La molteplicit√† algebrica di un autovalore ‚Ñé √® ùëöùëé(‚Ñé) ed √® pari alla ùëöùëé(‚Ñé) in det(ùê¥ ‚àí ùë°ùêºùëõ) = 0
Proposizione: ùëöùëî(‚Ñé) ‚â§ ùëöùëé(‚Ñé)
Dimostrazione: Prendiamo una base per ùëâ(‚Ñé) = ùëí1,‚Ä¶ , ùëíùë° con ùë° = ùëöùëî(‚Ñé) per definizione, allora completiamo
ad una base di ùëâùëõ: ùëí1,‚Ä¶ , ùëíùë°, ùëíùë°+1,‚Ä¶ , ùëíùëõ e ne prendo il riferimento corrispondente ‚Ñõ = (ùëí1,‚Ä¶ , ùëíùë°, ùëíùë°+1,‚Ä¶ , ùëíùëõ)
e vado a calcolare la matrice associata al riferimento:
fa st ‚Äú|14-45, (a)Sut 5
sche)
bem,LO)
Corollario: Se ùëöùëé(‚Ñé) = 1 ‚áí ùëöùëî(‚Ñé) = 1
Dimostrazione: Per definizione ùëöùëî(‚Ñé) ‚â• 1, mentre per il teorema precedente ùëöùëî(‚Ñé) ‚â§ ùëöùëé(‚Ñé) dunque
risulter√† 1 ‚â§ ùëöùëî(‚Ñé) ‚â§ ùëöùëé(‚Ñé) = 1
Teorema: Un endomorfismo ùëì: ùëâùëõ ‚Üí ùëâùëõ √® diagonalizzabile ‚áî il polinomio caratteristico di ùëì ha tutte le radici
in ‚Ñù (sono tutte radici reali) e per ogni radice ‚Ñé si ha che ùëöùëé(‚Ñé) = ùëöùëî(‚Ñé).
Dimostrazione: ‚áí: Essendo ùëì diagonalizzabile ‚àÉ‚Ñõ = (ùëí1, ‚Ä¶ , ùëíùëõ) di autovettori, non √® detto che questi vettori
siano autovettori di autovalori distinti, a tal proposito considero i vettori ordinati nel seguente modo:
‚å©ùëí1,‚Ä¶ , ùëíùëñ1‚å™ ‚â§ ùëâ(ùúÜ1),‚Ä¶ , ‚å©ùëíùëñùëö‚àí1+1,‚Ä¶ , ùëíùëñùëõ‚å™ ‚â§ ùëâ(ùúÜùëö) (praticamente metto vicini quelli con lo stesso
autovalore) cos√¨ facendo so che ùëâ(ùúÜ1)‚®Å‚Ä¶‚®Åùëâ(ùúÜùëö) = ùëâ poich√© questi sottospazi contengono delle basi di ùëâ
e ci√≤ vuol dire che ‚å©ùëí1,‚Ä¶ , ùëíùëñ1‚å™ = ùëâ(ùúÜ1),‚Ä¶ , ‚å©ùëíùëñùëö‚àí1+1,‚Ä¶ , ùëíùëñùëõ‚å™ = ùëâ(ùúÜùëö) altrimenti si arriverebbe all‚Äôassurdo
che V appartenga ad un suo sottospazio proprio. Grazie all‚Äôuguaglianza possiamo calcolarci la molteplicit√†
geometrica degli autovalori: ùëöùëî(ùúÜ1) = ùëñ1,‚Ä¶ , ùëöùëî(ùúÜùëö) = ùëõ ‚àí ùëñùëö‚àí1. A questo punto, computata la
molteplicit√† algebrica, andiamo a computare il polinomio caratteristico, riprendiamo il riferimento di
autovettore e prendiamo la matrice associata al riferimento (vedi immagine in seguito).
39

Esplicitando ogni volta il polinomio caratteristico avremo il seguente
determinante: det = (ùúÜ1 ‚àí ùë•)ùëñ1 ‚ãÖ ‚Ä¶ ‚ãÖ (ùúÜùëö ‚àí ùë•)ùëõ‚àíùëñùëö‚àí1 , questo non solo ci
dice che il polinomio caratteristico visto che si decompone in polinomi di primo
grado ha tutte radici reali ma anche che ùëöùëé(ùúÜ1) = ùëñ1,‚Ä¶ , ùëöùëé(ùúÜùëö) = ùëõ ‚àí ùëñùëö‚àí1.
‚áê: scriviamoci il polinomio caratteristico ùëù(ùë•) = (ùë• ‚àí ùúÜ1)ùëõ1 ‚ãÖ ‚Ä¶ ‚ãÖ (ùë• ‚àí ùúÜùë°)ùëõùë°,
poich√© ha tutte le radici in ‚Ñù il polinomio si pu√≤ decomporre nel prodotto di
polinomi di primo grado (a meno del segno); inoltre il prodotto di polinomi ha
come grado la somma dei gradi e quindi ùëõ1 +‚ãØ+ ùëõùë° = ùëõ, sapendo poi che per ogni radice dell‚Äôequazione
caratteristica (per ipotesi) deve succedere che la molteplicit√† geometrica √® uguale a quella algebrica, dunque:
ùëõ1 = ùëöùëî(ùúÜ1),‚Ä¶ , ùëõùë° = ùëöùëî(ùúÜùë°). Andiamo ad analizzare la somma diretta degli autospazi relativi all‚Äôautovalore
ùúÜùëñ: ùëâ(ùúÜ1)‚®Å‚Ä¶‚®Åùëâ(ùúÜùë°) che ha dimensione ùëõ (corollario della relazione di Grassmann) e allora essendo
ùëâ(ùúÜ1)‚®Å‚Ä¶‚®Åùëâ(ùúÜùë°) ‚â§ ùëâ si ha proprio ùëâ(ùúÜ1)‚®Å‚Ä¶‚®Åùëâ(ùúÜùë°) = ùëâ. Bisogna ora dimostrare che il mio
endomorfismo sia diagonalizzabile, dunque mostriamo che esista una base di ùëâ fatta di autovettori; per una
propriet√† della somma diretta (relazione di Grassmann per la somma diretta) prendiamo una base per ogni
autospazio e andiamo a farne l‚Äôunione, questa sar√† una base per ùëâ fatta di autovettori: ùêµ1 ‚à™ ‚Ä¶‚à™ ùêµùë° e a
questo punto per il criterio di diagonalizzabilit√† abbiamo terminato.
Diagonalizzazione di una matrice
Sia ùê¥ ‚àà ‚Ñùùëõ noi sappiamo che ùê¥ √® diagonalizzabile ‚áî ùêπùê¥ √® diagonalizzabile, i concetti di autovalore e
autovettore si trasportano per quanto riguarda la diagonalizzabilit√† per la matrice, definisco dunque
autovettore per ùë®: ùëå ‚àà ‚Ñùùëõ ‚â† 0 ‚à∂ ùê¥ùëå = ‚Ñéùëå con ùëå colonna o riga e ‚Ñé autovalore corrispondente. Invece con
autospazio si intende ùëâ(‚Ñé) = {ùëã ‚àà ‚Ñùùëõ|ùê¥ùëã = ‚Ñéùëã}.
Supponiamo che ùê¥ sia diagonalizzabile, se ùêπùê¥ √® diagonalizzabile (√® lo √® per la propriet√† di diagonalizzabilit√†)
allora esiste un riferimento di autovettori per ùêπùê¥: ‚Ñùùëõ ‚Üí ‚Ñùùëõ. Prendiamo un riferimento di autovettori ‚Ñõ =
(ùëí1,‚Ä¶ , ùëíùëõ), abbiamo quindi la matrice associata ùëÄ‚Ñõ(ùêπùê¥) diagonale, d‚Äôaltro canto, come gi√† osservato, nel
riferimento naturale ùêπùê¥ ha come matrice associata al riferimento naturale proprio ùê¥ = ùëÄ‚Ñõnat(ùêπùê¥). Di
conseguenza sappiamo che matrici associate a due riferimenti dello stesso endomorfismo sono simili, quindi
ùëÄ‚Ñõ(ùêπùê¥) = ùê∑ ‚àº ùê¥ = ùëÄ‚Ñõnat(ùêπùê¥), e questo vuol dire che esiste una matrice ùëÉ invertibile tale che ùëÉ‚àí1ùê¥ùëÉ = ùê∑
dove ùëÉ √® la matrice di passaggio da ‚Ñõ ‚Üí ‚Ñõnat (abbiamo gi√† visto in precedenza la possibilit√† di poter scegliere
ùëÉ come matrice di passaggio). La matrice ùëÉ sappiamo che √® fatta nel seguente modo (costruita per colonne):
Cy? 0 U9 o)++aa 0,0..,4)
¬©Qs
28 (4G-10) +.+ Car(8-. ,0,A)
Le colonne sono i componenti del vecchio nel nuovo, quindi quello che succede √® che, di fatto, ùê∑ √® la matrice
deli autovalori presi nell‚Äôordine.
Praticamente se noi troviamo le componenti degli autovettori nel riferimento naturale le possiamo mettere
nella matrice ùëÉ e questa matrice realizza la diagonalizzazione.
Spazi Vettoriali: Prodotti diretti esterni
Vediamo di seguito un procedimento per la costruzione di ulteriori spazi vettoriali ed √® il concetto di prodotto
esterno. Il prodotto che fino ad ora avevamo visto √® il prodotto diretto interno (dove abbiamo gi√† lo spazio
vettoriale e scrivevamo un prodotto diretto da qualcosa interno allo spazio vettoriale).
40

Il prodotto diretto esterno, invece, prende due spazi vettoriali e ne crea uno nuovo che viene poi ad essere
canonicamente somma diretta interna dei due ‚Äúsottospazi‚Äù. Siano ùëâ, ùëä spazi vettoriali arbitrari ne prendo il
prodotto cartesiano ùëâ √ó ùëä = {(ùë£, ùë§)|ùë£ ‚àà ùëâ, ùë§ ‚àà ùëä} che sar√† il nostro insieme sostegno, devo andarci a
definire una somma interna ed un prodotto esterno e li definisco nel seguente modo:
‚Ä¢ +‚à∂ (ùëâ √ó ùëä) √ó (ùëâ √ó ùëä) ‚Üí ùëâ √ó ùëä
((ùë£, ùë§), (ùë£‚Ä≤, ùë§‚Ä≤)) ‚Ü¶ (ùë£ + ùë£‚Ä≤, ùë§ + ùë§‚Ä≤)
‚Ä¢ ‚ãÖ ‚à∂ ‚Ñù √ó (ùëâ √ó ùëä) ‚Üí ùëâ √ó ùëä
(ùúÜ, (ùë£, ùë§)) ‚Ü¶ (ùúÜùë£, ùúÜùë§)
√à banale vedere che la terna (ùëâ √ó ùëä, +, ‚ãÖ ) √® spazio vettoriale (ovvero ne soddisfa gli assiomi). Esplicitiamo
a questo punte alcune cose:
‚Ä¢ Elemento neutro sar√† la coppia (0, 0) con primo elemento appartenente a ùëâ e secondo a ùëä
‚Ä¢ L‚Äôopposto di (ùë£, ùë§) sar√† la coppia (‚àíùë£,‚àíùë§)
Questo metodo di costruire spazi vettoriali crea uno spazio vettoriale pi√π grande perch√© all‚Äôinterno ùëâ √ó ùëä
contiene dei sottospazi vettoriali che sono da un lato isomorfi a ùëâ e dall‚Äôaltro isomorfi a ùëä. Sono inoltre
sottospazi canonici: ùêª = {(0, ùë§)|ùë§ ‚àà ùëä} e ùêæ = {(ùë£, 0)|ùë£ ‚àà ùëâ}, questi due sottospazi (√® semplice
dimostrare che siano sottospazi) hanno come intersezione la coppia (0, 0), quindi ùêª ‚à© ùêæ = {(0, 0)}, quello
che andremo a vedere √® che ogni elemento del prodotto diretto ùëâ √ó ùëä pu√≤ essere scritto come somma di
un elemento di ùêª e un elemento di ùêæ, praticamente ùëâ √ó ùëä ‚àã ùë¢ = (ùë£, ùë§) = (ùë£, 0) + (0, ùë§) e questo vuol
dire che ùëâ √ó ùëä √® in realt√† somma diretta dei sottospazi ùêª e ùêæ. √à facile convincersi che ùêæ √® isomorfo a ùëâ e
che ùêª √® isomorfo a ùëä, infatti ùêæ ‚âÉ ùëâ perch√© ùë£ ‚àà ùëâ ‚Üí (ùë£, 0) e analogamente ùêª ‚âÉ ùëä perch√© ùë§ ‚àà ùëä ‚Üí (0, ùë§)
quindi possiamo dire che il prodotto diretto ha dimensione la somma delle dimensioni di ùêª e di ùêæ, poich√©
ùëâ √ó ùëä = ùêª‚®Åùêæ, ma essendo dimùêæ = dimùëâ e dimùêª = dimùëä la dimensione del prodotto interno √® la
somma delle due dimensioni: dim(ùëâ √ó ùëä) = dimùëâ + dimùëä.
7. Geometria
Spazio vettoriale dei vettori geometrici liberi dello spazio (e del piano)
Gli spazi vettoriali dei vettori geometrici liberi nello spazio (o piano) sono classi di equipollenza in cui figurano
tutti i vettori equipollenti ad un dato vettore geometrico. Prendiamo due vettori geometrici liberi non nulli
(ovvero diversi dal vettore triviale): ùëé = ùê¥ùêµ e ùëè = ùê∂ùê∑ dove ùê¥ùêµ e ùê∂ùê∑ sono segmenti generici (posso prendere
tutti i segmenti a loro equipollenti). Andiamo adesso a dare le seguenti propriet√†:
‚Ä¢ ùëé ‚à• ùëè ‚áî ùê¥ùêµ e ùê∂ùê∑ (i segmenti corrispondenti che li rappresentano) sono paralleli
Verifichiamo che questa definizione sia ben posta (il che vuol dire che cambiando i rappresentanti i due
oggetti sono ancora paralleli tra loro): se prendo un vettore ùê¥‚Ä≤ùêµ‚Ä≤ equipollente ad ùê¥ùêµ e un vettore ùê∂‚Ä≤ùê∑‚Ä≤
equipollente a ùê∂ùê∑, a causa della relazione di equipollenza trovo subito, sotto il punto di vista pratico, che
anche ùê¥‚Ä≤ùêµ‚Ä≤ ‚à• ùê∂‚Ä≤ùê∑‚Ä≤.
In termini di spazi vettoriali fissiamo un punto ùëÇ, allora possiamo prenderci dei rappresentati come segue:
ùëÇùê¥‚Ä≤ ‚à• ùê¥ùêµ e ùëÇùêµ‚Ä≤ ‚à• ùê∂ùê∑, ora, poich√© la definizione precedente √® ben posta si ha che ùëé ‚à• ùëè ‚áî ùëÇùê¥‚Ä≤ e ùëÇùêµ‚Ä≤ sono
paralleli, ci√≤ significa che i due segmenti oltre ad essere paralleli ed hanno un punto in comune giacciono
sulla stessa retta. Di conseguenza si pu√≤ estendere la definizione
precedente in:
41

‚Ä¢ ùëé ‚à• ùëè ‚áî ùëÇùê¥‚Ä≤ e ùëÇùêµ‚Ä≤ sono paralleli ‚áî ùëÇùê¥‚Ä≤ e ùëÇùêµ‚Ä≤ giacciono sulla stessa retta ‚áî ùëÇùê¥‚Ä≤ e ùëÇùêµ‚Ä≤ sono
proporzionali ‚áî ùëé e ùëè sono proporzionali ‚áî ùëé e ùëè sono dipendenti (linearmente).
Il vettore nullo √® parallelo ad ogni vettore (per definizione), e questo implica, essendo un sistema contenente
il vettore nullo quasi per definizione dipendente, che:
‚Ä¢ ùëé ‚à• ùëè ‚áî ùëé e ùëè costituiscono un sistema linearmente dipendente
Siano ùëé, ùëè, ùëê ‚àà ùí± (dove ùí± √® spazio, ma vale anche per il piano) e fissiamo un punto ùëÇ, ùëé = ùëÇùê¥, ùëè = ùëÇùêµ, ùëê =
ùëÇùê∂, allora:
‚Ä¢ ùëé, ùëè, ùëê dipendenti ‚áî ùëé = ‚Ñéùëè + ùëòùëê con ‚Ñé, ùëò ‚àà ‚Ñù ‚áî ùëÇùê¥ = ‚ÑéùëÇùêµ + ùëòùëÇùê∂ ‚áî ùëÇùê¥, ùëÇùêµ, ùëÇùê∂ sono contenuti in
uno stesso piano
Dalla penultima condizione possiamo dire due cose: o che i punti ùëÇ, ùêµ, ùê∂ sono allineati e quindi giacciono
sulla stessa retta, e per questa retta passante per i tre punti esiste una retta per cui passa ùê¥, altrimenti,
se ùëÇ, ùêµ, ùê∂ non sono allineati significa che esiste un unico piano passante per ùëÇ, ùêµ, ùê∂ ed il punto ùê¥ sta nel
piano identificato da ùëÇ, ùêµ, ùê∂. Il piano che contiene ùëÇ, ùêµ, ùê∂, ùê¥ vale anche per il primo caso poich√© per una
retta passano infiniti piani.
Definizione: Siano ùëé, ùëè ‚àà ùí± vettori geometrici liberi non nulli e fissiamo un punto ùëÇ allora ùëé = ùëÇùëÉ e ùëè = ùëÇùëÑ
allora l‚Äôangolo ùëé^ùëè √® la misura in radianti dell‚Äôangolo convesso (‚â§ 180¬∞) formato da ùëÇùëÉ e ùëÇùëÑ (si verifica
intuitivamente che la definizione √® ben posta).
Definizione (prodotto scalare standard tra due vettori geometrici liberi) ùí± √ó ùí± ‚Üí ‚Ñù:
‚Ä¢ ùë£ ‚ãÖ ùë§ = 0 se ùë£ = 0 o ùë§ = 0
‚Ä¢ ùë£ ‚ãÖ ùë§ = |ùë£| ‚ãÖ |ùë§| ‚ãÖ cos(ùë£^ùë§) = |ùë§| ‚ãÖ |ùëÇùêª|
Questo prodotto gode delle seguenti propriet√†:
1. Simmetria (commutativit√† per prodotti esterni) ùë£ ‚ãÖ ùë§ = ùë§ ‚ãÖ ùë£ dipende dal fatto che ùë£^ùë§ = ùë§^ùë£
(poich√© per definizione bisogna fare la misura in radianti dell‚Äôangolo convesso)
2. Bilinearit√† (‚Ñéùë¢ + ùëòùë£)ùë§ = ‚Ñé(ùë¢ ‚ãÖ ùë§) + ùëò(ùë£ ‚ãÖ ùë§) (dimostrazione geometrica, servono altre nozioni)
3. Definito positivo ‚àÄùë£ ‚àà ùí± ùë£ ‚ãÖ ùë£ ‚â• 0 essendo ùë£ ‚ãÖ ùë£ = |ùë£| ‚ãÖ |ùë£| ‚ãÖ cos(ùë£^ùë£) = |ùë£|
2
ùë£ ‚ãÖ ùë£ = 0 ‚ü∫ ùë£ = 0 essendo ùë£ ‚ãÖ ùë£ = | ùë£| = 0 ‚áî ùë£ = 0
2
Questo prodotto scalare standard ci permette di dire che ùë£ ‚ãÖ ùë£ = |ùë£| ‚áí |ùë£| = ‚àöùë£ ‚ãÖ ùë£
2
Definizione: chiamer√≤ versore un vettore di lunghezza unitaria, ovvero un vettore il cui modulo √® pari ad uno.
Mentre con versore di una retta orientata indicher√≤ quel versore parallelo e concorde alla retta.
Vettori ortogonali: siano ùëé = ùê¥ùêµ e ùëè = ùê∂ùê∑ due vettori non nulli poniamo per definizione:
‚Ä¢
ùëé ‚ä• ùëè ‚áî ùê∂ùê∑ ‚ä• ùê¥ùêµ √® questa √® una definizione ben posta
‚Ä¢ Fissato un punto ùëÇ avremo ùëé ‚ä• ùëè ‚áî ùëÇùê¥ ‚ä• ùëÇùêµ ‚áî ùúÉ = ùëé^ùëè =
ùúã
2 ‚áî cos ùúÉ = 0 ‚áî ùëé ‚ãÖ ùëè = 0
Sempre per definizione si pone che il vettore nullo √® ortogonale ad ogni vettore; quindi, non √® necessario
prendere ùëé e ùëè non nulli per dire che ùëé ‚ãÖ ùëè = 0 ‚áî ùëé ‚ä• ùëè ‚àÄùëé, ùëè ‚àà ùí±
Cominciamo ora con il ricordare che lo spazio vettoriale dei vettori geometrici liberi del piano ùí±ùúã ha
dimensione 2 mentre lo spazio vettoriale dei vettori geometrici liberi dello spazio ha dimùí± = 3. E
generalizziamo questa osservazione dando la seguente definizione: Si dice riferimento ortonormale del
piano un riferimento in cui i vettori sono ortogonali quindi ‚Ñõùúã = (ùë£, ùë§) con ùë£ ‚ãÖ ùë§ = 0. Allo stesso modo il
riferimento ortonormale dello spazio √® ‚Ñõ = (ùë£, ùë§, ùë¢) con ùë£ ‚ãÖ ùë§ = 0, ùë§ ‚ãÖ ùë¢ = 0, ùë¢ ‚ãÖ ùë£ = 0. Sia per quanto
42

riguarda il piano che lo spazio, l‚Äôaltra condizione per essere un riferimento ortogonale √® che i vettori devono
essere versori. Per semplicit√† di notazione consideriamo un riferimento ortonormale del piano.
Proposizione: Se ùëé, ùëè sono vettori geometrici del piano (ùëé, ùëè ‚àà ùí±ùúã) e ùëé = ùëé1ùë£ + ùëé2ùë§, ùëè = ùëè1ùë£ + ùëè2ùë§ (ovvero
scritti come combinazione lineare del riferimento ‚Ñõùúã) allora ùëé ‚ãÖ ùëè = ùëé1ùëè1 + ùëé2ùëè2 = (ùëé1, ùëé2) ‚ãÖ (ùëè1, ùëè2).
Dimostrazione: applichiamo al prodotto scalare standard la sua propriet√† di essere bilineare e simmetrico:
ùëé ‚ãÖ ùëè = (ùëé1ùë£ + ùëé2ùë§) ‚ãÖ (ùëè1ùë£ + ùëè2ùë§) = ùëé1ùëè1(ùë£ ‚ãÖ ùë£) + ùëé1ùëè2(ùë£ ‚ãÖ ùë§) + ùëé2ùëè1(ùë§ ‚ãÖ ùë£) + ùëé2ùëè2(ùë§ ‚ãÖ ùë§) essendo ùë£
e ùë§ versori ortogonali si ha che (ùë£ ‚ãÖ ùë£) = 1, (ùë§ ‚ãÖ ùë§) = 1, (ùë£ ‚ãÖ ùë§) = 0 e (ùë§ ‚ãÖ ùë£) = 0, da cui la tesi.
Corollario: Se esprimo ùë£ come ùê∂‚Ñõ(ùë£) = (ùëé, ùëè) avr√≤ |ùë£| = ‚àöùë£ ‚ãÖ ùë£ = ‚àöùëé2 + ùëè2, considerazioni del tutto
analoghe (anche per la proposizione precedente) si attuano nello spazio: |ùë£| = ‚àöùëé2 + ùëè2 + ùëê2 esprimendo
ùë£ come ùê∂‚Ñõ(ùë£) = (ùëé, ùëè, ùëê).
Definizione: definisco riferimento cartesiano ortogonale monometrico una coppia che ha come primo
oggetto un punto detto origine e secondo oggetto una coppia (se si parla di spazio sar√† una terna), in
particolare un riferimento ortonormale del piano: ‚Ñõ = (ùëÇ, (ùëíùë•, ùëíùë¶)).
Essendo ùëíùë• un versore e quindi un vettore geometrico libero ne posso prendere un segmento che rappresenti
il vettore, ad esempio ùëÇùëà = ùëíùë•, allo stesso modo ùëÇùëà‚Ä≤ = ùëíùë¶, i punti ùëà, ùëà‚Ä≤ sono detti punti unitari. Mentre le
rette passanti per ùëÇùëà e ùëÇùëà‚Ä≤ sono detti assi coordinati (praticamente l‚Äôasse ùë• e l‚Äôasse ùë¶). Di seguito diamo la
rappresentazione delle rette orientate concordemente ai versori e calcolo di un punto ùëÉ tramite proiezioni:
Con ovviamente i punti particolari con coordinate:
ùëÇ(0,0)
ùëà(1,0)
ùëà‚Ä≤(0,1)
Proposizione: dati due punti ùê¥(ùë•1, ùë¶1) e ùêµ(ùë•2, ùë¶2), le componenti del vettore ùê¥ùêµ (la classe di equipollenza
dei vettori equipollenti del vettore geometrico di punto iniziale ùê¥ e finale ùêµ) sono ùë•2 ‚àí ùë•1 e ùë¶2 ‚àí ùë¶1.
Dimostrazione: iniziamo con l‚Äôosservare che per la regola del parallelogrammo ùëÇùê¥ + ùê¥ùêµ = ùëÇùêµ (vedi sotto),
ed essendo questi vettori geometrici liberi posso estrapolare ùê¥ùêµ = ùëÇùêµ ‚àí ùëÇùê¥
e da questa relazione applichiamo l‚Äôisomorfismo coordinato che ci dice che le
componenti di ùê¥ùêµ saranno uguali alle componenti di ùëÇùêµ meno le componenti
di ùëÇùê¥, dunque chiamiamo le componenti incognite di ùê¥ùêµ come (ùë•, ùë¶), mentre le
componenti di ùëÇùê¥ ed ùëÇùêµ sono per definizione (ùë•1, ùë¶1) e (ùë•2, ùë¶2) rispettivamente. Allora ci√≤ vuol dire che la
coppia (ùë•, ùë¶) = (ùë•1, ùë¶1) ‚àí (ùë•2, ùë¶2) = (ùë•2 ‚àí ùë•1, ùë¶2 ‚àí ùë¶1), come volevasi dimostrare.
Spesso denoteremo ùê¥ùêµ(ùë•2 ‚àí ùë•1, ùë¶2 ‚àí ùë¶1) direttamente con le componenti come facciamo con i punti, ma
questa definizione (per la precedente proposizione) non √® ambigua infatti ùëÇùëÉ(ùëé, ùëè) ‚â° ùëÉ(ùëé, ùëè).
Prendiamo due vettori geometrici liberi ùë£(ùë£ùë•, ùë£ùë¶) e ùë§(ùë§ùë•, ùë§ùë¶) diversi dal vettore nullo, allora:
ùë£ ‚ãÖ ùë§
cos ùë£^ùë§ =
=
|ùë£| ‚ãÖ |ùë§|
‚àöùë£ùë•
2 + ùë£ùë¶
2 ‚ãÖ ‚àöùë§ùë•
2 + ùë§ùë¶
2
ùë£ùë•ùë§ùë• + ùë£ùë¶ùë§ùë¶
cos ùë£^ùëíùë• =
ùë£ùë•
‚àöùë£ùë•
2 + ùë£ùë¶
2
cos ùë£^ùëíùë¶ =
ùë£ùë¶
‚àöùë£ùë•
2 + ùë£ùë¶
2
43

Cambiamenti di riferimento
Se dati due riferimenti cartesiani ortogonali monometrici ‚Ñõ = (ùëÇ, (ùëíùë•, ùëíùë¶)), ‚Ñõ‚Ä≤ = (ùëÇ‚Ä≤, (ùëíùë•
‚Ä≤ , ùëíùë¶
‚Ä≤ )) voglio
passare dalle coordinate di un punto ùëÉ in ‚Ñõ alle coordinate in ‚Ñõ‚Ä≤ si usa il seguente metodo.
Diciamo che il punto ùëÉ abbia coordinate (ùë•, ùë¶) in ‚Ñõ e coordinate (ùë•‚Ä≤, ùë¶‚Ä≤) in ‚Ñõ‚Ä≤ e noi vogliamo trovare delle
formule che ci permettono di passare da (ùë•, ùë¶) a (ùë•‚Ä≤, ùë¶‚Ä≤). Per poter fare questa cosa ci viene in aiuto la
matrice di passaggio da un riferimento ad un altro, in particolare dal riferimento (ùëíùë•, ùëíùë¶) al riferimento
(ùëíùë•
‚Ä≤ , ùëíùë¶
‚Ä≤ ), ovvero la matrice ùêµ = (
ùëè11 ùëè12
ùëè21 ùëè22
) e quindi (propriet√† della matrice di passaggio) se moltiplichiamo
a destra la colonna delle componenti di un vettore nel vecchio riferimento otteniamo la colonna delle
componenti nel nuovo riferimento.
Riprendiamo le coordinate nel punto ùëÉ che, come ben sappiamo, per definizione sono le componenti del
vettore geometrico libero ùëÇùëÉ e ùëÇ‚Ä≤ùëÉ, rispettivamente per (ùë•, ùë¶) e (ùë•‚Ä≤, ùë¶‚Ä≤), e andiamo a definire anche le
coordinate delle origini dei riferimenti. Per il punto ùëÇ √® (0,0) mentre per ùëÇ‚Ä≤, essendo traslato rispetto
all‚Äôorigine, chiamiamole (ùëê1, ùëê2), una volta conosciute le coordinate di ùëÇ e ùëÉ, per un lemma visto in
precedenza, si conoscono anche le coordinate di ùëÇùëÉ ovvero (ùë•, ùë¶), mentre ùëÇ‚Ä≤ùëÉ(ùë•‚Ä≤ ‚àí ùëê1, ùë¶‚Ä≤ ‚àí ùëê2).
A questo punto utilizziamo la matrice di passaggio moltiplicandola a sinistra della colonna delle componenti
del vettore ùëÇùëÉ nel riferimento che sottende ‚Ñõ, risulteranno le componenti dello stesso vettore nel
riferimento che sottende ùëÖ‚Ä≤ quindi ùêµ (
ùë•
ùë¶) = (
ùë•‚Ä≤ ‚àí ùëê1
ùë¶‚Ä≤ ‚àí ùëê2
) ‚áí {
ùë•‚Ä≤ ‚àí ùëê1 = ùëè11ùë• + ùëè12ùë¶
ùë¶‚Ä≤ ‚àí ùëê2 = ùëè21ùë• + ùëè22ùë¶
, a questo sistema
esplicitando le coordinate dei punti ùë•‚Ä≤ e ùë¶‚Ä≤ in termini di coordinate di ùëÉ nel riferimento ‚Ñõ otterremo il
seguente sistema: {
ùë•‚Ä≤ = ùëè11ùë• + ùëè12ùë¶ + ùëê1
ùë¶‚Ä≤ = ùëè21ùë• + ùëè22ùë¶ + ùëê2
che sono le formule di passaggio da ùì° a ùì°‚Ä≤.
Prodotto vettoriale
Definiamo il prodotte vettoriale nello spazio (non √® possibile definirlo con il piano): prendo un riferimento
cartesiano ortogonale monometrico ‚Ñõ = (ùëÇ, ‚Ñõùí± = (ùëíùë•, ùëíùë¶, ùëíùëß)) e due vettori: ùë£(ùë£ùë•, ùë£ùë¶, ùë£ùëß) = ùë£ùë•ùëíùë• +
ùë£ùë¶ùëíùë¶ + ùë£ùëßùëíùëß e ùë§(ùë§ùë•, ùë§ùë¶, ùë§ùëß) = ùë§ùë•ùëíùë• + ùë§ùë¶ùëíùë¶ + ùë§ùëßùëíùëß. Allora il prodotto vettoriale ùë£ √ó ùë§ √® per definizione
ùëíùë•
ùë£ √ó ùë§ = |ùë£ùë•
ùëíùë¶
ùë£ùë¶
ùëíùëß
ùë§ùë• ùë§ùë¶ ùë§ùëß
ùë£ùëß | = (ùë£ùë¶ùë§ùëß ‚àí ùë§ùë¶ùë£ùëß)ùëíùë• ‚àí (ùë£ùë•ùë§ùëß ‚àí ùë£ùëßùë§ùë•)ùëíùë¶ + (ùë£ùë•ùë§ùë¶ ‚àí ùë§ùë•ùë£ùë¶)ùëíùëß
sviluppato per la prima riga).
Esempio: ùë£(1,0,1), ùë§(2,2,0)
ùë£ √ó ùë§ = |
ùëíùë•
ùëíùë¶
ùëíùëß
1 0 1
2 2 0
| = (|0 1
2 0
| , ‚àí |1 1
2 0
| , |1 0
2 2
(il determinante
|) = (‚àí2,2,2)
Propriet√†: Siano ùë£(ùë£ùë•, ùë£ùë¶, ùë£ùëß) e ùë§(ùë§ùë•, ùë§ùë¶, ùë§ùëß) ‚àà ùí± e ‚Ñõ = (ùëÇ, ‚Ñõùí± = (ùëíùë•, ùëíùë¶, ùëíùëß))
1. (ùë£ √ó ùë§) ‚ãÖ ùë£ = 0 (il prodotto vettoriale √® sempre ortogonale al prodotto dei due vettori fra di loro)
Dim.: (ùë£ √ó ùë§) ‚ãÖ ùë£ = ((ùë£ùë¶ùë§ùëß ‚àí ùë§ùë¶ùë£ùëß)ùëíùë• ‚àí (ùë£ùë•ùë§ùëß ‚àí ùë£ùëßùë§ùë•)ùëíùë¶ + (ùë£ùë•ùë§ùë¶ ‚àí ùë§ùë•ùë£ùë¶)ùëíùëß) ‚ãÖ ùë£ = ùë£ùë¶ùë§ùëßùë£ùë•
Ãá ‚àí
Ãà ‚àí ùë£ùë•ùë§ùëßùë£ùë¶
ùë§ùë¶ùë£ùëßùë£ùë•
Ãá + ùë£ùëßùë§ùë•ùë£ùë¶
‚Éõ + ùë£ùë•ùë§ùë¶ùë£ùëß
Ãà ‚àí ùë§ùë•ùë£ùë¶ùë£ùëß
‚Éõ = 0
2. (ùë£ √ó ùë§) ‚ãÖ ùë§ = 0
Dunque: (ùë£ √ó ùë§) ‚ä• ùë£ e (ùë£ √ó ùë§) ‚ä• ùë§
3. ùë£ √ó ùë§ = 0 ‚áî ùë£ e ùë§ sono dipendenti ‚áî ùë£ ‚à• ùë§
Dim.: le componenti del prodotto vettoriale sono i minori di ordine 2 della matrice (
ùëíùë•
ùë£ùë•
ùëíùë¶
ùë£ùë¶
ùëíùëß
ùë§ùë• ùë§ùë¶ ùë§ùëß
ùë£ùëß )
e quindi per dimostrare la nostra ipotesi bisogna andare a dimostrare che tutti i minori di ordine 2
44

della matrice rettangolare [ùë§ùë• ùë§ùë¶ ùë§ùëß
ùë£ùë•
ùë£ùë¶
ùë£ùëß
sottomatrice [ùë§ùë• ùë§ùë¶ ùë§ùëß
ùë£ùë•
ùë£ùë¶
ùë£ùëß
] √® 1; e di conseguenza |ùë£ùë•
] siano uguali a 0 e lo sono, quindi il rango di questa
ùëíùë•
ùëíùë¶
ùë£ùë¶
ùëíùëß
ùë§ùë• ùë§ùë¶ ùë§ùëß
ùë£ùëß | = 0 e dunque per il teorema
degli orlati (essendo la sottomatrice di rango 1) le righe (ùë£ùë•, ùë£ùë¶, ùë£ùëß) e (ùë§ùë•, ùë§ùë¶, ùë§ùëß) sono
proporzionali (dipendenti), ovvero (per la coordinazione associata) ùë£ e ùë§ sono dipendenti.
4. ùë£ √ó ùë§ = ‚àí(ùë§ √ó ùë£) (non √® simmetrico)
Dim.: Pensando al determinante |
ùëíùë•
ùë£ùë•
ùëíùë¶
ùë£ùë¶
ùëíùëß
ùë§ùë• ùë§ùë¶ ùë§ùëß
ùë£ùëß | se si scambiano la seconda e la terza riga per la
propriet√† del determinante le due matrici avranno il segno scambiato.
5. ùëíùë• √ó ùëíùë¶ = ùëíùëß
ùëíùë¶ √ó ùëíùëß = ùëíùë•
ùëíùëß √ó ùëíùë• = ùëíùë¶
Dim.: Basta farsi i conti: (1,0,0) √ó (0,1,0) = (0,0,1) (nota che (0,1,0) √ó (1,0,0) = (0,0, ‚àí1)
Rappresentazioni
Definizione: un insieme di punti del piano ùëã √® rappresentabile se e soltanto se esiste un sistema di equazioni
ùëÜùëá in due incognite (ed eventuali parametri ùëá) tale che ùëÉ ‚àà ùëã ‚áî ‚àÉùëá (valore per quei parametri) tale che le
coordinate di ùëÉ nel riferimento fissato appartengono a ùëÜùëá (l‚Äôinsieme di soluzioni di ùëÜùëá).
Esempio ipotetico di parametri: se avessimo 3ùë• + 4ùë¶ + ùëò = 0 come nostro sistema e (2,3) ne √® soluzione
allora deve esistere un ùëò per cui (2,3) √® soluzione di 3ùë• + 4ùë¶ + ùëò = 0. Concetto analogo ovviamente nel
caso dello spazio, che avr√≤ semplicemente tre componenti e quindi 3 incognite.
Rappresentazione di un piano nello spazio
Rappresentazione parametrica: Sia ùúã un piano dello spazio. Allora ùúã √® rappresentato da un sistema
parametrico a coefficienti reali del tipo {
ùë• = ùë•ùëú + ùëôùë† + ùëô‚Ä≤ùë°
ùë¶ = ùë¶0 + ùëöùë† + ùëö‚Ä≤ùë°
ùë° = ùë°0 + ùëõùë† + ùëõ‚Ä≤ùë°
dove (ùëô, ùëö, ùëõ) ed (ùëô‚Ä≤, ùëö‚Ä≤, ùëõ‚Ä≤) sono terne di
numeri reali indipendenti ed ùë†, ùë° parametri reali. ùë•, ùë¶, ùëß sono soluzioni di questo sistema se esistono dei valori
per ùë†, ùë° tale che ùë•, ùë¶, ùëß soddisfino il sistema
Dimostrazione: Vogliamo trovare le costanti (ùëô, ùëö, ùëõ) e (ùëô‚Ä≤, ùëö‚Ä≤, ùëõ‚Ä≤) tali che un punto ùëÉ appartenga al piano se
e solamente se le sue coordinate soddisfino quel sistema di equazioni. Fissiamo un punto del piano
ùê¥(ùë•0, ùë¶0, ùëß0) ‚àà ùúã, dopodich√©, sapendo che il piano √® determinato da un punto e da due vettori che giacciono
sul piano e ci danno la direzione del piano (oppure semplicemente da tre punti), siano ùë£(ùëô, ùëö, ùëõ) e
ùë£‚Ä≤(ùëô‚Ä≤, ùëö‚Ä≤, ùëõ‚Ä≤) due vettori liberi indipendenti e paralleli a ùúã allora: ùëÉ ‚àà ùúã ‚áî ùê¥ùëÉ, ùë£, ùë£‚Ä≤ applicati in ùê¥ sono
complanari ‚áî ùê¥ùëÉ, ùë£, ùë£‚Ä≤ dipendenti ‚áî ùê¥ùëÉ ‚àà ‚å©ùë£, ùë£‚Ä≤‚å™. Abbiamo cos√¨ visto che questo sistema √® formato da tre
vettori dipendenti e quello che dipende dai rimanenti √® proprio ùê¥ùëÉ essendo ùë£, ùë£‚Ä≤ indipendenti. In termini di
componenti questo significa che (applicando la coordinazione associata) ùê∂‚Ñõ(ùëÉ) = ùê∂‚Ñõ(ùë• ‚àí ùë•0, ùë¶ ‚àí ùë¶0, ùëß ‚àí
ùëß0) ‚àà ‚å©(ùëô, ùëö, ùëõ), (ùëô‚Ä≤, ùëö‚Ä≤, ùëõ‚Ä≤)‚å™ = ‚å©ùë£, ùë£‚Ä≤‚å™ che √® equivalente a scrivere: ‚àÉùë°, ùë† ‚àà ‚Ñù ‚à∂ {
ùë• ‚àí ùë•0 = ùëôùë† + ùëô‚Ä≤ùë°
ùë¶ ‚àí ùë¶0 = ùëöùë† + ùëö‚Ä≤ùë°
ùë° ‚àí ùë°0 = ùëõùë† + ùëõ‚Ä≤ùë°
RF NN,
‚Äò
~
yw
‚ÄúA(9,46)
a?
Ogni sistema del tipo {
ùë• = ùë•0 + ùëôùë† + ùëô‚Ä≤ùë°
ùë¶ = ùë¶0 + ùëöùë† + ùëö‚Ä≤ùë°
ùë° = ùë°0 + ùëõùë† + ùëõ‚Ä≤ùë°
rappresenta un piano.
Di quanto mi muovo su una direzione √® determinato dal parametro ùë†
mentre di quanto mi muovo sull‚Äôaltra √® determinato dal parametro ùë°
45
.

Proposizione (rappresentazione in forma ordinaria o cartesiana): Il piano ùúã √® rappresentato da un‚Äôequazione
del tipo ùëéùë• + ùëèùë¶ + ùëêùëß + ùëë = 0 con (ùëé, ùëè, ùëê) ‚â† (0,0,0), inoltre il vettore ùë§(ùëé, ùëè, ùëê) ‚ä• ùúã.
Dimostrazione: Segue lo stesso percorso della dimostrazione precedente solamente che arrivati al punto
della coordinazione associata non sviluppiamo in termini di combinazione lineare ma utilizziamo il fatto che
questi tre vettori sono dipendenti: ùëÉ ‚àà ùúã ‚áî ùë£, ùë£‚Ä≤ e ùê¥ùëÉ dipendenti ‚áî |
ùë• ‚àí ùë•0 ùë¶ ‚àí ùë¶0 ùëß ‚àí ùëß0
ùëô
ùëö
ùëô‚Ä≤
ùëö‚Ä≤
ùëõ
ùëõ‚Ä≤
| = 0,
sviluppandolo per la prima riga: (ùëöùëõ‚Ä≤ ‚àí ùëõ‚Ä≤ùëö) (ùë• ‚àí ùë•0) + (ùëô‚Ä≤ùëõ ‚àí ùëôùëõ‚Ä≤) (ùë¶ ‚àí ùë¶0) + (ùëôùëö‚Ä≤ ‚àí ùëô‚Ä≤ùëö) (ùëß ‚àí ùëß0) = 0
‚èü        
ùëé
‚èü      
ùëè
‚èü        
ùëê
chiamando poi tutti i termini senza incognita ùëë = ‚àíùëéùë•0 ‚àí ùëèùë¶0 ‚àí ùëêùëß0 risulter√† ùëéùë• + ùëèùë¶ + ùëêùëß + ùëë = 0; ora ci
rimane da osservare che (ùëé, ùëè, ùëê) ‚â† (0,0,0) e che ùë§(ùëé, ùëè, ùëê) ‚ä• ùúã; ma essendo ùëé, ùëè, ùëê i minori della
sottomatrice rettangolare presi a segno alterno essi rappresentano le componenti del prodotto vettoriale tra
ùë£ e ùë£‚Ä≤, e poich√© questi ultimi non sono paralleli (essendo indipendenti) il prodotto non pu√≤ essere nullo (per
una propriet√† gi√† vista) deve succedere che (ùëé, ùëè, ùëê) = (ùëô, ùëö, ùëõ) √ó (ùëô‚Ä≤, ùëö‚Ä≤, ùëõ‚Ä≤) ‚â† 0, dopodich√© il prodotto
vettoriale √® perpendicolare a entrambi i vettori del prodotto, ma allora se √® perpendicolare ai due vettori del
piano ùë£ e ùë£‚Ä≤, evidentemente √® perpendicolare anche al piano stesso.
Proposizione (il viceversa della precedente): Ogni equazione ùëéùë• + ùëèùë¶ + ùëêùëß + ùëë = 0 con (ùëé, ùëè, ùëê) ‚â† (0,0,0)
rappresenta un piano ùúã ortogonale a (ùëé, ùëè, ùëê).
Dimostrazione: vado a prendere due vettori ortogonali ad (ùëé, ùëè, ùëê) che siano indipendenti, per trovarli devo
risolvere il sistema ùë£ ‚ãÖ (ùëé, ùëè, ùëê) = 0, ovvero devo risolvere il sistema ùëéùë£ùë• + ùëèùë£ùë¶ + ùëêùë£ùëß = 0; essendo per
ipotesi (ùëé, ùëè, ùëê) ‚â† (0,0,0) abbiamo la certezza che uno tra ùëé, ùëè, ùëê deve essere diverso da zero, diciamo che
sia ùëé ‚â† 0 allora ùë£ùë• = ‚àí
1
ùëé (ùëèùë£ùë¶ + ùëêùë£ùëß) e quindi ùëÜ = {(‚àí
1
ùëé (ùëèùë£ùë¶ + ùëêùë£ùëß), ùë£ùë¶, ùë£ùëß) |ùë£ùë¶, ùë£ùëß ‚àà ‚Ñù} con dimùëÜ = 2 e
quindi sicuramente posso trovare due vettori ortogonali ad (ùëé, ùëè, ùëê) che siano indipendenti. Prendo un piano
ùúã‚Ä≤ passante per questi due vettori e perpendicolare ad (ùëé, ùëè, ùëê), mi fisso una soluzione dell‚Äôequazione ùëéùë• +
ùëèùë¶ + ùëêùëß + ùëë = 0 e il punto corrispondente a questa soluzione lo chiamo ùê¥(ùë•0, ùë¶0, ùëß0) ‚àà ùúã allora succede che
il piano ùúã‚Ä≤ per la rappresentazione ordinaria √® rappresentato dall‚Äôequazione ùëé‚Ä≤ùë• + ùëè‚Ä≤ùë• + ùëê‚Ä≤ùëß + ùëë‚Ä≤ = 0 e
(ùëé‚Ä≤, ùëè‚Ä≤, ùëê‚Ä≤) ed √® perpendicolare a ùúã‚Ä≤, ma essendo ùúã‚Ä≤ perpendicolare a sua volta alla terna (ùëé, ùëè, ùëê) (essendo
parallelo ai due vettori fissati in precedenza), ed essendo le due terne entrambe perpendicolari allo stesso
piano risulta (ùëé, ùëè, ùëê) ‚à• (ùëé‚Ä≤, ùëè‚Ä≤, ùëê‚Ä≤) e questo vuol dire che posso scrivere ùëéùë• + ùëèùë¶ + ùëêùëß + ùëë = 0 come ùëòùëéùë• +
ùëòùëèùë¶ + ùëòùëêùëß + ùëë‚Ä≤ = 0 con ùëò ‚â† 0; il piano ùúã‚Ä≤ risulta quindi avere l‚Äôequazione cos√¨ descritta, inoltre essendo
ùëò ‚â† 0 posso scrivere ùëéùë• + ùëèùë¶ + ùëêùëß +
ùëë‚Ä≤
ùëò = 0; a questo punto per la scelta fatta di ùê¥ le sue coordinate
soddisfano l‚Äôequazione ùëéùë• + ùëèùë¶ + ùëêùëß + ùëë = 0 oltre all‚Äôequazione ùëéùë• + ùëèùë¶ + ùëêùëß +
ùëë‚Ä≤
ùëò = 0, ma poich√©
entrambi passano per ùê¥(ùë•0, ùë¶0, ùëß0) posso eguagliare le equazioni e quindi ùëë =
ùëë‚Ä≤
ùëò ‚áí ùëë‚Ä≤ = ùëëùëò e questo vuol
dire che l‚Äôequazione ùúã‚Ä≤: ùëé‚Ä≤ùë• + ùëè‚Ä≤ùë• + ùëê‚Ä≤ùëß + ùëë‚Ä≤ = 0 √® equivalente (dividendo per ùëò) a ùëéùë• + ùëèùë¶ + ùëêùëß + ùëë = 0
e quindi ùúã‚Ä≤ √® il piano ùúã stesso
Osservazione: Abbiamo dimostrato che due equazioni del tipo ùëéùë• + ùëèùë¶ + ùëêùëß + ùëë = 0 che rappresentano lo
stesso identico piano devono essere proporzionali (ùëò ‚â† 0) e viceversa.
Esempi:
1) Come si rappresenta tutto lo spazio?
Semplicemente con una tautologia (un‚Äôequazione sempre vera) come 0 = 0
2) Come si rappresenta l‚Äôinsieme vuoto?
Con una contraddizione: 0 ‚â† 0 (l‚Äôequazione parametrica non √® mai soddisfatta)
3) Sia ùúã per ùê¥(4,3, ‚àí2) e parallelo ai vettori ùë£(1,‚àí1,0) e ùë£‚Ä≤(2,1,3). Come si scrive l‚Äôequazione parametrica
e la rappresentazione cartesiana?
ùúã ‚à∂ {
ùë• = 4 + ùë† + 2ùë°
ùë¶ = 3 ‚àí ùë† + ùë°
ùëß = ‚àí2 + 3ùë°
|
ùë• ‚àí 4 ùë¶ ‚àí 3 ùëß + 2
1
‚àí1
2
1
0
3
| = ùë• + ùë¶ ‚àí ùëß ‚àí 9 = 0
46

Rappresentazione della retta nel piano
Rappresentazione parametrica: Una retta ùëü pu√≤ essere rappresentata mediante un sistema di equazioni in
un parametro del tipo {
ùë• = ùë•0 + ùëôùë°
ùë¶ = ùë¶0 + ùëöùë° con la coppia (ùëô, ùëö) ‚â† (0,0) e ùë° parametro (si nota subito che si ha una
dimostrazione simile a quella fatta con il piano nello spazio).
Dimostrazione (simile a quella fatta con il piano): Prendiamo un punto ùê¥(ùë•0, ùë¶0) ‚àà ùëü e un vettore direzione
ùë£(ùëô, ùëö) ‚à• ùëü con (ùëô, ùëö) ‚â† (0,0), vediamo che un punto ùëÉ ‚àà ùëü ‚áî ùê¥ùëÉ ‚à• ùë£, e questo mi assicura anche che ùëÉ
appartenga alla retta, altrimenti non potrebbe essere parallelo a ùë£, inoltre ùê¥ùëÉ ‚à• ùë£ ‚áî ùê¥ùëÉ e ùë£ sono dipendenti
‚áî (ùë• ‚àí ùë•0, ùë¶ ‚àí ùë¶0), (ùëô, ùëö) dipendenti (per la coordinazione associata) ‚áî ‚àÉùë° ‚àà ‚Ñù ‚à∂
{
ùë• ‚àí ùë•0 = ùë°ùëô
ùë¶ ‚àí ùë¶0 = ùë°ùëö;
quest‚Äôultima condizione significa anche che il primo vettore √® proporzionale al secondo.
Rappresentazione ordinaria: la retta √® rappresentata da un‚Äôequazione del tipo ùëéùë• + ùëèùë¶ + ùëê = 0 con (ùëé, ùëè) ‚â†
(0,0), inoltre il vettore ùë£(‚àíùëè, ùëé) ‚à• ùëü.
Dimostrazione: Prendiamo un punto ùê¥(ùë•0, ùë¶0) ‚àà ùëü e un vettore direzione ùë£(ùëô, ùëö) ‚à• ùëü con (ùëô, ùëö) ‚â† (0,0)
allora ùëÉ ‚àà ùëü ‚áî ùê¥ùëÉ ‚à• ùë£ ‚áî ùê¥ùëÉ e ùë£ sono dipendenti ‚áî (ùë• ‚àí ùë•0, ùë¶ ‚àí ùë¶0) con (ùëô, ùëö) dipendenti; si mettano
queste due coppie sulla matrice e se ne faccia il determinante: |
ùë• ‚àí ùë•0 ùë¶ ‚àí ùë¶0
ùëô
ùëö
| = 0; poich√© sono
dipendenti il determinante √® zero e questo vuol dire anche che (sviluppando il determinante attraverso la
prima riga) ùëö(ùë• ‚àí ùë•0) ‚àí ùëô(ùë¶ ‚àí ùë¶0) = 0 ‚áî ùëéùë• + ùëèùë¶ + ùëê = 0 con (ùëö, ‚àíùëô) = (ùëé, ùëè) e ùëê = ‚àíùëöùë•0 + ùëôùë¶0; si noti
anche che la coppia (ùëé, ùëè) ‚â† 0 per il semplice motivo che per ipotesi (ùëô, ùëö) ‚â† (0,0) ed inoltre (‚àíùëè, ùëé) ‚à• ùë£ e
quindi parallelo ad ùëü.
Viceversa (della rappresentazione parametrica): Ogni sistema {
ùë• = ùë•0 + ùë†ùëô
ùë¶ = ùë¶0 + ùë†ùëö con (ùëô, ùëö) ‚â† (0,0) rappresenta
la retta che passa per (ùë•0, ùë¶0) ed √® parallela al vettore ùë£(ùëô, ùëö)
Dimostrazione: non √® una vera dimostrazione, √® pi√π una cosa intuitiva. Il fatto che le
coordinate di un punto debbano soddisfare quel sistema parametrico significa che
(essendo ùë† parametro) se faccio ùë† = 0 si ha che ùë• = ùë•0 e ùë¶ = ùë¶0; di conseguenza
l‚Äôinsieme di soluzioni del sistema ammette per certo la soluzione (ùë•0, ùë¶0); in realt√†
per ogni valore di ùë† il sistema ammette sempre e solo una soluzione, essendo
(ùë•0, ùë¶0) fissati a monte. Quindi variare ùë† significa variare il modulo del vettore
ùë£(ùëô, ùëö) e quindi ci si sposta sempre muovendosi sulla stessa retta (vedi figura a destra).
Gu)
Viceversa (della rappresentazione ordinaria): Ogni equazione ùëéùë• + ùëèùë¶ + ùëê = 0 con (ùëé, ùëè) ‚â† (0,0)
rappresenta una retta ùëü che sia parallela al vettore ùë£(‚àíùëè, ùëé).
Dimostrazione: poich√© (ùëé, ùëè) ‚â† (0,0) l‚Äôequazione ammette ùëéùë• + ùëèùë¶ + ùëê = 0 almeno una soluzione (se ùëé ‚â†
0 esplicito ùë•, se √® ùëè ‚â† 0 allora esplicito ùë¶) allora mi prendo una soluzione ùê¥(ùë•0, ùë¶0) e vado a costruirmi la
retta ùëé‚Ä≤ùë• + ùëè‚Ä≤ùë¶ + ùëê‚Ä≤ = 0 passante per il punto ùê¥ e parallela al vettore ùë£(‚àíùëè, ùëé). Ne consegue che la coppia
(‚àíùëè‚Ä≤, ùëé‚Ä≤) deve essere proporzionale a (‚àíùëè, ùëé) e quindi (‚àíùëè‚Ä≤, ùëé‚Ä≤) ‚à• (‚àíùëè, ùëé) ed allora, essendo entrambe le
coppie diverse da (0,0), posso scrivere ùëé‚Ä≤ = ùëòùëé, ùëè‚Ä≤ = ùëòùëè, dopodich√© (esattamente come fatto con il piano),
imponendo la condizione che l‚Äôequazione ùëé‚Ä≤ùë• + ùëè‚Ä≤ùë¶ + ùëê‚Ä≤ = 0 deve passare per il punto (ùë•0, ùë¶0) si ha anche
ùëê‚Ä≤ = ùëòùëê. Quindi abbiamo trovato che ùëé‚Ä≤ùë• + ùëè‚Ä≤ùë¶ + ùëê‚Ä≤ = 0 ‚áî ùëòùëéùë• + ùëòùëèùë¶ + ùëòùëê = 0; semplificando la
costante ùëò (che non pu√≤ essere nulla per via delle tre uguaglianze descritte precedentemente) vuol dire che
√® la stessa equazione di partenza e mi rappresenta proprio la retta ùëü: ùëéùë• + ùëèùë¶ + ùëê = 0.
Osservazione: Due equazioni ordinarie rappresentano la stessa retta se e solo se sono proporzionali (ùëò ‚â† 0).
Definizione: se ho una retta ùëü ed un vettore non nullo ùë£(ùëô, ùëö) ‚à• ùëü allora le componenti di questo vettore ùë£ le
chiamo numeri direttori (o parametri) della retta ùëü. E questi parametri di vettori (per il parallelismo) sono
ben definiti a meno di un fattore di proporzionalit√† non nullo.
47

Altre tipologie di espressione
Espressione di una retta passante per due punti
Siano ùê¥(ùë•1, ùë¶1), ùêµ(ùë•2, ùë¶2) ‚àà ùúã, una retta ùëü passante per ùê¥ e ùêµ (si pu√≤ parafrasare anche con ùê¥ùêµ ‚à• ùëü ) √®
semplicemente una retta passante per ùê¥ e parallela a ùë£(ùë•2 ‚àí ùë•1, ùë¶2 ‚àí ùë¶1). Quindi ùëü ‚à∂ {
ùë• = ùë•1 + (ùë•2 ‚àí ùë•1)ùë°
ùë¶ = ùë¶1 + (ùë¶2 ‚àí ùë¶1)ùë°
con stesso parametro ùë°. Inoltre, il precedente sistema si pu√≤ riscrivere come {
ùë• ‚àí ùë•1 = (ùë•2 ‚àí ùë•1)ùë°
ùë¶ ‚àí ùë¶1 = (ùë¶2 ‚àí ùë¶1)ùë°
e se li
guardo come vettori avr√≤ che (ùë• ‚àí ùë•1, ùë¶ ‚àí ùë¶1) √® proporzionale a (ùë•2 ‚àí ùë•1, ùë¶2 ‚àí ùë¶1), altro modo per vedere
che sono proporzionali √® il determinante: |
ùë• ‚àí ùë•1
ùë¶ ‚àí ùë¶1
ùë•2 ‚àí ùë•1 ùë¶2 ‚àí ùë¶1
| = 0 ‚áî (ùë• ‚àí ùë•1)(ùë¶2 ‚àí ùë¶1) ‚àí (ùë¶ ‚àí ùë¶1)(ùë•2 ‚àí
ùë•1) = 0 ‚áî (ùë• ‚àí ùë•1)(ùë¶2 ‚àí ùë¶1) = (ùë¶ ‚àí ùë¶1)(ùë•2 ‚àí ùë•1); che si avvicina molto alla formula dei rapporti uguali,
infatti se succede che ùë¶2 ‚àí ùë¶1 ‚â† 0 ‚â† ùë•2 ‚àí ùë•1 (quindi se ùê¥, ùêµ non sono paralleli ne all‚Äôasse ùë• ne all‚Äôasse ùë¶)
dividendo per (ùë•2 ‚àí ùë•1) e (ùë¶2 ‚àí ùë¶1) avr√≤ la formula dei rapporti uguali:
ùë•‚àíùë•1
=
.
ùë•2‚àíùë•1
ùë¶2‚àíùë¶1
Altra formulazione classica per una retta √® la forma esplicita: parto dalla forma ordinaria ùëéùë• + ùëèùë¶ + ùëê = 0
della retta ùëü (si noti che una retta passa per (0,0) se e soltanto se ùëê = 0). Pu√≤ succedere che se ùëè ‚â† 0 posso
esplicitare ùë¶ ‚áí ùë¶ = ‚àí
ùëé
ùëè ùë• ‚àí
ùëê
ùëè, chiamando ‚àí
ùëé
ùëè = ùëö e ‚àí
ùë¶‚àíùë¶1
ùëê
ùëè = ùëù, si ha la forma esplicita (che non √® detto che
esista, infatti dipende da ùëè) ùë¶ = ùëöùë• + ùëù dove ùëö √® detto coefficiente angolare di ùëü.
Coseni direttori
Prendiamo una retta orientata ùëü (ne fissiamo un verso) e prendiamo un vettore ùë£ ‚à• ùëü e definisco i coseni
direttori di questa retta come i coseni dell‚Äôangolo che forma con il vettore delle ascisse e quello che forma
con il vettore unitario dell‚Äôasse delle ordinate, rispettivamente: cos(ùëíùë•^ùë£) e cos(ùëíùë¶^ùë£). E come gi√† visto in
precedenza per ùëü: ùëéùë• + ùëèùë¶ + ùëê = 0 sar√†: : cos(ùëíùë•^ùë£) = ¬± ùëè
‚ÅÑ
‚àöùëé2 + ùëè2 e cos(ùëíùë¶^ùë£) = ¬±ùëé
‚ÅÑ‚àöùëé2 + ùëè2; non
√® detto che il vettore (‚àíùëè, ùëé) punti nella stessa direzione della retta, ma √® comunque parallelo ad essa, per
cui il ¬± √® a seconda che (‚àíùëè, ùëé) sia concorde (+) o discorde (‚àí) con l‚Äôorientazione fissata della retta ùëü.
Quindi, i coseni direttori sono semplicemente i coseni degli angoli che formano un vettore parallelo alla retta
con gli assi coordinati.
Rette parallele
Due rette ùëü e ùëü‚Ä≤ sono parallele se e solamente se o hanno intersezione vuota ùëü ‚à© ùëü‚Ä≤ ‚â† ‚àÖ (propriamente),
oppure se coincidono ùëü = ùëü‚Ä≤ (impropriamente).
Al fine di determinare qualche criterio di parallelismo prendiamo dei numeri di vettori per ùëü e ùëü‚Ä≤. Ovvero
siano ùë£(ùëô, ùëö) ‚à• ùëü e ùë£‚Ä≤(ùëô‚Ä≤, ùëö‚Ä≤) ‚à• ùëü‚Ä≤, dire che ùëü ‚à• ùëü‚Ä≤ equivale a dire che quei vettori sono paralleli e dunque che
(ùëô, ùëö), (ùëô‚Ä≤, ùëö‚Ä≤) sono proporzionali. Volendo esprimere le rette con la rappresentazione cartesiana, quindi si
ha ùëü: ùëéùë• + ùëèùë¶ + ùëê = 0 e ùëü‚Ä≤: ùëé‚Ä≤ùë• + ùëè‚Ä≤ùë¶ + ùëê‚Ä≤ = 0 con (‚àíùëè, ùëé) ‚à• ùëü e (‚àíùëè‚Ä≤, ùëé‚Ä≤) ‚à• ùëü‚Ä≤ e di conseguenza ùëü ‚à• ùëü‚Ä≤ ‚áî
(‚àíùëè, ùëé) √® proporzionale a (‚àíùëè‚Ä≤, ùëé‚Ä≤) ‚áî (ùëé, ùëè) proporzionale a (ùëé‚Ä≤, ùëè‚Ä≤); a questo punto possono succedere
due cose: tutta la terna (ùëé, ùëè, ùëê) √® proporzionale a (ùëé‚Ä≤, ùëè‚Ä≤, ùëê‚Ä≤) e quindi le due equazioni sono proporzionali di
una costante non nulla e definiscono quindi la stessa retta (parallele impropriamente); altrimenti, (ùëé, ùëè, ùëê)
non √® proporzionale a (ùëé‚Ä≤, ùëè‚Ä≤, ùëê‚Ä≤) e quindi sono parallele propriamente (non possono avere punti in comune).
Vediamo perch√© non possono avere punti in comune, prendiamo il sistema {
ùëéùë• + ùëèùë¶ + ùëê = 0
ùëé‚Ä≤ùë• + ùëè‚Ä≤ùë¶ + ùëê‚Ä≤ = 0
, sappiamo
che (ùëé, ùëè) √® proporzionale a (ùëé‚Ä≤, ùëè‚Ä≤), se risulta (ùëé, ùëè, ùëê) non proporzionale a (ùëé‚Ä≤, ùëè‚Ä≤, ùëê‚Ä≤) allora il rango della
matrice incompleta di quel sistema √® 1, ma essendo le terne non proporzionali la matrice completa ha rango
2 e quindi per Rouch√©-Capelli avremo un sistema non compatibile e cio√® che le due equazioni non hanno
punti in comune. Viceversa, se anche (ùëé, ùëè, ùëê) √® proporzionale a (ùëé‚Ä≤, ùëè‚Ä≤, ùëê‚Ä≤) allora il sistema avr√† un numero
infinito di soluzione e quindi sar√† un sistema compatibile.
48

Distanza tra insiemi
Vediamola per il piano (in maniera analoga si procede per lo spazio), siano ùê¥(ùë•1, ùë¶1) e ùêµ(ùë•2, ùë¶2) ‚àà ùúã la
distanza dal punta ùê¥ al punto ùêµ si definisce ùëë(ùê¥, ùêµ) = ‚àö(ùë•2 ‚àí ùë•1)2 + (ùë¶2 ‚àí ùë¶1)2 = |ùê¥ùêµ|
La distanza tra due sottoinsiemi ùëÜ, ùëá ‚äÜ ùúã la definisco come l‚Äôestremo inferiore (non dico minimo poich√© non
ho la certezza che stia all‚Äôinterno degli insiemi) delle distanze tra i punti di ùëÜ e di ùëá, pi√π precisamente
dist(ùëÜ, ùëá) = inf{ùëë(ùëÉ, ùëÑ), ùëÉ ‚àà ùëÜ, ùëÑ ‚àà ùëá} ‚â• 0.
Esempi: 1)
A
LA
ye
7 1¬∞)|m eN
(4,0)/ aeN :
olt (X,Y) =0
2) ùëã ‚à© ùëå ‚â† ‚àÖ ‚áí dist(ùëã, ùëå) = 0 poich√© ‚àÉùëÉ ‚àà ùëã ‚à© ùëå per cui ùëë(ùëÉ, ùëÉ) = 0.
Punto medio di un segmento
Prendiamo un segmento del piano, che √® ovviamente delimitato dai suoi estremi: ùê¥(ùë•1, ùë¶1) ‚â† ùêµ(ùë•2, ùë¶2),
allora il punto medio ùëÄ(ùë•ùëÄ, ùë¶ùëÄ) √® un punto che si trova alla stessa distanza dal punta ùê¥ e dal punto ùêµ, quindi
risulter√† che i vettori geometrici ùê¥ùëÄ e ùëÄùêµ sono uguali, allora passando alla coordinazione associata le
componenti di ùê¥ùëÄ e ùëÄùêµ devono essere le stesse, quindi (ùë•ùëÄ ‚àí ùë•1, ùë¶ùëÄ ‚àí ùë¶1) = (ùë•2 ‚àí ùë•ùëÄ, ùë¶2 ‚àí ùë¶ùëÄ) da cui il
sistema {
ùë•ùëÄ ‚àí ùë•1 = ùë•2 ‚àí ùë•ùëÄ
ùë¶ùëÄ ‚àí ùë¶1 = ùë¶2 ‚àí ùë¶ùëÄ
‚áí {
ùë•ùëÄ =
ùë¶ùëÄ =
ùë•2+ùë•1
2
ùë¶2+ùë¶1
2
Asse del segmento
Dato sempre il segmento precedente, l‚Äôasse di un segmento √® il luogo geometrico dei
punti (l‚Äôinsieme dei punti) che ha la stessa distanza dal punto ùê¥ e dal punto ùêµ (si trova
quindi sulla retta perpendicolare al segmento ùê¥ùêµ che passa per il punto medio).
Si pu√≤ verificare con le distanze, infatti, ùëÉ ‚àà ùëü ‚áî ùëë(ùê¥, ùëÉ) = ùëë(ùêµ, ùëÉ), e ci√≤ vuol dire
(ùë• ‚àí ùë•1)2 + (ùë¶ ‚àí ùë¶1)2 = (ùë• ‚àí ùë•2)2 + (ùë¶ ‚àí ùë¶2)2 ‚áî 2ùë•(ùë•2 ‚àí ùë•1) + 2ùë¶(ùë¶2 ‚àí ùë¶1) + ùë•1
2 + ùë¶1
2 ‚àí ùë•2
2 ‚àí ùë¶2
2 = 0
che, se la si nota bene, √® l‚Äôequazione di una retta del tipo ùëéùë• + ùëèùë¶ + ùëê = 0 e poich√© ùê¥ ‚â† ùêµ o ùë•2 ‚àí ùë•1 ‚â† 0
oppure ùë¶2 ‚àí ùë¶1 ‚â† 0 perch√© o ùë•1 ‚â† ùë•2 oppure ùë¶1 ‚â† ùë¶2, questa √® un equazione ordinaria della retta.
Un vettore parallelo a questa retta sar√† dunque (‚àíùëè, ùëé) = (ùë¶1 ‚àí ùë¶2, ùë•2 ‚àí ùë•1) e quello che si pu√≤ verificare e
che il vettore geometrico ùê¥ùêµ ha le componenti (ùë•2 ‚àí ùë•1, ùë¶2 ‚àí ùë¶1) ovvero (ùëé, ùëè) e quindi il prodotto scalare
tra (‚àíùëè, ùëé) e (ùëé, ùëè) √® pari a zero, di conseguenza se ‚àíùëéùëè + ùëéùëè = 0 ‚áí ùëü ‚ä• ùê¥ùêµ.
Quest‚Äôasse, come gi√† detto in precedenza, deve passare per il punto medio del segmento, infatti se si prova
a sostituire le espressioni del punto medio alle incognite dell‚Äôequazione 2ùë•(ùë•2 ‚àí ùë•1) + 2ùë¶(ùë¶2 ‚àí ùë¶1) + ùë•1
2 +
2 ‚àí ùë•2
ùë¶1
2 ‚àí ùë¶2
2 = 0 (quindi ùë•ùëÄ con ùë• e ùë¶ùëÄ con ùë¶) si trover√† che il punto medio √® soluzione dell‚Äôequazione.
Rappresentazione ordinaria di una retta nello spazio
Una retta nello spazio si potrebbe rappresentare utilizzando una rappresentazione parametrica dello stesso
ùë• = ùë•0 + ùëôùë°
tipo di quella usata con il piano, quindi {
ùë¶ = ùë¶0 + ùëöùë°
ùëß = ùëß0 + ùëõùë°
, ma quello che non funziona come analogo √® la
rappresentazione ordinaria poich√© quello che ottengo √® una matrice rettangolare e quindi non mi √® possibile
49

fare il determinante, ma quello che posso fare √® definire una retta nello spazio come intersezione dei due
piani, e sar√† proprio questa che chiamer√≤ rappresentazione ordinaria della retta nello spazio.
Vado a prendere due piani che non siano paralleli (altrimenti avrei una intersezione o vuota oppure i due
piani sono coincidenti) {
ùëéùë• + ùëèùë¶ + ùëêùëß + ùëë = 0
ùëé‚Ä≤ùë• + ùëè‚Ä≤ùë¶ + ùëê‚Ä≤ùëß + ùëë‚Ä≤ = 0
e la loro intersezione sar√† una retta, il fatto che non siano
paralleli implica, oltre che la terna (ùëé, ùëè, ùëê) rappresenta le componenti di un vettore perpendicolare al piano,
che le due terne non sono proporzionali, e questo significa che la matrice incompleta (3x2), dato che coincide
con la dimensione del sottospazio generato ad esempio dalle righe (non proporzionali), ha rango 2. Quindi
avendo rango massimo, per la teoria dei sistemi, il sistema omogeneo associato ammette ‚àû1 soluzioni.
Dopodich√©, considerato che questo √® un sistema particolare perch√© dovrebbe esserci ùëë, tutte le soluzioni di
questa equazione si scrivono come una somma di una soluzione particolare, ad esempio (ùë•0, ùë¶0, ùëß0), cio√® un
punto comune alle due equazioni e somma di un elemento generico che sia soluzione del sistema omogeneo
associato, quindi con ùëë = ùëë‚Ä≤ = 0, la si pu√≤ scrivere quindi come sistema generato dalla terna (ùëô, ùëö, ùëõ). Pi√π
precisamente, tutte le soluzioni saranno descritte da (ùë•0, ùë¶0, ùëß0) + ‚å©(ùëô, ùëö, ùëõ)‚å™ con, ovviamente, (ùëô, ùëö, ùëõ) ‚â†
(0,0,0), e sostanzialmente sto descrivendo una retta, perch√© prendo (ùë•0, ùë¶0, ùëß0) e ci sommo una cosa
proporzionale ad (ùëô, ùëö, ùëõ) di una certa costante di proporzionalit√†:
Dall‚Äôintersezione di due piani non paralleli viene fuori una retta.
ZA.Oma)
Oot)
50
